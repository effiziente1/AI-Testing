{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e468ff14",
   "metadata": {},
   "source": [
    "### Create AI Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011c7956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import deepeval\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "api_key = os.getenv(\"CONFIDENT_API_KEY\")\n",
    "deepeval.login_with_confident_api_key(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d22e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load = load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc23779",
   "metadata": {},
   "source": [
    "### Answer Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b430744e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fb5ddca68d48d0847fec658e50b46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 The score is 1.00 because the output perfectly translates 'hola' to 'hello' in English, addressing the input directly and accurately without any irrelevant information.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric()\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Convert hola to english\",\n",
    "    actual_output=\"hola\",\n",
    "    expected_output=\"Hi or Hello\",\n",
    "    retrieval_context=[\"Hi or Hello\"],\n",
    ")\n",
    "\n",
    "answer_relevancy_metric.measure(test_case)\n",
    "print (answer_relevancy_metric.score, answer_relevancy_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3731ec07",
   "metadata": {},
   "source": [
    "### Contextual Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be848958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313b4d07e0f343aaac0981e7a1f899ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Contextual Precision (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant node in the retrieval context is perfectly aligned with the input, ensuring that the most pertinent information is prioritized. Great job on achieving the highest precision!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: You need to buy other shoes\n",
      "  - expected output: You are eligible for a 30 day full refund at no extra cost.\n",
      "  - context: None\n",
      "  - retrieval context: ['All customers are eligible for a 30 day full refund at no extra cost.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Contextual Precision: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Done ðŸŽ‰! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcves4cn0f9tw4psvvhcww33/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcves4cn0f9tw4psvvhcww33/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcves4cn0f9tw4psvvhcww33/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141mâœ“\u001b[0m Done ðŸŽ‰! View results on \n",
       "\u001b]8;id=996545;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcves4cn0f9tw4psvvhcww33/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcves4cn0f9tw4psvvhcww33/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=996545;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcves4cn0f9tw4psvvhcww33/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Contextual Precision', threshold=0.7, success=True, score=1.0, reason='The score is 1.00 because the relevant node in the retrieval context is perfectly aligned with the input, ensuring that the most pertinent information is prioritized. Great job on achieving the highest precision!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0033025000000000003, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context directly states \\'All customers are eligible for a 30 day full refund at no extra cost,\\' which aligns perfectly with the expected output.\"\\n    }\\n]')], conversational=False, multimodal=False, input=\"What if these shoes don't fit?\", actual_output='You need to buy other shoes', expected_output='You are eligible for a 30 day full refund at no extra cost.', context=None, retrieval_context=['All customers are eligible for a 30 day full refund at no extra cost.'], additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcves4cn0f9tw4psvvhcww33/test-cases')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.evaluate import evaluate\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"You need to buy other shoes\"\n",
    "\n",
    "# Replace this with the expected output of your RAG generator\n",
    "expected_output = \"You are eligible for a 30 day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "evaluate(test_cases=[test_case], metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e5d571",
   "metadata": {},
   "source": [
    "### FaithFulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval import assert_test\n",
    "\n",
    "def test_correctness():\n",
    "\n",
    "    correctness_metric = GEval(\n",
    "        name=\"Correctness\",\n",
    "        criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "                           LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "        threshold=0.8\n",
    "    )\n",
    "    actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "    # actual_output = \"You should pay for another shoes\"\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What if these shoes don't fit?\",\n",
    "        # Replace this with the actual output from your LLM application\n",
    "        actual_output=actual_output,\n",
    "        expected_output=\"You are eligible for a 30 day full refund at no extra cost.\"\n",
    "    )\n",
    "    correctness_metric.measure(test_case)\n",
    "    print(correctness_metric.score, correctness_metric.reason)\n",
    "\n",
    "    assert_test(test_case, [correctness_metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73572550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval import assert_test\n",
    "\n",
    "def test_correctness():\n",
    "\n",
    "    correctness_metric = GEval(\n",
    "        name=\"Correctness\",\n",
    "        criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "                           LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "        threshold=0.8\n",
    "    )\n",
    "    actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "    # actual_output = \"You should pay for another shoes\"\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What if these shoes don't fit?\",\n",
    "        # Replace this with the actual output from your LLM application\n",
    "        actual_output=actual_output,\n",
    "        expected_output=\"You are eligible for a 30 day full refund at no extra cost.\"\n",
    "    )\n",
    "    correctness_metric.measure(test_case)\n",
    "    print(correctness_metric.score, correctness_metric.reason)\n",
    "\n",
    "    assert_test(test_case, [correctness_metric])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7196014",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "844f5f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|100% (1/1) [Time Taken: 00:02,  2.80s/test case]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant node in the retrieval contexts is perfectly aligned with the input query, ensuring that the correct information is prioritized. Great job on achieving the highest precision!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who is the current president of the United States of America?\n",
      "  - actual output: not evaluated\n",
      "  - expected output: Donald Trump\n",
      "  - context: None\n",
      "  - retrieval context: ['Donald Trump serves as the current president of America.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Contextual Precision: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Tests finished ðŸŽ‰! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmc2tv57n05l3fte05d6ps676/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmc2tv57n05l3fte05d6ps676/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmc2tv57n05l3fte05d6ps676/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141mâœ“\u001b[0m Tests finished ðŸŽ‰! View results on \n",
       "\u001b]8;id=986450;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmc2tv57n05l3fte05d6ps676/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmc2tv57n05l3fte05d6ps676/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=986450;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmc2tv57n05l3fte05d6ps676/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\u001b[4;94m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Contextual Precision', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the relevant node in the retrieval contexts is perfectly aligned with the input query, ensuring that the correct information is prioritized. Great job on achieving the highest precision!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0031950000000000004, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context directly states \\'Donald Trump serves as the current president of America,\\' which aligns with the expected output.\"\\n    }\\n]')], conversational=False, multimodal=False, input='Who is the current president of the United States of America?', actual_output='not evaluated', expected_output='Donald Trump', context=None, retrieval_context=['Donald Trump serves as the current president of America.'], additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmc2tv57n05l3fte05d6ps676/test-cases')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric()\n",
    "context_precision_metric = ContextualPrecisionMetric()\n",
    "test_case = LLMTestCase(\n",
    "  input=\"Who is the current president of the United States of America?\",\n",
    "  actual_output=\"not evaluated\",\n",
    "  expected_output=\"Donald Trump\",\n",
    "  retrieval_context=[\"Donald Trump serves as the current president of America.\"]\n",
    ")\n",
    "\n",
    "dataset = EvaluationDataset(test_cases=[test_case])\n",
    "dataset.evaluate([context_precision_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72efaac",
   "metadata": {},
   "source": [
    "# Set local LLM for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964060b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama deepseek-r1:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c6b16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c937b698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f23635a8a3438590c21f73d105b777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationDataset(test_cases=[], goldens=[Golden(input=\"What if these shoes don't fit?\", actual_output=None, expected_output='You are eligible for a 30 day full refund at no extra cost.', context=None, retrieval_context=['All customers are eligible for a 30 day full refund at no extra cost.'], additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values={})], conversational_goldens=[], _alias=test, _id=cmbiukhu906baowa2bu2zub7q)\n"
     ]
    }
   ],
   "source": [
    "dataset.pull(\"test\")\n",
    "\n",
    "print(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
