{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Applications 📑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "**RAG** is a method that imprves the Large Language Models (LLMs) by combining them with external knowledge. RAG allows the model to access and use up-to-date, specific information from external data sources.\n",
    "\n",
    "### When to Use RAG:\n",
    "\n",
    "You can use RAG for example to add a content of the classes to allow students ask questions, or for ecoomerce to include the info of your products.\n",
    "\n",
    "- **Private/Proprietary Data**: When you need to query company documents, internal knowledge bases\n",
    "- **Up-to-date Information**: When the LLM's training data is outdated\n",
    "- **Domain-Specific Knowledge**: For specialized fields not well-represented in training data\n",
    "- **Reducing Hallucinations**: By grounding responses in actual documents\n",
    "- **Cost Efficiency**: Cheaper than fine-tuning for many use cases\n",
    "\n",
    "\n",
    "### Key Components of RAG:\n",
    "\n",
    "1. **Document Store**: A collection of documents containing the knowledge you want to query can be a document, code repo\n",
    "2. **Embeddings**: Vector representations of text chunks that capture semantic meaning\n",
    "3. **Vector Store**: A database that stores embeddings and enables similarity search\n",
    "4. **Retriever**: Finds the most relevant documents based on the query\n",
    "5. **Generator**: The LLM that generates answers using retrieved context\n",
    "\n",
    "### How RAG Works:\n",
    "\n",
    "1. The info added to the LLM is break down into chunks and converted to vector\n",
    "2. The vectors are stored in a database \n",
    "3. The user submits a prompt\n",
    "5. The system tranforms into a numerical format (vector)\n",
    "5. Use the vector to search into the knowledge base \n",
    "6. The info is passed to the language model\n",
    "7. The answer is returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Application Example\n",
    "\n",
    "In this example the RAG will add some info about MCP (Model Context Protocol) and after we can check that the correct answer and context it's returned\n",
    "\n",
    "1. **Loads data** from the website https://www.descope.com/learn/post/mcp\n",
    "2. **Chunks the text** into smaller, manageable chunks\n",
    "3. **Creates embeddings** a embedding is the numerical vector representation of the text that caputre the semantic meaning and stores them in a vector database\n",
    "4. **Retrieves relevant chunks** when answering questions\n",
    "5. **Generates answers** using the retrieved context\n",
    "\n",
    "This approach allows us to answer questions about MCP even though the base LLM might not have been trained on this specific information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: \n",
    "\n",
    "We will use:\n",
    "\n",
    "- **LangChain** A framework for developing application powered by languages modesl\n",
    "- **LangSmith** is LangChain's platform for debugging, testing, and monitoring LLM applications.\n",
    "- **OpenAI** The model to add the new info about MCP\n",
    "- **DeepEval** To test the model\n",
    "- **Ollama* To use free models from your computer\n",
    "\n",
    "### API Keys and Environment Variables\n",
    "\n",
    "Before we start, we need to set up our API keys. This notebook uses:\n",
    "- **OpenAI API Key**: Required for embeddings and LLM generation\n",
    "- **LangSmith API Key**: Optional, but recommended for debugging and tracing\n",
    "\n",
    "You can get a free API key at [https://smith.langchain.com/](https://smith.langchain.com/)\n",
    "\n",
    "You need to add this API key in a .env file or set as environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉🥳 Congratulations! You've successfully logged in! 🙌 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "🎉🥳 Congratulations! You've successfully logged in! 🙌 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import deepeval\n",
    "\n",
    "# Set up OpenAI API Key (Required)\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "if \"CONFIDENT_API_KEY\" not in os.environ:\n",
    "    os.environ[\"CONFIDENT_API_KEY\"] = getpass(\"Enter your Confident API key: \")\n",
    "\n",
    "# Set up LangSmith API Key (Optional - suppresses warning)\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = \"not-needed\"  # Prevents the warning\n",
    "\n",
    "api_key = os.getenv(\"CONFIDENT_API_KEY\")\n",
    "deepeval.login_with_confident_api_key(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the next python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process Documents\n",
    "\n",
    "The first step in building a RAG application is to load and process your documents. This involves:\n",
    "\n",
    "1. **Loading documents** from various sources (web, PDFs, databases, etc.)\n",
    "2. **Splitting text** into smaller chunks for better retrieval\n",
    "3. **Creating embeddings** (vector representations) of each chunk\n",
    "4. **Storing embeddings** in a vector database for fast similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create RAG Chain\n",
    "\n",
    "Now we'll create a RAG chain that combines retrieval with generation. The chain will:\n",
    "1. Take a user question\n",
    "2. Retrieve relevant documents from our vector store\n",
    "3. Format those documents as context\n",
    "4. Pass both context and question to the LLM\n",
    "5. Generate an answer based on the retrieved context\n",
    "\n",
    "#### Using LangChain Hub Prompts\n",
    "\n",
    "LangChain Hub provides pre-built, tested prompts for common use cases. The `rlm/rag-prompt` is a popular RAG prompt that instructs the model to:\n",
    "- Answer based on the provided context\n",
    "- Say \"I don't know\" if the answer isn't in the context\n",
    "- Keep answers concise (3 sentences max)\n",
    "\n",
    "In the next examle we will use a free model llama3.2 \n",
    "\n",
    "The temperature controls the randomness and creativity of the model's outputs. With lower temperature < .2 will probably returns the same words always. with higher value for example 0.8 will use more random choices and will return results with more variants. \n",
    "\n",
    "The max_tokens sets the maximun lenght of the model's repsonse. \n",
    "\n",
    "RAG applications generally use low temperature (0 to 0.3) for accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is Model Context Protocol?',\n",
       " 'result': '<think>\\nOkay, the user is asking about the Model Context Protocol (MCP). Let me check the provided contexts.\\n\\nFirst context mentions MCP as a standardized way for LLMs to connect with external data and tools, acting like a \"universal remote.\" It\\'s from Anthropic, open-source, and eliminates custom integrations. \\n\\nAnother context talks about solving the NxM problem by standardizing connections, improving security, and allowing better AI apps. The conclusion highlights MCP\\'s role in connecting LLMs to systems, standardizing the ecosystem, and resolving integration challenges.\\n\\nSo, MCP is a protocol by Anthropic that allows LLMs to connect to external tools and data sources without custom code. It addresses the NxM problem by standardizing interactions, enhancing security and app capabilities. Developers can use it to build more integrated AI applications.\\n</think>\\n\\nThe Model Context Protocol (MCP) is an open-source protocol by Anthropic that enables large language models (LLMs) to connect with external data sources and tools seamlessly. It standardizes integration, eliminating the need for custom code and addressing the \"NxM problem\" of fragmented AI-system interactions. MCP enhances security and allows developers to build more capable, context-aware AI applications.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.3,\n",
    "    max_token=500\n",
    ")\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm, retriever=vectorstore.as_retriever(), prompt=prompt\n",
    ")\n",
    "\n",
    "qa_chain.invoke({\"query\": \"What is Model Context Protocol?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG that returns the context\n",
    "\n",
    "LangChain provides multiple ways to create RAG chains. You can use the `create_retrieval_chain` to return the answer and the context (the documents that were added)\n",
    "\n",
    "1. **Automatically handles retrieval**: The chain manages document retrieval internally\n",
    "2. **Returns structured output**: Includes both the answer and the retrieved context\n",
    "3. **Uses a different prompt template**: The `langchain-ai/retrieval-qa-chat` prompt is optimized for conversational interactions\n",
    "\n",
    "This approach is useful when you want:\n",
    "- More control over the retrieval process\n",
    "- Access to both the answer and the source documents\n",
    "- A more conversational interaction style\n",
    "\n",
    "In this example I am using the OpenAI that will return better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Model Context Protocol?',\n",
       " 'context': [Document(id='6979f00f-7032-42b8-baba-70e06397ab81', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='excel at responding to natural language, they’ve been constrained by their isolation from real-world data and systems.\\xa0The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means developers'),\n",
       "  Document(id='c48fda92-ccfa-425f-9ce1-56b540581c06', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content=\"What Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin us for the launch party for the world's largest MCP hackathon! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of\"),\n",
       "  Document(id='976ba58a-e1c8-44ec-82fe-c5c7f1e31275', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources. You can learn more about customizing scopes in our OAuth scopes and provider tokens guide.ConclusionThe Model Context Protocol represents a significant leap in connecting LLMs to external systems, standardizing a fragmented ecosystem and potentially resolving the NxM problem. By universalizing how AI applications talk to tools and data sources,'),\n",
       "  Document(id='fe640bca-f476-409a-bb24-c3fa32669c8c', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='can build more capable, context-aware applications without reinventing the wheel for each combination of AI model and external system.This guide explains the Model Context Protocol’s architecture and capabilities, how it solves the inherent challenges of AI integration, and how you can begin using it to build better AI apps that go beyond isolated chat interfaces.LLM isolation & the NxM problemIt’s no secret that LLMs are remarkably capable, but they typically operate in isolation from')],\n",
       " 'answer': 'The Model Context Protocol (MCP) is a standardized way for Language Learning Models (LLMs) to connect with external data sources and tools. It is essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP eliminates the need for custom integration between LLMs and other apps, allowing developers to build more capable, context-aware applications.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\", \n",
    "    temperature=0.3,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "rag_chain = create_retrieval_chain(vectorstore.as_retriever(), combine_docs_chain)\n",
    "\n",
    "rag_chain.invoke({\"input\": \"What is Model Context Protocol?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RetrievalQA Chain\n",
    "\n",
    "Here's yet another way to create a RAG chain using the `RetrievalQA` class. This is a more traditional approach that:\n",
    "\n",
    "1. **Simplifies chain creation**: Combines retrieval and QA in a single class\n",
    "2. **Uses the same prompt**: We can reuse the `rlm/rag-prompt` from LangChain Hub\n",
    "3. **Returns structured output**: The response includes both the query and the result\n",
    "\n",
    "Key differences:\n",
    "- Uses `query` as the input key instead of `question` or `input`\n",
    "- Returns a dictionary with `query` and `result` keys\n",
    "- Handles the retrieval and formatting internally\n",
    "\n",
    "This approach is ideal when you want a simple, straightforward RAG implementation without complex chain composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Context in RAG Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved documents: 4\n",
      "\n",
      "Retrieved documents:\n",
      "\n",
      "Document 1:\n",
      "Content: excel at responding to natural language, they’ve been constrained by their isolation from real-world data and systems. The Model Context Protocol (MCP) addresses this challenge by providing a standard...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 2:\n",
      "Content: What Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin us for the launch party for the world's largest MCP hackathon! Let's go >Log InUser CircleProductUse CasesD...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 3:\n",
      "Content: exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources. You can learn more about customizing scopes in ou...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 4:\n",
      "Content: can build more capable, context-aware applications without reinventing the wheel for each combination of AI model and external system.This guide explains the Model Context Protocol’s architecture and ...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Formatted context (first 500 chars):\n",
      "excel at responding to natural language, they’ve been constrained by their isolation from real-world data and systems. The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means developers\n",
      "...\n",
      "\n",
      "The prompt receives:\n",
      "- context: 1986 characters of retrieved text\n",
      "- question: What is Model Context Protocol?\n"
     ]
    }
   ],
   "source": [
    "# Define the format_docs function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format a list of documents into a single string for context.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# When using hub.pull(\"rlm/rag-prompt\"), the prompt expects:\n",
    "# - context: The retrieved documents formatted as text\n",
    "# - question: The user's question\n",
    "\n",
    "# Let's see how the context is passed through the chain\n",
    "question = \"What is Model Context Protocol?\"\n",
    "\n",
    "# Get the retrieved documents\n",
    "retrieved_docs = vectorstore.as_retriever().invoke(question)\n",
    "print(f\"Number of retrieved documents: {len(retrieved_docs)}\")\n",
    "print(\"\\nRetrieved documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "# Format the documents as context\n",
    "context = format_docs(retrieved_docs)\n",
    "print(f\"\\nFormatted context (first 500 chars):\\n{context[:500]}...\")\n",
    "\n",
    "# This is what gets passed to the prompt\n",
    "print(f\"\\nThe prompt receives:\")\n",
    "print(f\"- context: {len(context)} characters of retrieved text\")\n",
    "print(f\"- question: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Testing RAG Application with DeepEval\n",
    "\n",
    "Now that we have a working RAG application, we need to evaluate its performance. This is crucial because:\n",
    "\n",
    "1. **RAG Quality Varies**: The quality of answers depends on retrieval accuracy and generation quality\n",
    "2. **No Ground Truth**: Unlike traditional ML, we often don't have exact \"correct\" answers\n",
    "3. **Multiple Dimensions**: We need to evaluate relevance, accuracy, completeness, and more\n",
    "\n",
    "#### What is DeepEval?\n",
    "\n",
    "DeepEval is an open-source framework for evaluating LLM applications. It provides:\n",
    "- Pre-built metrics for common evaluation needs\n",
    "- Custom metric creation capabilities\n",
    "- Integration with popular LLM frameworks\n",
    "- Detailed evaluation reports\n",
    "\n",
    "#### Creating Test Cases\n",
    "\n",
    "A test case in DeepEval consists of:\n",
    "- **Input**: The question asked to the RAG system\n",
    "- **Actual Output**: What the RAG system returned\n",
    "- **Expected Output**: What we expect the system to return (optional)\n",
    "- **Retrieval Context**: The documents retrieved by the RAG system (important for RAG metrics!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "question = \"What is Model Context Protocol?\"\n",
    "test_case = LLMTestCase(\n",
    "  input=question,\n",
    "  actual_output=qa_chain(question)[\"result\"],\n",
    "  expected_output=\"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    ")\n",
    "\n",
    "dataset = EvaluationDataset(test_cases=[test_case])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Evaluation Metrics\n",
    "\n",
    "DeepEval provides various metrics to evaluate RAG systems:\n",
    "\n",
    "#### 1. GEval (General Evaluation)\n",
    "GEval is a flexible metric that uses LLMs to evaluate outputs based on custom criteria. You can define:\n",
    "- **Name**: A descriptive name for your metric\n",
    "- **Criteria**: What the LLM should evaluate\n",
    "- **Evaluation Parameters**: Which parts of the test case to evaluate\n",
    "\n",
    "#### 2. Built-in Metrics\n",
    "- **Answer Relevancy**: Measures if the answer addresses the question\n",
    "- **Faithfulness**: Checks if the answer is grounded in the retrieved context\n",
    "- **Contextual Precision**: Evaluates if relevant docs are ranked higher\n",
    "- **Contextual Relevancy**: Measures how relevant the retrieved context is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "# Custom metric to evaluate conciseness\n",
    "# This uses an LLM to judge if the output is concise while complete\n",
    "concise_metrics = GEval(\n",
    "    name = \"Concise\",\n",
    "    criteria=\"Assess if the actual output remains concise while preserving all essential information.\",\n",
    "    \n",
    "    # Only evaluate the actual output (not comparing with expected)\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Custom metric to evaluate completeness\n",
    "# This checks if all key information is retained in the output\n",
    "completeness_metrics = GEval(\n",
    "    name = \"Completeness\",\n",
    "    criteria=\"Assess whether the actual output retains all the key information from the input\",\n",
    "    \n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to evauate with a local model un comment and execute the next command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!deepeval set-ollama deepseek-r1:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with GEval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Evaluation\n",
    "\n",
    "Now let's run our evaluation with multiple metrics. DeepEval will:\n",
    "1. Execute each metric on our test case\n",
    "2. Provide detailed scores and explanations\n",
    "3. Show which metrics passed or failed based on thresholds\n",
    "\n",
    "Note: Since our test case doesn't include `retrieval_context`, metrics like Faithfulness and Contextual Precision won't work properly. In a real RAG evaluation, you should always include the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Completeness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCompleteness \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Concise </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mConcise \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4bc3bb6e244effbb7b58d6a50affde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Completeness [GEval] (score: 0.9977022632222555, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The actual output accurately and comprehensively includes all key information from the input: MCP is open-source from Anthropic, standardizes LLM connections to external data and tools, acts as a 'universal remote,' eliminates custom integrations, addresses the NxM problem, improves security, reduces supply chain risks, and enables developers to build better applications. No key details are missing, altered, or added., error: None)\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and addressed the question directly without any irrelevant information. Great job staying focused and concise!, error: None)\n",
      "  - ✅ Concise [GEval] (score: 0.8939913345197406, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The Actual Output is concise and includes all essential information from the provided context: it explains MCP as an open-source protocol by Anthropic, its role in standardizing LLM connections to external tools, eliminating custom integrations, enhancing security, and addressing the NxM problem. It also mentions the 'universal remote' analogy and the benefits for developers. The output is not unnecessarily verbose and omits only minor implementation details (like OAuth scopes and provider tokens), which are less central. Overall, it balances conciseness and completeness very well., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Model Context Protocol?\n",
      "  - actual output: <think>\n",
      "Okay, the user is asking about the Model Context Protocol (MCP). Let me check the provided context.\n",
      "\n",
      "First context mentions MCP as a standardized way for LLMs to connect with external data and tools, acting like a \"universal remote.\" It's open-source from Anthropic, eliminating custom integrations. \n",
      "\n",
      "Another context talks about solving the NxM problem by standardizing connections, improving security and reducing supply chain attacks. It also mentions OAuth scopes and provider tokens for customization.\n",
      "\n",
      "The conclusion highlights MCP's role in connecting LLMs to systems, standardizing the ecosystem, and allowing developers to build better apps without reinventing integrations.\n",
      "\n",
      "Putting this together: MCP is an open-source protocol by Anthropic that standardizes how LLMs interact with external tools and data, solving integration challenges and enhancing security. It eliminates the need for custom code, making it easier to connect AI models with various systems. This standardization helps address the NxM problem and improves the overall ecosystem for AI applications.\n",
      "</think>\n",
      "\n",
      "The Model Context Protocol (MCP) is an open-source protocol by Anthropic that standardizes how large language models (LLMs) connect to external data sources and tools, acting as a \"universal remote\" for AI applications. It eliminates the need for custom integrations, enabling seamless interaction between LLMs and external systems while enhancing security and reducing supply chain risks. MCP addresses the NxM problem by universalizing AI app interactions, allowing developers to build more capable, context-aware applications efficiently.\n",
      "  - expected output: The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Completeness [GEval]: 100.00% pass rate\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Concise [GEval]: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Done 🎉! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb8bzf0473m17cklhfpw25/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb8bzf0473m17cklhfpw25/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb8bzf0473m17cklhfpw25/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Done 🎉! View results on \n",
       "\u001b]8;id=215513;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb8bzf0473m17cklhfpw25/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb8bzf0473m17cklhfpw25/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=215513;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb8bzf0473m17cklhfpw25/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Completeness [GEval]', threshold=0.5, success=True, score=0.9977022632222555, reason=\"The actual output accurately and comprehensively includes all key information from the input: MCP is open-source from Anthropic, standardizes LLM connections to external data and tools, acts as a 'universal remote,' eliminates custom integrations, addresses the NxM problem, improves security, reduces supply chain risks, and enables developers to build better applications. No key details are missing, altered, or added.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.0028299999999999996, verbose_logs='Criteria:\\nAssess whether the actual output retains all the key information from the input \\n \\nEvaluation Steps:\\n[\\n    \"Identify the key information present in the input.\",\\n    \"Compare the actual output to the input to check if each key piece of information is included.\",\\n    \"Note any missing, altered, or added information in the actual output.\",\\n    \"Evaluate the outputs against each other based on how completely and accurately they retain all the key information from the input.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.9977022632222555'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer was fully relevant and addressed the question directly without any irrelevant information. Great job staying focused and concise!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.006045999999999999, verbose_logs='Statements:\\n[\\n    \"The Model Context Protocol (MCP) is an open-source protocol by Anthropic.\",\\n    \"MCP standardizes how large language models (LLMs) connect to external data sources and tools.\",\\n    \"MCP acts as a \"universal remote\" for AI applications.\",\\n    \"MCP eliminates the need for custom integrations.\",\\n    \"MCP enables seamless interaction between LLMs and external systems.\",\\n    \"MCP enhances security and reduces supply chain risks.\",\\n    \"MCP addresses the NxM problem by universalizing AI app interactions.\",\\n    \"MCP allows developers to build more capable, context-aware applications efficiently.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Concise [GEval]', threshold=0.5, success=True, score=0.8939913345197406, reason=\"The Actual Output is concise and includes all essential information from the provided context: it explains MCP as an open-source protocol by Anthropic, its role in standardizing LLM connections to external tools, eliminating custom integrations, enhancing security, and addressing the NxM problem. It also mentions the 'universal remote' analogy and the benefits for developers. The output is not unnecessarily verbose and omits only minor implementation details (like OAuth scopes and provider tokens), which are less central. Overall, it balances conciseness and completeness very well.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.003032, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Compare the length of each Actual Output and identify if any are unnecessarily verbose.\",\\n    \"Check if each Actual Output includes all essential information from the source or prompt.\",\\n    \"Determine if any Output omits crucial details that are present in others.\",\\n    \"Rank the Actual Outputs by how well they balance conciseness with complete essential information.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.8939913345197406')], conversational=False, multimodal=False, input='What is Model Context Protocol?', actual_output='<think>\\nOkay, the user is asking about the Model Context Protocol (MCP). Let me check the provided context.\\n\\nFirst context mentions MCP as a standardized way for LLMs to connect with external data and tools, acting like a \"universal remote.\" It\\'s open-source from Anthropic, eliminating custom integrations. \\n\\nAnother context talks about solving the NxM problem by standardizing connections, improving security and reducing supply chain attacks. It also mentions OAuth scopes and provider tokens for customization.\\n\\nThe conclusion highlights MCP\\'s role in connecting LLMs to systems, standardizing the ecosystem, and allowing developers to build better apps without reinventing integrations.\\n\\nPutting this together: MCP is an open-source protocol by Anthropic that standardizes how LLMs interact with external tools and data, solving integration challenges and enhancing security. It eliminates the need for custom code, making it easier to connect AI models with various systems. This standardization helps address the NxM problem and improves the overall ecosystem for AI applications.\\n</think>\\n\\nThe Model Context Protocol (MCP) is an open-source protocol by Anthropic that standardizes how large language models (LLMs) connect to external data sources and tools, acting as a \"universal remote\" for AI applications. It eliminates the need for custom integrations, enabling seamless interaction between LLMs and external systems while enhancing security and reducing supply chain risks. MCP addresses the NxM problem by universalizing AI app interactions, allowing developers to build more capable, context-aware applications efficiently.', expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None, additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb8bzf0473m17cklhfpw25/test-cases')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepeval.metrics\n",
    "\n",
    "deepeval.evaluate(dataset, metrics=[\n",
    "    completeness_metrics,\n",
    "    deepeval.metrics.AnswerRelevancyMetric(),\n",
    "    concise_metrics\n",
    "] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Test Dataset\n",
    "\n",
    "For more comprehensive testing, let's create a dataset with multiple test cases. DeepEval allows you to:\n",
    "\n",
    "1. **Create Golden datasets**: These are test cases with expected outputs that serve as ground truth\n",
    "2. **Push datasets to the cloud**: Store and version your test datasets\n",
    "3. **Pull datasets**: Retrieve datasets for consistent testing across teams\n",
    "\n",
    "#### Golden Test Cases\n",
    "\n",
    "Golden test cases are particularly useful for:\n",
    "- Regression testing\n",
    "- Comparing different model versions\n",
    "- Establishing baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aborted.\n"
     ]
    }
   ],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"What is MCP\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is Relationship between function calling & Model Context Protocol\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are the core components of MCP, just give the heading\",\n",
    "        \"reference\":\"\"\" \n",
    "                    - MCP Client\n",
    "                    - MCP Servers\n",
    "                    - Protocol Handshake\n",
    "                    - Capability Discovery\n",
    "                \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "goldens = []\n",
    "for data in test_data:\n",
    "    golden = Golden(\n",
    "        input=data['input'],\n",
    "        expected_output=data['reference']\n",
    "    )\n",
    "\n",
    "    goldens.append(golden)\n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "\n",
    "dataset.push('test_rag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Comprehensive Evaluation on Multiple Test Cases\n",
    "\n",
    "The code below demonstrates how to evaluate multiple test cases with RAG-specific metrics. Notice how the evaluation includes retrieval context, which is essential for metrics like:\n",
    "\n",
    "- **Faithfulness**: Ensures the answer is grounded in retrieved documents\n",
    "- **Contextual Precision**: Checks if relevant documents are ranked higher\n",
    "- **Contextual Relevancy**: Measures the relevance of retrieved documents\n",
    "\n",
    "These metrics help identify issues like:\n",
    "- Hallucinations (answers not supported by context)\n",
    "- Poor retrieval quality\n",
    "- Irrelevant document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fd8106ef914d0c8e14514590f15d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Concise </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mConcise \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24576960a2da4ebd84c94e9372ef9321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and directly addressed the request without any irrelevant information. Great job staying focused and concise!, error: None)\n",
      "  - ❌ Faithfulness (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.00 because the actual output incorrectly claims that MCP architecture consists of only 'Host application and MCP client', omitting other core components explicitly mentioned in the retrieval context., error: None)\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the top two nodes in the retrieval contexts are highly relevant, directly mentioning the core components of MCP and their headings, while the irrelevant nodes that do not discuss the core components are ranked lower. Great job keeping the most useful information at the top!, error: None)\n",
      "  - ❌ Contextual Relevancy (score: 0.2222222222222222, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.22 because most of the retrieval context discusses topics unrelated to the core components of MCP, such as security, benefits, and general statements about LLMs. Only a few statements like 'MCP architecture consists of four primary elements: Fig: MCP core components' and 'MCP architecture and core components' are relevant to the input., error: None)\n",
      "  - ❌ Concise [GEval] (score: 0.469030064238248, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The response is concise, listing the core components of MCP as 'Host application and MCP client.' However, it lacks additional context or details that might be expected, such as the roles or functions of these components, which could be considered essential information., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What are the core components of MCP, just give the heading\n",
      "  - actual output: The core components of MCP are: Host application and MCP client.\n",
      "  - expected output:  \n",
      "                    - MCP Client\n",
      "                    - MCP Servers\n",
      "                    - Protocol Handshake\n",
      "                    - Capability Discovery\n",
      "                \n",
      "  - context: None\n",
      "  - retrieval context: ['a universal way for AI applications to interact with external systems by standardizing context.Fig: MCP general architectureCore componentsMCP architecture consists of four primary elements:Fig: MCP core componentsHost application: LLMs that interact with users and initiate connections. This includes Claude Desktop, AI-enhanced IDEs like Cursor, and standard web-based LLM chat interfaces.MCP client: Integrated within the host application to handle connections with MCP servers, translating', 'contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.SubscribeLarge language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models', 'MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire community—rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour', 'history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 66.67% pass rate\n",
      "Contextual Precision: 66.67% pass rate\n",
      "Contextual Relevancy: 66.67% pass rate\n",
      "Concise [GEval]: 66.67% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and addressed the question directly without any irrelevant information. Great job staying focused and concise!, error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: Great job! There are no contradictions, so the actual output is fully faithful to the retrieval context., error: None)\n",
      "  - ❌ Contextual Precision (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.00 because all the top-ranked nodes in the retrieval contexts are irrelevant—they do not define what MCP is or explain what the acronym stands for, as shown by statements like 'does not define what MCP stands for or provide a direct explanation' in the first node and similar issues in the other nodes. No relevant nodes are present, so irrelevant nodes are not ranked below any relevant ones., error: None)\n",
      "  - ✅ Contextual Relevancy (score: 0.9166666666666666, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.92 because most statements in the retrieval context directly describe MCP, such as 'MCP reduces development overhead' and 'The MCP ecosystem comprises a diverse range of servers,' making the context highly relevant to the input question about MCP, with only minor irrelevant content., error: None)\n",
      "  - ✅ Concise [GEval] (score: 0.7321830080570518, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The response correctly identifies the lack of specific information about what MCP stands for or what it is, addressing the essential information. It is concise and does not include unnecessary details. However, it could be improved by explicitly stating that no key facts are omitted, ensuring clarity in the evaluation., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is MCP\n",
      "  - actual output: The text does not provide specific information on what MCP stands for or what it is.\n",
      "  - expected output: The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\n",
      "  - context: None\n",
      "  - retrieval context: ['MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire community—rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour', 'history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP', 'first-party offering, providing comprehensive support for everything MCP can do.\\xa0Several code editors and IDEs have adopted support for the protocol, including Zed (which surfaces prompts as slash commands), Cursor (with MCP tools in its Composer environment), Continue (an open-source AI code assistant for JetBrains and Visual Studio Code), and Sourcegraph Cody (which implements MCP through OpenCtx).Framework support has expanded to include integrations for Firebase Genkit, LangChain adapters,', 'and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 66.67% pass rate\n",
      "Contextual Precision: 66.67% pass rate\n",
      "Contextual Relevancy: 66.67% pass rate\n",
      "Concise [GEval]: 66.67% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 0.7142857142857143, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.71 because while the answer touches on the relationship between function calling and Model Context Protocol, it includes some general background and tangential benefits that do not directly address the specific relationship, preventing a higher score. However, it still provides some relevant information, justifying its current score., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: Great job! There are no contradictions, so the actual output is fully faithful to the retrieval context., error: None)\n",
      "  - ✅ Contextual Precision (score: 0.8055555555555555, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.81 because the first node is highly relevant, but the second node is irrelevant as it only discusses security and OAuth scopes, not the relationship between function calling and Model Context Protocol. This irrelevant node appears before two additional relevant nodes (ranked 3 and 4), which both provide direct explanations such as 'MCP builds on existing function calling' and 'the new protocol simply standardizes how this API feature works.' The presence of the irrelevant node at rank 2 lowers the score, but the relevant nodes are otherwise ranked well, keeping the score relatively high., error: None)\n",
      "  - ✅ Contextual Relevancy (score: 0.7272727272727273, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.73 because, while several statements directly explain how the Model Context Protocol (MCP) builds on and standardizes function calling (e.g., 'MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.'), there are also irrelevant statements about security, OAuth, and vague references like 'MCP' alone, which do not address the relationship between function calling and MCP., error: None)\n",
      "  - ✅ Concise [GEval] (score: 0.836180029498611, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output effectively includes all key points from the original material, such as the role of function calling in LLMs, the introduction of the Model Context Protocol (MCP), and its purpose in standardizing connections with external data sources. It avoids unnecessary verbosity and maintains clarity. However, the output could be slightly more concise by reducing the explanation of MCP's role, which slightly affects the overall conciseness compared to the original material., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Relationship between function calling & Model Context Protocol\n",
      "  - actual output: Function calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. The Model Context Protocol (MCP) builds on this existing function calling by providing a standardized way for LLMs to connect with external data sources and tools. This eliminates the need for custom integration between LLMs and other apps. Therefore, function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\n",
      "  - expected output: The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\n",
      "  - context: None\n",
      "  - retrieval context: ['doesn’t solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling—the primary method for calling APIs from LLMs—to make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes referred to as “tool', 'exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources. You can learn more about customizing scopes in our OAuth scopes and provider tokens guide.ConclusionThe Model Context Protocol represents a significant leap in connecting LLMs to external systems, standardizing a fragmented ecosystem and potentially resolving the NxM problem. By universalizing how AI applications talk to tools and data sources,', 'excel at responding to natural language, they’ve been constrained by their isolation from real-world data and systems.\\xa0The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means developers', 'use,” function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\\xa0Without MCP, when you use a function call directly with an LLM API, you need to:Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.Implement handlers (the actual code that executes when a function is called) for those functions.Create different implementations for each model you support.MCP']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 66.67% pass rate\n",
      "Contextual Precision: 66.67% pass rate\n",
      "Contextual Relevancy: 66.67% pass rate\n",
      "Concise [GEval]: 66.67% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Done 🎉! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb4py101n2vz6w4qp73w72/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb4py101n2vz6w4qp73w72/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb4py101n2vz6w4qp73w72/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Done 🎉! View results on \n",
       "\u001b]8;id=3606;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb4py101n2vz6w4qp73w72/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb4py101n2vz6w4qp73w72/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=3606;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb4py101n2vz6w4qp73w72/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1754021529.331437  940670 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_2', success=False, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer was fully relevant and directly addressed the request without any irrelevant information. Great job staying focused and concise!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.0036179999999999997, verbose_logs='Statements:\\n[\\n    \"The core components of MCP are the Host application and the MCP client.\",\\n    \"Host application.\",\\n    \"MCP client.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=False, score=0.0, reason=\"The score is 0.00 because the actual output incorrectly claims that MCP architecture consists of only 'Host application and MCP client', omitting other core components explicitly mentioned in the retrieval context.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.007298000000000001, verbose_logs='Truths (limit=None):\\n[\\n    \"MCP is a universal way for AI applications to interact with external systems by standardizing context.\",\\n    \"MCP architecture consists of four primary elements.\",\\n    \"Host applications in MCP architecture include LLMs that interact with users and initiate connections, such as Claude Desktop, AI-enhanced IDEs like Cursor, and standard web-based LLM chat interfaces.\",\\n    \"The MCP client is integrated within the host application to handle connections with MCP servers.\",\\n    \"Large language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have changed how people interact with information and technology.\",\\n    \"LLMs can write eloquently, perform deep research, and solve complex problems.\",\\n    \"MCP reduces development overhead and enables a more interoperable ecosystem.\",\\n    \"An official MCP registry is being planned to simplify discovery and integration of available tools.\",\\n    \"The planned MCP registry will be a centralized repository for MCP servers.\",\\n    \"MCP can retrieve information from a wide variety of sources, including popular messaging apps.\",\\n    \"The GitHub MCP server supports a wide variety of actions, such as automating processes, extracting or analyzing data, and building AI-based apps.\",\\n    \"The current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\"\\n] \\n \\nClaims:\\n[\\n    \"The core components of MCP are: Host application and MCP client.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The retrieval context states that MCP architecture consists of four primary elements, not just \\'Host application and MCP client\\' as the claim suggests. The claim omits other core components.\"\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the top two nodes in the retrieval contexts are highly relevant, directly mentioning the core components of MCP and their headings, while the irrelevant nodes that do not discuss the core components are ranked lower. Great job keeping the most useful information at the top!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.00507, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"This context directly mentions \\'MCP architecture consists of four primary elements\\' and lists \\'Host application\\', \\'MCP client\\', and references to \\'MCP servers\\', which are closely related to the expected output headings.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context includes \\'MCP architecture and core components\\' in the table of contents, indicating it covers the core components of MCP, which aligns with the expected output.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context discusses the MCP registry and ecosystem benefits but does not mention or list the core components of MCP.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context focuses on the capabilities of the GitHub MCP server and its use cases, not on the core components of MCP.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=False, score=0.2222222222222222, reason=\"The score is 0.22 because most of the retrieval context discusses topics unrelated to the core components of MCP, such as security, benefits, and general statements about LLMs. Only a few statements like 'MCP architecture consists of four primary elements: Fig: MCP core components' and 'MCP architecture and core components' are relevant to the input.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.015369999999999998, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"MCP architecture consists of four primary elements: Fig: MCP core components\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement directly addresses the core components of MCP, which is the focus of the input question.\"\\n            },\\n            {\\n                \"statement\": \"Host application: LLMs that interact with users and initiate connections. This includes Claude Desktop, AI-enhanced IDEs like Cursor, and standard web-based LLM chat interfaces.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement lists one of the core components of MCP, which is relevant to the input.\"\\n            },\\n            {\\n                \"statement\": \"MCP client: Integrated within the host application to handle connections with MCP servers, translating\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement describes another core component of MCP, which is relevant to the input.\"\\n            },\\n            {\\n                \"statement\": \"a universal way for AI applications to interact with external systems by standardizing context.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'a universal way for AI applications to interact with external systems by standardizing context.\\' describes the purpose or function of MCP, not its core components.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"LLM isolation & the NxM problem\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This heading refers to LLM isolation and the NxM problem, not to the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"MCP architecture and core components\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This heading directly references the architecture and core components of MCP, which is what the input asks for.\"\\n            },\\n            {\\n                \"statement\": \"How MCP works\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This heading is about the operation of MCP, not specifically its core components.\"\\n            },\\n            {\\n                \"statement\": \"MCP client & server ecosystem\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This heading refers to the client and server ecosystem, not the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Security considerations for MCP servers\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This heading is about security considerations, not the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Conclusion\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This is a generic conclusion heading and does not relate to the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Identity and auth news.  Straight to your inbox.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This is a promotional statement about news delivery, not about MCP or its components.\"\\n            },\\n            {\\n                \"statement\": \"Large language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This statement is about LLMs in general, not about MCP or its core components.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire community\\\\u2014rather than remaining siloed.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This statement describes the benefits of MCP, not its core components or headings.\"\\n            },\\n            {\\n                \"statement\": \"As MCP continues to progress as a standard, several new developments have appeared on the horizon: Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This statement discusses new developments and the official MCP registry, not the core components or headings of MCP.\"\\n            },\\n            {\\n                \"statement\": \"This centralized repository will make it easier for anyone (not just those willing to scour\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This statement refers to the benefits of a centralized repository, not the core components or headings of MCP.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"MCP can retrieve information from a wide variety of sources, including popular messaging apps.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement discusses MCP\\'s ability to retrieve information from various sources, but does not mention or list the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This statement is about GitHub\\'s capabilities, not the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"The current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This statement is about the GitHub MCP server as a benchmark, not about the core components of MCP.\"\\n            }\\n        ]\\n    }\\n]'), MetricData(name='Concise [GEval]', threshold=0.5, success=False, score=0.469030064238248, reason=\"The response is concise, listing the core components of MCP as 'Host application and MCP client.' However, it lacks additional context or details that might be expected, such as the roles or functions of these components, which could be considered essential information.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00248, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Compare the length of the Actual Output to determine conciseness.\",\\n    \"Identify and list the essential information present in the Actual Output.\",\\n    \"Check if any essential information is missing in the Actual Output compared to expectations.\",\\n    \"Evaluate whether the Actual Output maintains brevity without omitting key details.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.469030064238248')], conversational=False, multimodal=False, input='What are the core components of MCP, just give the heading', actual_output='The core components of MCP are: Host application and MCP client.', expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=['a universal way for AI applications to interact with external systems by standardizing context.Fig: MCP general architectureCore componentsMCP architecture consists of four primary elements:Fig: MCP core componentsHost application: LLMs that interact with users and initiate connections. This includes Claude Desktop, AI-enhanced IDEs like Cursor, and standard web-based LLM chat interfaces.MCP client: Integrated within the host application to handle connections with MCP servers, translating', 'contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.SubscribeLarge language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models', 'MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire community—rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour', 'history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP'], additional_metadata=None), TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer was fully relevant and addressed the question directly without any irrelevant information. Great job staying focused and concise!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.003484, verbose_logs='Statements:\\n[\\n    \"The text does not provide specific information on what MCP stands for.\",\\n    \"The text does not provide specific information on what MCP is.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='Great job! There are no contradictions, so the actual output is fully faithful to the retrieval context.', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.008450000000000001, verbose_logs='Truths (limit=None):\\n[\\n    \"MCP reduces development overhead.\",\\n    \"MCP enables a more interoperable ecosystem.\",\\n    \"Innovation in an MCP ecosystem benefits the entire community rather than remaining siloed.\",\\n    \"An official MCP registry for MCP servers is being planned.\",\\n    \"The official MCP registry will simplify discovery and integration of available tools.\",\\n    \"The centralized MCP repository will make it easier for anyone to find MCP servers.\",\\n    \"MCP can retrieve information from a wide variety of sources, including popular messaging apps.\",\\n    \"GitHub supports a wide variety of actions, including automating processes, extracting or analyzing data, and building AI-based apps.\",\\n    \"The current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\",\\n    \"Several code editors and IDEs have adopted support for the MCP protocol, including Zed, Cursor, Continue, and Sourcegraph Cody.\",\\n    \"Zed surfaces prompts as slash commands for MCP.\",\\n    \"Cursor includes MCP tools in its Composer environment.\",\\n    \"Continue is an open-source AI code assistant for JetBrains and Visual Studio Code that supports MCP.\",\\n    \"Sourcegraph Cody implements MCP through OpenCtx.\",\\n    \"Framework support for MCP has expanded to include integrations for Firebase Genkit and LangChain adapters.\",\\n    \"Superinterface helps developers build in-app AI assistants with MCP functionality.\",\\n    \"The MCP ecosystem comprises a diverse range of servers, including reference servers, official integrations, and community servers.\",\\n    \"Reference servers are created by the protocol maintainers as implementation examples.\",\\n    \"Official integrations are maintained by companies for their platforms.\",\\n    \"Community servers are developed by independent contributors.\",\\n    \"Reference servers demonstrate core MCP functionality.\"\\n] \\n \\nClaims:\\n[\\n    \"The text does not provide specific information on what MCP stands for or what it is.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=False, score=0.0, reason=\"The score is 0.00 because all the top-ranked nodes in the retrieval contexts are irrelevant—they do not define what MCP is or explain what the acronym stands for, as shown by statements like 'does not define what MCP stands for or provide a direct explanation' in the first node and similar issues in the other nodes. No relevant nodes are present, so irrelevant nodes are not ranked below any relevant ones.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.005304, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context discusses the benefits of MCP, such as reducing development overhead and enabling interoperability, but does not define what MCP stands for or provide a direct explanation of what it is.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context mentions MCP\\'s ability to retrieve information from various sources and gives an example of a GitHub MCP server, but does not explain what MCP is or what the acronym stands for.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context lists code editors and IDEs that support MCP and mentions framework integrations, but does not define MCP or describe its purpose.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context describes the MCP ecosystem and types of servers but does not provide a definition or explanation of what MCP is.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=True, score=0.9166666666666666, reason=\"The score is 0.92 because most statements in the retrieval context directly describe MCP, such as 'MCP reduces development overhead' and 'The MCP ecosystem comprises a diverse range of servers,' making the context highly relevant to the input question about MCP, with only minor irrelevant content.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.013489999999999999, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire community\\\\u2014rather than remaining siloed.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement describes what MCP does, which is relevant to the question \\'What is MCP?\\'\"\\n            },\\n            {\\n                \"statement\": \"As MCP continues to progress as a standard, several new developments have appeared on the horizon: Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement provides information about MCP\\'s development and features, which helps explain what MCP is.\"\\n            },\\n            {\\n                \"statement\": \"This centralized repository will make it easier for anyone (not just those willing to scour\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement further explains a feature of MCP, which is relevant to understanding what MCP is.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"MCP can retrieve information from a wide variety of sources, including popular messaging apps.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement is relevant because it describes a capability of MCP, which is what the input is asking about.\"\\n            },\\n            {\\n                \"statement\": \"GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"This statement is about GitHub\\'s capabilities, not MCP specifically, as indicated by \\'GitHub: Supports a wide variety of actions...\\'.\"\\n            },\\n            {\\n                \"statement\": \"The current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement is relevant because it describes the role of the GitHub MCP server in demonstrating MCP\\'s capabilities.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"first-party offering, providing comprehensive support for everything MCP can do.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement is relevant because it describes MCP as a first-party offering and mentions comprehensive support for its capabilities, which helps explain what MCP is.\"\\n            },\\n            {\\n                \"statement\": \"Several code editors and IDEs have adopted support for the protocol, including Zed (which surfaces prompts as slash commands), Cursor (with MCP tools in its Composer environment), Continue (an open-source AI code assistant for JetBrains and Visual Studio Code), and Sourcegraph Cody (which implements MCP through OpenCtx).\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement is relevant because it lists software that supports MCP, giving context to what MCP is and where it is used.\"\\n            },\\n            {\\n                \"statement\": \"Framework support has expanded to include integrations for Firebase Genkit, LangChain adapters,\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement is relevant because it shows that MCP is being integrated into various frameworks, which helps explain its scope and application.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"Superinterface helps developers build in-app AI assistants with MCP functionality.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement directly mentions MCP functionality, which is relevant to the question \\'What is MCP\\'.\"\\n            },\\n            {\\n                \"statement\": \"The MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement describes the MCP ecosystem, which helps explain what MCP is.\"\\n            },\\n            {\\n                \"statement\": \"Reference servers demonstrate core MCP functionality and serve as implementation examples.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement explains the purpose of reference servers in the context of MCP, which is relevant to understanding what MCP is.\"\\n            }\\n        ]\\n    }\\n]'), MetricData(name='Concise [GEval]', threshold=0.5, success=True, score=0.7321830080570518, reason='The response correctly identifies the lack of specific information about what MCP stands for or what it is, addressing the essential information. It is concise and does not include unnecessary details. However, it could be improved by explicitly stating that no key facts are omitted, ensuring clarity in the evaluation.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00251, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Identify all essential information in the actual output.\",\\n    \"Evaluate the conciseness of the output by checking for unnecessary details.\",\\n    \"Ensure that all essential information is preserved without omitting key facts.\",\\n    \"Compare the actual output with other versions to determine relative conciseness.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.7321830080570518')], conversational=False, multimodal=False, input='What is MCP', actual_output='The text does not provide specific information on what MCP stands for or what it is.', expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=['MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire community—rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour', 'history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP', 'first-party offering, providing comprehensive support for everything MCP can do.\\xa0Several code editors and IDEs have adopted support for the protocol, including Zed (which surfaces prompts as slash commands), Cursor (with MCP tools in its Composer environment), Continue (an open-source AI code assistant for JetBrains and Visual Studio Code), and Sourcegraph Cody (which implements MCP through OpenCtx).Framework support has expanded to include integrations for Firebase Genkit, LangChain adapters,', 'and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as'], additional_metadata=None), TestResult(name='test_case_1', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=0.7142857142857143, reason='The score is 0.71 because while the answer touches on the relationship between function calling and Model Context Protocol, it includes some general background and tangential benefits that do not directly address the specific relationship, preventing a higher score. However, it still provides some relevant information, justifying its current score.', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.0059240000000000004, verbose_logs='Statements:\\n[\\n    \"Function calling allows LLMs to invoke predetermined functions based on user requests.\",\\n    \"Function calling is a well-established feature of modern AI models.\",\\n    \"The Model Context Protocol (MCP) builds on existing function calling.\",\\n    \"MCP provides a standardized way for LLMs to connect with external data sources and tools.\",\\n    \"MCP eliminates the need for custom integration between LLMs and other apps.\",\\n    \"Function calling is not mutually exclusive with MCP.\",\\n    \"MCP standardizes how the function calling API feature works.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"Stating that function calling is a well-established feature is general background information and does not directly address the relationship between function calling and MCP.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"Eliminating the need for custom integration is a benefit of MCP, but it does not directly address the relationship between function calling and MCP.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='Great job! There are no contradictions, so the actual output is fully faithful to the retrieval context.', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.007810000000000001, verbose_logs='Truths (limit=None):\\n[\\n    \"The Model Context Protocol (MCP) connects AI apps to context while building on top of function calling.\",\\n    \"Function calling allows LLMs to invoke predetermined functions based on user requests.\",\\n    \"Function calling is a well-established feature of modern AI models.\",\\n    \"The Model Context Protocol (MCP) provides a standardized way for LLMs to connect with external data sources and tools.\",\\n    \"MCP was released by Anthropic as an open-source protocol.\",\\n    \"MCP eliminates the need for custom integration between LLMs and other apps.\",\\n    \"Function calling is not mutually exclusive with MCP; MCP standardizes how this API feature works.\",\\n    \"Without MCP, developers need to define model-specific function schemas, implement handlers, and create different implementations for each model.\",\\n    \"MCP aims to standardize a fragmented ecosystem and potentially resolve the NxM problem.\",\\n    \"MCP can help strengthen resilience against supply chain attacks that leverage unsecured connections between different resources.\"\\n] \\n \\nClaims:\\n[\\n    \"Function calling allows LLMs to invoke predetermined functions based on user requests.\",\\n    \"Function calling is a well-established feature of modern AI models.\",\\n    \"The Model Context Protocol (MCP) builds on existing function calling by providing a standardized way for LLMs to connect with external data sources and tools.\",\\n    \"MCP eliminates the need for custom integration between LLMs and other apps.\",\\n    \"Function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=True, score=0.8055555555555555, reason=\"The score is 0.81 because the first node is highly relevant, but the second node is irrelevant as it only discusses security and OAuth scopes, not the relationship between function calling and Model Context Protocol. This irrelevant node appears before two additional relevant nodes (ranked 3 and 4), which both provide direct explanations such as 'MCP builds on existing function calling' and 'the new protocol simply standardizes how this API feature works.' The presence of the irrelevant node at rank 2 lowers the score, but the relevant nodes are otherwise ranked well, keeping the score relatively high.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.006268, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"This context directly discusses the relationship: \\'It connects AI apps to context while building on top of function calling\\\\u2014the primary method for calling APIs from LLMs\\\\u2014to make development simpler and more consistent.\\' It also mentions that function calling allows LLMs to invoke predetermined functions, which is central to the expected output.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context focuses on security, OAuth scopes, and the general significance of MCP, but does not mention function calling or its relationship to MCP.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"This context states, \\'MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\\' This directly supports the expected output about MCP building on function calling and standardizing development.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"This context explains that \\'function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\\' It also details the differences in implementation with and without MCP, which is relevant to the relationship described in the expected output.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=True, score=0.7272727272727273, reason=\"The score is 0.73 because, while several statements directly explain how the Model Context Protocol (MCP) builds on and standardizes function calling (e.g., 'MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.'), there are also irrelevant statements about security, OAuth, and vague references like 'MCP' alone, which do not address the relationship between function calling and MCP.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.013604000000000002, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"It connects AI apps to context while building on top of function calling\\\\u2014the primary method for calling APIs from LLMs\\\\u2014to make development simpler and more consistent.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement directly addresses the relationship between function calling and Model Context Protocol by stating that the protocol builds on top of function calling.\"\\n            },\\n            {\\n                \"statement\": \"Function calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement is relevant because it explains what function calling is, which is necessary to understand its relationship with Model Context Protocol.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"The Model Context Protocol represents a significant leap in connecting LLMs to external systems, standardizing a fragmented ecosystem and potentially resolving the NxM problem.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"By universalizing how AI applications talk to tools and data sources,\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources\\' is about security and supply chain attacks, not about the relationship between function calling and the Model Context Protocol.\"\\n            },\\n            {\\n                \"statement\": \"You can learn more about customizing scopes in our OAuth scopes and provider tokens guide.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'You can learn more about customizing scopes in our OAuth scopes and provider tokens guide\\' is about OAuth scopes and provider tokens, not about function calling or the Model Context Protocol.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools\\\\u2014essentially a \\'universal remote\\' for AI apps.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement is relevant because it describes what MCP is and how it relates to connecting LLMs (which use function calling) to external data and tools.\"\\n            },\\n            {\\n                \"statement\": \"Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement is directly relevant as it explains the relationship between function calling and MCP, stating that MCP builds on function calling and removes the need for custom integration.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"Function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement directly addresses the relationship between function calling and Model Context Protocol (MCP), which is the input question.\"\\n            },\\n            {\\n                \"statement\": \"Without MCP, when you use a function call directly with an LLM API, you need to: Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns. Implement handlers (the actual code that executes when a function is called) for those functions. Create different implementations for each model you support.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": \"This statement explains what happens when function calling is used without MCP, which is relevant to understanding their relationship.\"\\n            },\\n            {\\n                \"statement\": \"MCP\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'MCP\\' alone does not provide any information about the relationship between function calling and Model Context Protocol.\"\\n            }\\n        ]\\n    }\\n]'), MetricData(name='Concise [GEval]', threshold=0.5, success=True, score=0.836180029498611, reason=\"The actual output effectively includes all key points from the original material, such as the role of function calling in LLMs, the introduction of the Model Context Protocol (MCP), and its purpose in standardizing connections with external data sources. It avoids unnecessary verbosity and maintains clarity. However, the output could be slightly more concise by reducing the explanation of MCP's role, which slightly affects the overall conciseness compared to the original material.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0031425000000000003, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Check if the actual output includes all key points presented in the original material.\",\\n    \"Verify that the actual output eliminates unnecessary verbosity or filler content.\",\\n    \"Compare the length of the actual output to the original to ensure conciseness is preserved.\",\\n    \"Evaluate whether the essential information in the actual output remains clear and understandable.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.836180029498611')], conversational=False, multimodal=False, input='What is Relationship between function calling & Model Context Protocol', actual_output='Function calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. The Model Context Protocol (MCP) builds on this existing function calling by providing a standardized way for LLMs to connect with external data sources and tools. This eliminates the need for custom integration between LLMs and other apps. Therefore, function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.', expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=['doesn’t solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function calling—the primary method for calling APIs from LLMs—to make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes referred to as “tool', 'exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources. You can learn more about customizing scopes in our OAuth scopes and provider tokens guide.ConclusionThe Model Context Protocol represents a significant leap in connecting LLMs to external systems, standardizing a fragmented ecosystem and potentially resolving the NxM problem. By universalizing how AI applications talk to tools and data sources,', 'excel at responding to natural language, they’ve been constrained by their isolation from real-world data and systems.\\xa0The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means developers', 'use,” function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\\xa0Without MCP, when you use a function call directly with an LLM API, you need to:Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.Implement handlers (the actual code that executes when a function is called) for those functions.Create different implementations for each model you support.MCP'], additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmdsb4py101n2vz6w4qp73w72/test-cases')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase] :\n",
    "    test_cases = []\n",
    "    for golden in goldens.goldens:\n",
    "        response = rag_chain.invoke({\"input\": golden.input})\n",
    "        \n",
    "        test_case = LLMTestCase(\n",
    "            input=golden.input,\n",
    "            actual_output=response[\"answer\"],\n",
    "            expected_output=golden.expected_output,\n",
    "            retrieval_context=[doc.page_content for doc in response[\"context\"]]\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.pull(alias=\"test_rag\")\n",
    "\n",
    "data = convert_goldens_to_test_cases(dataset)\n",
    "\n",
    "concise_metrics = GEval(\n",
    "    name=\"Concise\",\n",
    "    model=\"gpt-4o\",  # Specify OpenAI model\n",
    "    criteria=\"Assess if the actual output remains concise while preserving all essential information.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "deepeval.evaluate(\n",
    "    data,\n",
    "    metrics=[\n",
    "        deepeval.metrics.AnswerRelevancyMetric(),\n",
    "        deepeval.metrics.FaithfulnessMetric(),\n",
    "        deepeval.metrics.ContextualPrecisionMetric(),\n",
    "        deepeval.metrics.ContextualRelevancyMetric(),\n",
    "        concise_metrics\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
