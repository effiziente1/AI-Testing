{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Applications ðŸ“‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "**RAG** is a method that imprves the Large Language Models (LLMs) by combining them with external knowledge. RAG allows the model to access and use up-to-date, specific information from external data sources.\n",
    "\n",
    "### When to Use RAG:\n",
    "\n",
    "You can use RAG for example to add a content of the classes to allow students ask questions, or for ecoomerce to include the info of your products.\n",
    "\n",
    "- **Private/Proprietary Data**: When you need to query company documents, internal knowledge bases\n",
    "- **Up-to-date Information**: When the LLM's training data is outdated\n",
    "- **Domain-Specific Knowledge**: For specialized fields not well-represented in training data\n",
    "- **Reducing Hallucinations**: By grounding responses in actual documents\n",
    "- **Cost Efficiency**: Cheaper than fine-tuning for many use cases\n",
    "\n",
    "\n",
    "### Key Components of RAG:\n",
    "\n",
    "1. **Document Store**: A collection of documents containing the knowledge you want to query can be a document, code repo\n",
    "2. **Embeddings**: Vector representations of text chunks that capture semantic meaning\n",
    "3. **Vector Store**: A database that stores embeddings and enables similarity search\n",
    "4. **Retriever**: Finds the most relevant documents based on the query\n",
    "5. **Generator**: The LLM that generates answers using retrieved context\n",
    "\n",
    "### How RAG Works:\n",
    "\n",
    "1. The info added to the LLM is break down into chunks and converted to vector\n",
    "2. The vectors are stored in a database \n",
    "3. The user submits a prompt\n",
    "5. The system tranforms into a numerical format (vector)\n",
    "5. Use the vector to search into the knowledge base \n",
    "6. The info is passed to the language model\n",
    "7. The answer is returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Application Example\n",
    "\n",
    "In this example the RAG will add some info abotu MCP (Model Context Protocol) and after we can check that the correct answer and context it's returned\n",
    "\n",
    "1. **Loads data** from the website https://www.descope.com/learn/post/mcp\n",
    "2. **Chunks the text** into smaller, manageable chunks\n",
    "3. **Creates embeddings** a embedding is the numerical vector representation of the text that caputre the semantic meaning and stores them in a vector database\n",
    "4. **Retrieves relevant chunks** when answering questions\n",
    "5. **Generates answers** using the retrieved context\n",
    "\n",
    "This approach allows us to answer questions about MCP even though the base LLM might not have been trained on this specific information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: \n",
    "\n",
    "We will use:\n",
    "\n",
    "- **LangChain** A framework for developing application powered by languages modesl\n",
    "- **LangSmith** is LangChain's platform for debugging, testing, and monitoring LLM applications. It provides:\n",
    "- **OpenAI** The model to add the new info about MCP\n",
    "- **DeepEval** To test the model\n",
    "- **Ollama* To use free models from your computer\n",
    "\n",
    "### API Keys and Environment Variables\n",
    "\n",
    "Before we start, we need to set up our API keys. This notebook uses:\n",
    "- **OpenAI API Key**: Required for embeddings and LLM generation\n",
    "- **LangSmith API Key**: Optional, but recommended for debugging and tracing\n",
    "\n",
    "You can get a free API key at [https://smith.langchain.com/](https://smith.langchain.com/)\n",
    "\n",
    "You need to add this API key in a .env file or set as environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import deepeval\n",
    "\n",
    "# Set up OpenAI API Key (Required)\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "if \"CONFIDENT_API_KEY\" not in os.environ:\n",
    "    os.environ[\"CONFIDENT_API_KEY\"] = getpass(\"Enter your Confident API key: \")\n",
    "\n",
    "# Set up LangSmith API Key (Optional - suppresses warning)\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = \"not-needed\"  # Prevents the warning\n",
    "\n",
    "api_key = os.getenv(\"CONFIDENT_API_KEY\")\n",
    "deepeval.login_with_confident_api_key(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the next python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/bin/python -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/bin/python -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/bin/python -m pip install [options] [-e] <vcs project url> ...\n",
      "  /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/bin/python -m pip install [options] [-e] <local project path> ...\n",
      "  /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/bin/python -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -u\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (1.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain) (0.4.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.7.9)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-community in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (0.3.72)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (3.12.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (0.4.5)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-community) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (2025.7.9)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-openai in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (0.3.28)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-openai) (0.3.72)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-openai) (1.95.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-text-splitters in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (0.3.8)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-text-splitters) (0.3.72)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.4.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-chroma in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (0.2.5)\n",
      "Requirement already satisfied: langchain-core>=0.3.70 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-chroma) (0.3.72)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-chroma) (2.1.3)\n",
      "Requirement already satisfied: chromadb>=1.0.9 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-chroma) (1.0.15)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (2.11.7)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (3.25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (4.14.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (1.73.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (8.5.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from chromadb>=1.0.9->langchain-chroma) (4.25.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (1.17.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3.0,>=2.7->posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain-chroma) (2025.7.9)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=1.0.9->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: anyio in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain-chroma) (0.26.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain-chroma) (0.6.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core>=0.3.70->langchain-chroma) (0.4.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langchain-core>=0.3.70->langchain-chroma) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.70->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain-chroma) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain-chroma) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb>=1.0.9->langchain-chroma) (0.4.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (5.29.5)\n",
      "Requirement already satisfied: sympy in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain-chroma) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.35.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain-chroma) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain-chroma) (0.56b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (0.33.4)\n",
      "Requirement already satisfied: filelock in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.9->langchain-chroma) (1.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.9->langchain-chroma) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain-chroma) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: faiss-cpu in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from faiss-cpu) (24.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from beautifulsoup4) (4.14.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (1.95.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tiktoken in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.7.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: deepeval in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (3.2.6)\n",
      "Requirement already satisfied: aiohttp in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (3.12.14)\n",
      "Requirement already satisfied: anthropic in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (0.57.1)\n",
      "Requirement already satisfied: click<8.3.0,>=8.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (8.2.1)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.9.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (1.25.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (1.73.1)\n",
      "Requirement already satisfied: nest_asyncio in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (1.6.0)\n",
      "Requirement already satisfied: ollama in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (0.5.1)\n",
      "Requirement already satisfied: openai in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (1.95.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (1.35.0)\n",
      "Requirement already satisfied: portalocker in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (3.2.0)\n",
      "Requirement already satisfied: posthog<4.0.0,>=3.23.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (3.25.0)\n",
      "Requirement already satisfied: pyfiglet in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (1.0.3)\n",
      "Requirement already satisfied: pytest in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (8.4.1)\n",
      "Requirement already satisfied: pytest-asyncio in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (1.0.0)\n",
      "Requirement already satisfied: pytest-repeat in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (0.9.4)\n",
      "Requirement already satisfied: pytest-rerunfailures<13.0,>=12.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (12.0)\n",
      "Requirement already satisfied: pytest-xdist in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (3.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (2.32.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (13.9.4)\n",
      "Requirement already satisfied: sentry-sdk in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (2.32.0)\n",
      "Requirement already satisfied: setuptools in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (80.9.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (0.9.0)\n",
      "Requirement already satisfied: tenacity<=9.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (8.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (4.67.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (0.16.0)\n",
      "Requirement already satisfied: wheel in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from deepeval) (0.45.1)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.11.7)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.9.0->deepeval) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.9.0->deepeval) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (4.9.1)\n",
      "Requirement already satisfied: certifi in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.35.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.35.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-proto==1.35.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (5.29.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (0.56b0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.17.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (2.9.0.post0)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from posthog<4.0.0,>=3.23.0->deepeval) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.1)\n",
      "Requirement already satisfied: packaging>=17.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pytest-rerunfailures<13.0,>=12.0->deepeval) (24.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from rich<14.0.0,>=13.6.0->deepeval) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from rich<14.0.0,>=13.6.0->deepeval) (2.19.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.6.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.9->deepeval) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->deepeval) (0.1.2)\n",
      "Requirement already satisfied: iniconfig>=1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pytest->deepeval) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pytest->deepeval) (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp->deepeval) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp->deepeval) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp->deepeval) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp->deepeval) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp->deepeval) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp->deepeval) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from aiohttp->deepeval) (1.20.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from anthropic->deepeval) (0.10.0)\n",
      "Requirement already satisfied: execnet>=2.1 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from pytest-xdist->deepeval) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/abigailarmijohernandez/GitHub/AI-Testing/venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -u DeepEval\n",
    "# Environment and utilities\n",
    "%pip install python-dotenv  # For loading environment variables\n",
    "\n",
    "# LangChain packages\n",
    "%pip install langchain\n",
    "%pip install langchain-community\n",
    "%pip install langchain-openai\n",
    "%pip install langchain-text-splitters\n",
    "%pip install langchain-chroma  # Already mentioned in the notebook\n",
    "\n",
    "# Vector stores\n",
    "%pip install faiss-cpu  # or faiss-gpu if you have GPU support\n",
    "\n",
    "# Document loading\n",
    "%pip install beautifulsoup4  # For WebBaseLoader\n",
    "\n",
    "# OpenAI\n",
    "%pip install openai\n",
    "%pip install tiktoken  # For token counting with OpenAI models\n",
    "\n",
    "# Testing and evaluation\n",
    "%pip install deepeval\n",
    "\n",
    "# Additional dependencies that may be required\n",
    "%pip install numpy\n",
    "%pip install scikit-learn  # For some vector operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process Documents\n",
    "\n",
    "The first step in building a RAG application is to load and process your documents. This involves:\n",
    "\n",
    "1. **Loading documents** from various sources (web, PDFs, databases, etc.)\n",
    "2. **Splitting text** into smaller chunks for better retrieval\n",
    "3. **Creating embeddings** (vector representations) of each chunk\n",
    "4. **Storing embeddings** in a vector database for fast similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vectorstosre with 39 documents\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 1. Load data from Web\n",
    "# WebBaseLoader fetches and parses web pages\n",
    "loader = WebBaseLoader(\"https://www.descope.com/learn/post/mcp\")\n",
    "data = loader.load()\n",
    "\n",
    "# 2. Split text into smaller chunks\n",
    "# Why? LLMs have context limits, and smaller chunks improve retrieval accuracy\n",
    "# chunk_size=500: Each chunk will have ~500 characters\n",
    "# chunk_overlap=0: No overlap between chunks (you might want some overlap in practice)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# 3. Create embeddings and store in vector database\n",
    "# FAISS: Facebook AI Similarity Search - efficient vector storage and retrieval\n",
    "# OpenAIEmbeddings: Converts text to vectors using OpenAI's embedding model\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "print(f\"Created vectorstosre with {len(all_splits)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create RAG Chain\n",
    "\n",
    "Now we'll create a RAG chain that combines retrieval with generation. The chain will:\n",
    "1. Take a user question\n",
    "2. Retrieve relevant documents from our vector store\n",
    "3. Format those documents as context\n",
    "4. Pass both context and question to the LLM\n",
    "5. Generate an answer based on the retrieved context\n",
    "\n",
    "#### Using LangChain Hub Prompts\n",
    "\n",
    "LangChain Hub provides pre-built, tested prompts for common use cases. The `rlm/rag-prompt` is a popular RAG prompt that instructs the model to:\n",
    "- Answer based on the provided context\n",
    "- Say \"I don't know\" if the answer isn't in the context\n",
    "- Keep answers concise (3 sentences max)\n",
    "\n",
    "In the next examle we will use a free model llama3.2 \n",
    "\n",
    "The temperature controls the randomness and creativity of the model's outputs. With lower temperature < .2 will probably returns the same words always. with higher value for example 0.8 will use more random choices and will return results with more variants. \n",
    "\n",
    "The max_tokens sets the maximun lenght of the model's repsonse. \n",
    "\n",
    "RAG applications generally use low temperature (0 to 0.3) for accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is Model Context Protocol?',\n",
       " 'result': '<think>\\nOkay, the user is asking about the Model Context Protocol (MCP). Let me check the provided context.\\n\\nFirst, the context mentions that MCP is a protocol by Anthropic, open-source, allowing LLMs to connect with external data and tools. It\\'s called a \"universal remote\" for AI apps. Also, it addresses the NxM problem by standardizing integration, so developers don\\'t need custom solutions each time. The key points are standardization, reducing custom integration, and enabling better AI app development.\\n\\nI need to answer in three sentences max. Start with what MCP is, then its purpose, and finally its impact. Make sure it\\'s concise and uses the context provided. Avoid any extra info not in the context. Check for clarity and brevity.\\n</think>\\n\\nThe Model Context Protocol (MCP) is an open-source protocol by Anthropic that enables large language models (LLMs) to connect with external data sources and tools. It addresses the \"NxM problem\" by standardizing integration, eliminating the need for custom code between LLMs and applications. This allows developers to build more capable, context-aware AI apps without reinventing integration solutions for each system combination.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.3,\n",
    "    max_token=500\n",
    ")\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm, retriever=vectorstore.as_retriever(), prompt=prompt\n",
    ")\n",
    "\n",
    "qa_chain.invoke({\"query\": \"What is Model Context Protocol?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG that returns the context\n",
    "\n",
    "LangChain provides multiple ways to create RAG chains. You can use the `create_retrieval_chain` to return the answer and the context (the documents that were added)\n",
    "\n",
    "1. **Automatically handles retrieval**: The chain manages document retrieval internally\n",
    "2. **Returns structured output**: Includes both the answer and the retrieved context\n",
    "3. **Uses a different prompt template**: The `langchain-ai/retrieval-qa-chat` prompt is optimized for conversational interactions\n",
    "\n",
    "This approach is useful when you want:\n",
    "- More control over the retrieval process\n",
    "- Access to both the answer and the source documents\n",
    "- A more conversational interaction style\n",
    "\n",
    "In this example I am using the OpenAI that will return better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Model Context Protocol?',\n",
       " 'context': [Document(id='ae01d6df-14d0-4c4c-b3d0-5eb261626714', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content=\"What Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightGet a complimentary copy of the 2025 Gartner Buyers Guide for CIAM. Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of\"),\n",
       "  Document(id='ae326841-4f3a-4843-a39d-fdbb907ec5e8', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='excel at responding to natural language, theyâ€™ve been constrained by their isolation from real-world data and systems.\\xa0The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and toolsâ€”essentially a â€œuniversal remoteâ€ for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means developers'),\n",
       "  Document(id='f986d23c-9e76-40a9-b005-f71198cce018', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources. You can learn more about customizing scopes in our OAuth scopes and provider tokens guide.ConclusionThe Model Context Protocol represents a significant leap in connecting LLMs to external systems, standardizing a fragmented ecosystem and potentially resolving the NxM problem. By universalizing how AI applications talk to tools and data sources,'),\n",
       "  Document(id='5d24ae01-8607-4002-a3e2-d3b7b3e06ea8', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='can build more capable, context-aware applications without reinventing the wheel for each combination of AI model and external system.This guide explains the Model Context Protocolâ€™s architecture and capabilities, how it solves the inherent challenges of AI integration, and how you can begin using it to build better AI apps that go beyond isolated chat interfaces.LLM isolation & the NxM problemItâ€™s no secret that LLMs are remarkably capable, but they typically operate in isolation from')],\n",
       " 'answer': 'The Model Context Protocol (MCP) is a standardized way for Language Learning Models (LLMs) to connect with external data sources and tools. It serves as a \"universal remote\" for AI apps. MCP was released by Anthropic as an open-source protocol and it eliminates the need for custom integration between LLMs and other apps. This allows developers to build more capable, context-aware applications without having to create a new solution for each combination of AI model and external system.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\", \n",
    "    temperature=0.3,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "rag_chain = create_retrieval_chain(vectorstore.as_retriever(), combine_docs_chain)\n",
    "\n",
    "rag_chain.invoke({\"input\": \"What is Model Context Protocol?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RetrievalQA Chain\n",
    "\n",
    "Here's yet another way to create a RAG chain using the `RetrievalQA` class. This is a more traditional approach that:\n",
    "\n",
    "1. **Simplifies chain creation**: Combines retrieval and QA in a single class\n",
    "2. **Uses the same prompt**: We can reuse the `rlm/rag-prompt` from LangChain Hub\n",
    "3. **Returns structured output**: The response includes both the query and the result\n",
    "\n",
    "Key differences:\n",
    "- Uses `query` as the input key instead of `question` or `input`\n",
    "- Returns a dictionary with `query` and `result` keys\n",
    "- Handles the retrieval and formatting internally\n",
    "\n",
    "This approach is ideal when you want a simple, straightforward RAG implementation without complex chain composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Context in RAG Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved documents: 4\n",
      "\n",
      "Retrieved documents:\n",
      "\n",
      "Document 1:\n",
      "Content: What Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightGet a complimentary copy of the 2025 Gartner Buyers Guide for CIAM. Let's go >Log InUser CircleProductUse CasesD...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 2:\n",
      "Content: excel at responding to natural language, theyâ€™ve been constrained by their isolation from real-world data and systems.Â The Model Context Protocol (MCP) addresses this challenge by providing a standard...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 3:\n",
      "Content: exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources. You can learn more about customizing scopes in ou...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 4:\n",
      "Content: can build more capable, context-aware applications without reinventing the wheel for each combination of AI model and external system.This guide explains the Model Context Protocolâ€™s architecture and ...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Formatted context (first 500 chars):\n",
      "What Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightGet a complimentary copy of the 2025 Gartner Buyers Guide for CIAM. Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of\n",
      "\n",
      "exce...\n",
      "\n",
      "The prompt receives:\n",
      "- context: 1986 characters of retrieved text\n",
      "- question: What is Model Context Protocol?\n"
     ]
    }
   ],
   "source": [
    "# Define the format_docs function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format a list of documents into a single string for context.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# When using hub.pull(\"rlm/rag-prompt\"), the prompt expects:\n",
    "# - context: The retrieved documents formatted as text\n",
    "# - question: The user's question\n",
    "\n",
    "# Let's see how the context is passed through the chain\n",
    "question = \"What is Model Context Protocol?\"\n",
    "\n",
    "# Get the retrieved documents\n",
    "retrieved_docs = vectorstore.as_retriever().invoke(question)\n",
    "print(f\"Number of retrieved documents: {len(retrieved_docs)}\")\n",
    "print(\"\\nRetrieved documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "# Format the documents as context\n",
    "context = format_docs(retrieved_docs)\n",
    "print(f\"\\nFormatted context (first 500 chars):\\n{context[:500]}...\")\n",
    "\n",
    "# This is what gets passed to the prompt\n",
    "print(f\"\\nThe prompt receives:\")\n",
    "print(f\"- context: {len(context)} characters of retrieved text\")\n",
    "print(f\"- question: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Testing RAG Application with DeepEval\n",
    "\n",
    "Now that we have a working RAG application, we need to evaluate its performance. This is crucial because:\n",
    "\n",
    "1. **RAG Quality Varies**: The quality of answers depends on retrieval accuracy and generation quality\n",
    "2. **No Ground Truth**: Unlike traditional ML, we often don't have exact \"correct\" answers\n",
    "3. **Multiple Dimensions**: We need to evaluate relevance, accuracy, completeness, and more\n",
    "\n",
    "#### What is DeepEval?\n",
    "\n",
    "DeepEval is an open-source framework for evaluating LLM applications. It provides:\n",
    "- Pre-built metrics for common evaluation needs\n",
    "- Custom metric creation capabilities\n",
    "- Integration with popular LLM frameworks\n",
    "- Detailed evaluation reports\n",
    "\n",
    "#### Creating Test Cases\n",
    "\n",
    "A test case in DeepEval consists of:\n",
    "- **Input**: The question asked to the RAG system\n",
    "- **Actual Output**: What the RAG system returned\n",
    "- **Expected Output**: What we expect the system to return (optional)\n",
    "- **Retrieval Context**: The documents retrieved by the RAG system (important for RAG metrics!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/hls4830123s9kn1cxgy9l_2r0000gn/T/ipykernel_81222/4266105146.py:7: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  actual_output=qa_chain(question)[\"result\"],\n"
     ]
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "question = \"What is Model Context Protocol?\"\n",
    "test_case = LLMTestCase(\n",
    "  input=question,\n",
    "  actual_output=qa_chain(question)[\"result\"],\n",
    "  expected_output=\"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and toolsâ€”essentially a â€œuniversal remoteâ€ for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    ")\n",
    "\n",
    "dataset = EvaluationDataset(test_cases=[test_case])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Evaluation Metrics\n",
    "\n",
    "DeepEval provides various metrics to evaluate RAG systems:\n",
    "\n",
    "#### 1. GEval (General Evaluation)\n",
    "GEval is a flexible metric that uses LLMs to evaluate outputs based on custom criteria. You can define:\n",
    "- **Name**: A descriptive name for your metric\n",
    "- **Criteria**: What the LLM should evaluate\n",
    "- **Evaluation Parameters**: Which parts of the test case to evaluate\n",
    "\n",
    "#### 2. Built-in Metrics\n",
    "- **Answer Relevancy**: Measures if the answer addresses the question\n",
    "- **Faithfulness**: Checks if the answer is grounded in the retrieved context\n",
    "- **Contextual Precision**: Evaluates if relevant docs are ranked higher\n",
    "- **Contextual Relevancy**: Measures how relevant the retrieved context is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "# Custom metric to evaluate conciseness\n",
    "# This uses an LLM to judge if the output is concise while complete\n",
    "concise_metrics = GEval(\n",
    "    name = \"Concise\",\n",
    "    criteria=\"Assess if the actual output remains concise while preserving all essential information.\",\n",
    "    \n",
    "    # Only evaluate the actual output (not comparing with expected)\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Custom metric to evaluate completeness\n",
    "# This checks if all key information is retained in the output\n",
    "completeness_metrics = GEval(\n",
    "    name = \"Completeness\",\n",
    "    criteria=\"Assess whether the actual output retains all the key information from the input\",\n",
    "    \n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to evauate with a local model un comment and execute the next command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!deepeval set-ollama deepseek-r1:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with GEval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Evaluation\n",
    "\n",
    "Now let's run our evaluation with multiple metrics. DeepEval will:\n",
    "1. Execute each metric on our test case\n",
    "2. Provide detailed scores and explanations\n",
    "3. Show which metrics passed or failed based on thresholds\n",
    "\n",
    "Note: Since our test case doesn't include `retrieval_context`, metrics like Faithfulness and Contextual Precision won't work properly. In a real RAG evaluation, you should always include the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Completeness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mCompleteness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Concise </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mConcise \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089947396a2543d6b1bd4e95e582acfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Completeness (GEval) (score: 0.9932453312614516, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output accurately captures all key points from the input: it identifies MCP as an open-source protocol by Anthropic, explains its purpose of connecting LLMs to external systems, and describes how it solves the NxM problem by standardizing integration. The output is complete, accurate, and retains the necessary level of detail, reflecting the depth of information in the input., error: None)\n",
      "  - âœ… Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the question about Model Context Protocol without any irrelevant information. Great job on staying focused and relevant!, error: None)\n",
      "  - âœ… Concise (GEval) (score: 0.9377540668798146, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The response effectively captures the essential information about the Model Context Protocol (MCP) by mentioning it is an open-source protocol by Anthropic, its purpose of connecting LLMs to external systems, and how it solves the NxM problem through standardization. The output is concise, consisting of three sentences, and avoids unnecessary details, aligning well with the evaluation steps for completeness and conciseness., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Model Context Protocol?\n",
      "  - actual output: <think>\n",
      "Okay, the user is asking about the Model Context Protocol (MCP). Let me check the provided context.\n",
      "\n",
      "First, the context mentions that MCP is a protocol by Anthropic, open-source, allowing LLMs to connect with external data and tools. It's called a \"universal remote\" for AI apps. Also, it addresses the NxM problem by standardizing integration, so developers don't need custom solutions each time. The key points are standardization, eliminating custom integrations, and enabling better AI app development.\n",
      "\n",
      "I need to summarize this in three concise sentences. Make sure to mention it's an open-source protocol by Anthropic, its purpose of connecting LLMs to external systems, and how it solves the NxM problem by standardizing integration. Avoid any extra details. Let me put that together.\n",
      "</think>\n",
      "\n",
      "The Model Context Protocol (MCP) is an open-source protocol by Anthropic that enables large language models (LLMs) to connect with external data sources and tools. It addresses the \"NxM problem\" by standardizing integration, eliminating the need for custom code between LLMs and applications. This allows developers to build more capable, context-aware AI apps without reinventing integration solutions for each system combination.\n",
      "  - expected output: The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and toolsâ€”essentially a â€œuniversal remoteâ€ for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Completeness (GEval): 100.00% pass rate\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Concise (GEval): 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Done ðŸŽ‰! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqo8ka01y1trrq16fjn52j/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqo8ka01y1trrq16fjn52j/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqo8ka01y1trrq16fjn52j/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141mâœ“\u001b[0m Done ðŸŽ‰! View results on \n",
       "\u001b]8;id=895440;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqo8ka01y1trrq16fjn52j/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqo8ka01y1trrq16fjn52j/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=895440;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqo8ka01y1trrq16fjn52j/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Completeness (GEval)', threshold=0.5, success=True, score=0.9932453312614516, reason='The actual output accurately captures all key points from the input: it identifies MCP as an open-source protocol by Anthropic, explains its purpose of connecting LLMs to external systems, and describes how it solves the NxM problem by standardizing integration. The output is complete, accurate, and retains the necessary level of detail, reflecting the depth of information in the input.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0036525000000000004, verbose_logs='Criteria:\\nAssess whether the actual output retains all the key information from the input \\n \\nEvaluation Steps:\\n[\\n    \"Compare the key points in the actual output with the input to identify any missing information.\",\\n    \"Evaluate the completeness of the actual output by verifying if all main ideas from the input are present.\",\\n    \"Check the accuracy of the information in the actual output against the input to ensure nothing is misrepresented or altered.\",\\n    \"Determine the level of detail retained in the actual output by examining whether it reflects the depth of information contained in the input.\"\\n] \\n \\nRubric:\\nNone'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addresses the question about Model Context Protocol without any irrelevant information. Great job on staying focused and relevant!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.005757500000000001, verbose_logs='Statements:\\n[\\n    \"The Model Context Protocol (MCP) is an open-source protocol by Anthropic.\",\\n    \"MCP enables large language models (LLMs) to connect with external data sources and tools.\",\\n    \"It addresses the \\'NxM problem\\' by standardizing integration.\",\\n    \"MCP eliminates the need for custom code between LLMs and applications.\",\\n    \"Developers can build more capable, context-aware AI apps.\",\\n    \"There is no need to reinvent integration solutions for each system combination.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Concise (GEval)', threshold=0.5, success=True, score=0.9377540668798146, reason='The response effectively captures the essential information about the Model Context Protocol (MCP) by mentioning it is an open-source protocol by Anthropic, its purpose of connecting LLMs to external systems, and how it solves the NxM problem through standardization. The output is concise, consisting of three sentences, and avoids unnecessary details, aligning well with the evaluation steps for completeness and conciseness.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0036125000000000003, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Step 1: Identify and list the essential information present in the Actual Output.\",\\n    \"Step 2: Determine the length of the Actual Output to evaluate conciseness.\",\\n    \"Step 3: Compare the presence of essential information within the concise structure of the Actual Output.\",\\n    \"Step 4: Rank the Actual Outputs based on how well they balance conciseness and completeness of essential information.\"\\n] \\n \\nRubric:\\nNone')], conversational=False, multimodal=False, input='What is Model Context Protocol?', actual_output='<think>\\nOkay, the user is asking about the Model Context Protocol (MCP). Let me check the provided context.\\n\\nFirst, the context mentions that MCP is a protocol by Anthropic, open-source, allowing LLMs to connect with external data and tools. It\\'s called a \"universal remote\" for AI apps. Also, it addresses the NxM problem by standardizing integration, so developers don\\'t need custom solutions each time. The key points are standardization, eliminating custom integrations, and enabling better AI app development.\\n\\nI need to summarize this in three concise sentences. Make sure to mention it\\'s an open-source protocol by Anthropic, its purpose of connecting LLMs to external systems, and how it solves the NxM problem by standardizing integration. Avoid any extra details. Let me put that together.\\n</think>\\n\\nThe Model Context Protocol (MCP) is an open-source protocol by Anthropic that enables large language models (LLMs) to connect with external data sources and tools. It addresses the \"NxM problem\" by standardizing integration, eliminating the need for custom code between LLMs and applications. This allows developers to build more capable, context-aware AI apps without reinventing integration solutions for each system combination.', expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and toolsâ€”essentially a â€œuniversal remoteâ€ for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None, additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqo8ka01y1trrq16fjn52j/test-cases')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepeval.metrics\n",
    "\n",
    "deepeval.evaluate(dataset, metrics=[\n",
    "    completeness_metrics,\n",
    "    deepeval.metrics.AnswerRelevancyMetric(),\n",
    "    concise_metrics\n",
    "] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Test Dataset\n",
    "\n",
    "For more comprehensive testing, let's create a dataset with multiple test cases. DeepEval allows you to:\n",
    "\n",
    "1. **Create Golden datasets**: These are test cases with expected outputs that serve as ground truth\n",
    "2. **Push datasets to the cloud**: Store and version your test datasets\n",
    "3. **Pull datasets**: Retrieve datasets for consistent testing across teams\n",
    "\n",
    "#### Golden Test Cases\n",
    "\n",
    "Golden test cases are particularly useful for:\n",
    "- Regression testing\n",
    "- Comparing different model versions\n",
    "- Establishing baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"What is MCP\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and toolsâ€”essentially a â€œuniversal remoteâ€ for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is Relationship between function calling & Model Context Protocol\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are the core components of MCP, just give the heading\",\n",
    "        \"reference\":\"\"\" \n",
    "                    - MCP Client\n",
    "                    - MCP Servers\n",
    "                    - Protocol Handshake\n",
    "                    - Capability Discovery\n",
    "                \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "goldens = []\n",
    "for data in test_data:\n",
    "    golden = Golden(\n",
    "        input=data['input'],\n",
    "        expected_output=data['reference']\n",
    "    )\n",
    "\n",
    "    goldens.append(golden)\n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "\n",
    "dataset.push('test_rag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Comprehensive Evaluation on Multiple Test Cases\n",
    "\n",
    "The code below demonstrates how to evaluate multiple test cases with RAG-specific metrics. Notice how the evaluation includes retrieval context, which is essential for metrics like:\n",
    "\n",
    "- **Faithfulness**: Ensures the answer is grounded in retrieved documents\n",
    "- **Contextual Precision**: Checks if relevant documents are ranked higher\n",
    "- **Contextual Relevancy**: Measures the relevance of retrieved documents\n",
    "\n",
    "These metrics help identify issues like:\n",
    "- Hallucinations (answers not supported by context)\n",
    "- Poor retrieval quality\n",
    "- Irrelevant document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79e071bc24b43ee86a814cdf5319d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Concise </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mConcise \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c687f3719ce47e9822278ba5dc279eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âŒ Answer Relevancy (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the output failed to address the input question about what MCP stands for or is, rendering the statements irrelevant to the query., error: None)\n",
      "  - âœ… Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating that the actual output is perfectly aligned with the retrieval context. Great job maintaining accuracy and consistency!, error: None)\n",
      "  - âŒ Contextual Precision (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because all nodes in the retrieval contexts are irrelevant to the input question. The first node discusses the benefits and future developments of MCP, but it does not define what MCP is. The second node highlights the potential of MCP in various applications, yet it fails to explain what MCP is. The third node lists tools and platforms that support MCP, but it does not define MCP itself. Lastly, the fourth node describes the types of MCP servers without providing a definition of MCP. As a result, none of the nodes provide the necessary information to answer the input question, leading to a score of 0.00., error: None)\n",
      "  - âœ… Contextual Relevancy (score: 0.8, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.80 because while the retrieval context provides some relevant information about MCP, such as its role in reducing development overhead and enabling interoperability, many statements focus on peripheral aspects like GitHub capabilities and vague references that don't directly explain what MCP is., error: None)\n",
      "  - âŒ Concise (GEval) (score: 0.34274484262376254, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The response correctly identifies that the text lacks specific information about what MCP stands for or what it is, addressing the key point of missing information. However, it fails to provide any additional context or analysis, making it overly brief and lacking in depth. The response is concise but does not fully engage with the evaluation steps, particularly in terms of clarity and completeness., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is MCP\n",
      "  - actual output: The text does not provide specific information on what MCP stands for or what it is.\n",
      "  - expected output: The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and toolsâ€”essentially a â€œuniversal remoteâ€ for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\n",
      "  - context: None\n",
      "  - retrieval context: ['MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour', 'history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP', 'first-party offering, providing comprehensive support for everything MCP can do.\\xa0Several code editors and IDEs have adopted support for the protocol, including Zed (which surfaces prompts as slash commands), Cursor (with MCP tools in its Composer environment), Continue (an open-source AI code assistant for JetBrains and Visual Studio Code), and Sourcegraph Cody (which implements MCP through OpenCtx).Framework support has expanded to include integrations for Firebase Genkit, LangChain adapters,', 'and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 66.67% pass rate\n",
      "Faithfulness: 66.67% pass rate\n",
      "Contextual Precision: 66.67% pass rate\n",
      "Contextual Relevancy: 66.67% pass rate\n",
      "Concise (GEval): 66.67% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the relationship between function calling and Model Context Protocol without any irrelevant statements. Great job on staying focused and relevant!, error: None)\n",
      "  - âœ… Faithfulness (score: 0.8333333333333334, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.83 because the actual output inaccurately claims that MCP standardizes the function calling API feature, whereas the retrieval context specifies that MCP standardizes AI applications' connections with external systems, without focusing on the API feature., error: None)\n",
      "  - âœ… Contextual Precision (score: 0.8055555555555555, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.81 because the first node effectively addresses the input by explaining that MCP 'connects AI apps to context while building on top of function calling,' which is directly relevant. However, the second node, ranked higher than it should be, focuses on security and OAuth scopes, which are not pertinent to the relationship between function calling and MCP. The third and fourth nodes further support the input by describing MCP as a 'universal remote' and clarifying that 'function calling is not mutually exclusive with MCP,' respectively. These relevant nodes contribute positively to the score, but the presence of an irrelevant node in a higher rank prevents a perfect score., error: None)\n",
      "  - âœ… Contextual Relevancy (score: 0.6470588235294118, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.65 because while there are relevant statements like 'Function calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models,' many other statements discuss unrelated topics such as 'sensitive data' and 'supply chain attacks,' which do not pertain to the relationship between function calling and the Model Context Protocol., error: None)\n",
      "  - âœ… Concise (GEval) (score: 0.8238341812583523, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output includes essential information about function calling and the Model Context Protocol (MCP), explaining their roles and relationship. It effectively conveys that function calling allows LLMs to invoke functions and that MCP standardizes connections with external data sources, building on function calling. However, the output could be more concise by reducing repetitive phrases and focusing on the core differences and benefits of MCP over function calling., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Relationship between function calling & Model Context Protocol\n",
      "  - actual output: Function calling and the Model Context Protocol (MCP) are both used in AI models. Function calling allows LLMs to invoke predetermined functions based on user requests. It is not mutually exclusive with MCP. The MCP, on the other hand, is a standardized way for LLMs to connect with external data sources and tools. It builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means that MCP standardizes how the function calling API feature works.\n",
      "  - expected output: The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\n",
      "  - context: None\n",
      "  - retrieval context: ['doesnâ€™t solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function callingâ€”the primary method for calling APIs from LLMsâ€”to make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes referred to as â€œtool', 'exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources. You can learn more about customizing scopes in our OAuth scopes and provider tokens guide.ConclusionThe Model Context Protocol represents a significant leap in connecting LLMs to external systems, standardizing a fragmented ecosystem and potentially resolving the NxM problem. By universalizing how AI applications talk to tools and data sources,', 'excel at responding to natural language, theyâ€™ve been constrained by their isolation from real-world data and systems.\\xa0The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and toolsâ€”essentially a â€œuniversal remoteâ€ for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means developers', 'use,â€ function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\\xa0Without MCP, when you use a function call directly with an LLM API, you need to:Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.Implement handlers (the actual code that executes when a function is called) for those functions.Create different implementations for each model you support.MCP']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 66.67% pass rate\n",
      "Faithfulness: 66.67% pass rate\n",
      "Contextual Precision: 66.67% pass rate\n",
      "Contextual Relevancy: 66.67% pass rate\n",
      "Concise (GEval): 66.67% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly aligns with the input request, providing only the core components of MCP as headings without any irrelevant information. Great job!, error: None)\n",
      "  - âŒ Faithfulness (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output incorrectly claims that the core components of MCP are only the Host application and MCP client, while the retrieval context clearly states that MCP architecture consists of four primary elements, not just two., error: None)\n",
      "  - âœ… Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant nodes are perfectly ranked higher than the irrelevant nodes. The first node provides a direct mention of 'MCP architecture consists of four primary elements' and includes 'MCP client', which aligns with the input. The second node further supports this by discussing the 'MCP client & server ecosystem', directly relating to the expected output. The irrelevant nodes, ranked third and fourth, focus on benefits, future developments, and potential applications of MCP, which do not directly address the core components, thus justifying their lower ranking. Great job on achieving perfect precision!, error: None)\n",
      "  - âŒ Contextual Relevancy (score: 0.18181818181818182, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.18 because the retrieval context primarily contains references and descriptions that do not directly list the core components of MCP. While there is a mention of 'MCP architecture and core components,' it lacks detailed headings or a clear list, making it minimally relevant to the input request for core component headings., error: None)\n",
      "  - âœ… Concise (GEval) (score: 0.7540211996828792, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual output succinctly identifies the key components of MCP as the 'Host application' and 'MCP client,' aligning well with the evaluation steps. It is concise and free of unnecessary verbosity, making it easy to understand. However, it could provide slightly more context to ensure all essential points are covered., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What are the core components of MCP, just give the heading\n",
      "  - actual output: The core components of MCP are: Host application and MCP client.\n",
      "  - expected output:  \n",
      "                    - MCP Client\n",
      "                    - MCP Servers\n",
      "                    - Protocol Handshake\n",
      "                    - Capability Discovery\n",
      "                \n",
      "  - context: None\n",
      "  - retrieval context: ['a universal way for AI applications to interact with external systems by standardizing context.Fig: MCP general architectureCore componentsMCP architecture consists of four primary elements:Fig: MCP core componentsHost application: LLMs that interact with users and initiate connections. This includes Claude Desktop, AI-enhanced IDEs like Cursor, and standard web-based LLM chat interfaces.MCP client: Integrated within the host application to handle connections with MCP servers, translating', 'contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.SubscribeLarge language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models', 'MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour', 'history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 66.67% pass rate\n",
      "Faithfulness: 66.67% pass rate\n",
      "Contextual Precision: 66.67% pass rate\n",
      "Contextual Relevancy: 66.67% pass rate\n",
      "Concise (GEval): 66.67% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Done ðŸŽ‰! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqpfe801ybtrrq5oj64pbf/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqpfe801ybtrrq5oj64pbf/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqpfe801ybtrrq5oj64pbf/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141mâœ“\u001b[0m Done ðŸŽ‰! View results on \n",
       "\u001b]8;id=276595;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqpfe801ybtrrq5oj64pbf/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqpfe801ybtrrq5oj64pbf/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=276595;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqpfe801ybtrrq5oj64pbf/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=False, score=0.0, reason='The score is 0.00 because the output failed to address the input question about what MCP stands for or is, rendering the statements irrelevant to the query.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00453, verbose_logs='Statements:\\n[\\n    \"The text does not provide specific information on what MCP stands for.\",\\n    \"The text does not provide specific information on what MCP is.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The statement does not provide any information about what MCP stands for or is, making it irrelevant to the input question.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The statement does not provide any information about what MCP is, making it irrelevant to the input question.\"\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions, indicating that the actual output is perfectly aligned with the retrieval context. Great job maintaining accuracy and consistency!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.005975000000000001, verbose_logs='Truths (limit=None):\\n[\\n    \"MCP reduces development overhead and enables a more interoperable ecosystem.\",\\n    \"MCP benefits the entire community rather than remaining siloed.\",\\n    \"An official MCP registry is being planned to simplify discovery and integration of available tools.\",\\n    \"The centralized repository will make it easier for anyone to access MCP tools.\",\\n    \"MCP can retrieve information from a wide variety of sources, including popular messaging apps.\",\\n    \"GitHub supports a wide variety of actions, including automating processes, extracting or analyzing data, and building AI-based apps.\",\\n    \"The current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\",\\n    \"Several code editors and IDEs have adopted support for the MCP protocol.\",\\n    \"Zed surfaces prompts as slash commands.\",\\n    \"Cursor includes MCP tools in its Composer environment.\",\\n    \"Continue is an open-source AI code assistant for JetBrains and Visual Studio Code.\",\\n    \"Sourcegraph Cody implements MCP through OpenCtx.\",\\n    \"Framework support for MCP has expanded to include integrations for Firebase Genkit and LangChain adapters.\",\\n    \"Superinterface helps developers build in-app AI assistants with MCP functionality.\",\\n    \"The MCP ecosystem comprises a diverse range of servers, including reference servers, official integrations, and community servers.\",\\n    \"Reference servers demonstrate core MCP functionality and serve as implementation examples.\"\\n] \\n \\nClaims:\\n[] \\n \\nVerdicts:\\n[]'), MetricData(name='Contextual Precision', threshold=0.5, success=False, score=0.0, reason='The score is 0.00 because all nodes in the retrieval contexts are irrelevant to the input question. The first node discusses the benefits and future developments of MCP, but it does not define what MCP is. The second node highlights the potential of MCP in various applications, yet it fails to explain what MCP is. The third node lists tools and platforms that support MCP, but it does not define MCP itself. Lastly, the fourth node describes the types of MCP servers without providing a definition of MCP. As a result, none of the nodes provide the necessary information to answer the input question, leading to a score of 0.00.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0060125000000000005, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context discusses the benefits and future developments of MCP, but does not define what MCP is.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context highlights the potential of MCP in various applications but does not explain what MCP is.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context lists tools and platforms that support MCP but does not define MCP itself.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context describes the types of MCP servers but does not provide a definition of MCP.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=True, score=0.8, reason=\"The score is 0.80 because while the retrieval context provides some relevant information about MCP, such as its role in reducing development overhead and enabling interoperability, many statements focus on peripheral aspects like GitHub capabilities and vague references that don't directly explain what MCP is.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.013625, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire community\\\\u2014rather than remaining siloed.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"As MCP continues to progress as a standard, several new developments have appeared on the horizon.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"This centralized repository will make it easier for anyone (not just those willing to scour\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"MCP can retrieve information from a wide variety of sources, including popular messaging apps.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement primarily discusses GitHub\\'s capabilities rather than directly explaining what MCP is.\"\\n            },\\n            {\\n                \"statement\": \"While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Official MCP\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Official MCP\\' is too vague and does not provide any information about what MCP is.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"first-party offering, providing comprehensive support for everything MCP can do.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Several code editors and IDEs have adopted support for the protocol, including Zed (which surfaces prompts as slash commands), Cursor (with MCP tools in its Composer environment), Continue (an open-source AI code assistant for JetBrains and Visual Studio Code), and Sourcegraph Cody (which implements MCP through OpenCtx).\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Framework support has expanded to include integrations for Firebase Genkit, LangChain adapters.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Framework support has expanded to include integrations for Firebase Genkit, LangChain adapters\\' does not directly explain what MCP is.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Examples of MCP servers\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"The MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Reference servers demonstrate core MCP functionality and serve as\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    }\\n]'), MetricData(name='Concise (GEval)', threshold=0.5, success=False, score=0.34274484262376254, reason='The response correctly identifies that the text lacks specific information about what MCP stands for or what it is, addressing the key point of missing information. However, it fails to provide any additional context or analysis, making it overly brief and lacking in depth. The response is concise but does not fully engage with the evaluation steps, particularly in terms of clarity and completeness.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00304, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Compare the Actual Output to the source material, checking if all key points and essential information are included.\",\\n    \"Evaluate the length of the Actual Output to ensure it conveys the message in a concise manner without unnecessary information.\",\\n    \"Assess the clarity of the Actual Output to determine if the essential information is presented clearly and logically.\",\\n    \"Compare multiple versions of Actual Output to see which one most effectively balances conciseness with completeness of essential information.\"\\n] \\n \\nRubric:\\nNone')], conversational=False, multimodal=False, input='What is MCP', actual_output='The text does not provide specific information on what MCP stands for or what it is.', expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and toolsâ€”essentially a â€œuniversal remoteâ€ for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=['MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour', 'history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP', 'first-party offering, providing comprehensive support for everything MCP can do.\\xa0Several code editors and IDEs have adopted support for the protocol, including Zed (which surfaces prompts as slash commands), Cursor (with MCP tools in its Composer environment), Continue (an open-source AI code assistant for JetBrains and Visual Studio Code), and Sourcegraph Cody (which implements MCP through OpenCtx).Framework support has expanded to include integrations for Firebase Genkit, LangChain adapters,', 'and platforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as'], additional_metadata=None), TestResult(name='test_case_1', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addresses the relationship between function calling and Model Context Protocol without any irrelevant statements. Great job on staying focused and relevant!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.005515000000000001, verbose_logs='Statements:\\n[\\n    \"Function calling and the Model Context Protocol (MCP) are used in AI models.\",\\n    \"Function calling allows LLMs to invoke predetermined functions based on user requests.\",\\n    \"Function calling is not mutually exclusive with MCP.\",\\n    \"The MCP is a standardized way for LLMs to connect with external data sources and tools.\",\\n    \"MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\",\\n    \"MCP standardizes how the function calling API feature works.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=0.8333333333333334, reason=\"The score is 0.83 because the actual output inaccurately claims that MCP standardizes the function calling API feature, whereas the retrieval context specifies that MCP standardizes AI applications' connections with external systems, without focusing on the API feature.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0103225, verbose_logs='Truths (limit=None):\\n[\\n    \"The Model Context Protocol (MCP) connects AI applications to context while building on function calling.\",\\n    \"Function calling allows LLMs to invoke predetermined functions based on user requests.\",\\n    \"The Model Context Protocol standardizes how AI applications connect with external systems.\",\\n    \"MCP is released by Anthropic as an open-source protocol.\",\\n    \"MCP eliminates the need for custom integration between LLMs and other applications.\",\\n    \"Function calling is a well-established feature of modern AI models.\",\\n    \"MCP provides a standardized way for LLMs to connect with external data sources and tools.\",\\n    \"Without MCP, using a function call directly with an LLM API requires defining model-specific function schemas.\",\\n    \"MCP is not mutually exclusive with function calling; it standardizes the API feature.\",\\n    \"The Model Context Protocol aims to resolve the NxM problem by universalizing AI application communication with tools and data sources.\"\\n] \\n \\nClaims:\\n[\\n    \"Function calling and the Model Context Protocol (MCP) are both used in AI models.\",\\n    \"Function calling allows LLMs to invoke predetermined functions based on user requests.\",\\n    \"Function calling is not mutually exclusive with MCP.\",\\n    \"The MCP is a standardized way for LLMs to connect with external data sources and tools.\",\\n    \"The MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\",\\n    \"MCP standardizes how the function calling API feature works.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The actual output claims that MCP standardizes how the function calling API feature works, which is not directly supported by the retrieval context. The context states that MCP standardizes how AI applications connect with external systems, not specifically the function calling API feature.\"\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=True, score=0.8055555555555555, reason=\"The score is 0.81 because the first node effectively addresses the input by explaining that MCP 'connects AI apps to context while building on top of function calling,' which is directly relevant. However, the second node, ranked higher than it should be, focuses on security and OAuth scopes, which are not pertinent to the relationship between function calling and MCP. The third and fourth nodes further support the input by describing MCP as a 'universal remote' and clarifying that 'function calling is not mutually exclusive with MCP,' respectively. These relevant nodes contribute positively to the score, but the presence of an irrelevant node in a higher rank prevents a perfect score.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0066575, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context explains that MCP \\'connects AI apps to context while building on top of function calling,\\' which directly relates to the expected output.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context focuses on security and OAuth scopes, which are not relevant to the relationship between function calling and MCP.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context describes MCP as a \\'universal remote\\' for AI apps and mentions it \\'builds on existing function calling,\\' aligning with the expected output.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context clarifies that \\'function calling is not mutually exclusive with MCP\\' and explains how MCP standardizes API interactions, which is pertinent to the expected output.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=True, score=0.6470588235294118, reason=\"The score is 0.65 because while there are relevant statements like 'Function calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models,' many other statements discuss unrelated topics such as 'sensitive data' and 'supply chain attacks,' which do not pertain to the relationship between function calling and the Model Context Protocol.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0159575, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"doesn\\\\u2019t solve the NxM problem by simply replacing the integration methods that came before.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'doesn\\\\u2019t solve the NxM problem by simply replacing the integration methods that came before\\' does not address the relationship between function calling and Model Context Protocol.\"\\n            },\\n            {\\n                \"statement\": \"It connects AI apps to context while building on top of function calling\\\\u2014the primary method for calling APIs from LLMs\\\\u2014to make development simpler and more consistent.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Relationship between function calling & Model Context Protocol\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Function calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Sometimes referred to as \\\\u201ctool\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Sometimes referred to as \\\\u201ctool\\' is incomplete and does not provide information about the relationship between function calling and Model Context Protocol.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement discusses \\'sensitive data\\' and \\'supply chain attacks\\', which are unrelated to the relationship between function calling and the Model Context Protocol.\"\\n            },\\n            {\\n                \"statement\": \"You can learn more about customizing scopes in our OAuth scopes and provider tokens guide.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement refers to \\'customizing scopes\\' and \\'OAuth scopes\\', which do not pertain to the relationship between function calling and the Model Context Protocol.\"\\n            },\\n            {\\n                \"statement\": \"The Model Context Protocol represents a significant leap in connecting LLMs to external systems, standardizing a fragmented ecosystem and potentially resolving the NxM problem.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"By universalizing how AI applications talk to tools and data sources.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"excel at responding to natural language, they\\\\u2019ve been constrained by their isolation from real-world data and systems.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'excel at responding to natural language, they\\\\u2019ve been constrained by their isolation from real-world data and systems\\' does not directly address the relationship between function calling and the Model Context Protocol.\"\\n            },\\n            {\\n                \"statement\": \"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools\\\\u2014essentially a \\\\u201cuniversal remote\\\\u201d for AI apps.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"This means developers\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'This means developers\\' is incomplete and does not provide information relevant to the relationship between function calling and the Model Context Protocol.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"Function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Without MCP, when you use a function call directly with an LLM API, you need to: Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Implement handlers (the actual code that executes when a function is called) for those functions.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Create different implementations for each model you support.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    }\\n]'), MetricData(name='Concise (GEval)', threshold=0.5, success=True, score=0.8238341812583523, reason='The actual output includes essential information about function calling and the Model Context Protocol (MCP), explaining their roles and relationship. It effectively conveys that function calling allows LLMs to invoke functions and that MCP standardizes connections with external data sources, building on function calling. However, the output could be more concise by reducing repetitive phrases and focusing on the core differences and benefits of MCP over function calling.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0032425000000000006, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Step 1: Identify and list all essential information present in the expected output.\",\\n    \"Step 2: Analyze the actual output to ensure all identified essential information from Step 1 is included.\",\\n    \"Step 3: Compare the length of the actual output to the expected standard of conciseness.\",\\n    \"Step 4: Evaluate if the actual output efficiently conveys the necessary information without unnecessary details.\"\\n] \\n \\nRubric:\\nNone')], conversational=False, multimodal=False, input='What is Relationship between function calling & Model Context Protocol', actual_output='Function calling and the Model Context Protocol (MCP) are both used in AI models. Function calling allows LLMs to invoke predetermined functions based on user requests. It is not mutually exclusive with MCP. The MCP, on the other hand, is a standardized way for LLMs to connect with external data sources and tools. It builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means that MCP standardizes how the function calling API feature works.', expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=['doesnâ€™t solve the NxM problem by simply replacing the integration methods that came before. It connects AI apps to context while building on top of function callingâ€”the primary method for calling APIs from LLMsâ€”to make development simpler and more consistent.\\xa0Relationship between function calling & Model Context ProtocolFunction calling, which allows LLMs to invoke predetermined functions based on user requests, is a well-established feature of modern AI models. Sometimes referred to as â€œtool', 'exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources. You can learn more about customizing scopes in our OAuth scopes and provider tokens guide.ConclusionThe Model Context Protocol represents a significant leap in connecting LLMs to external systems, standardizing a fragmented ecosystem and potentially resolving the NxM problem. By universalizing how AI applications talk to tools and data sources,', 'excel at responding to natural language, theyâ€™ve been constrained by their isolation from real-world data and systems.\\xa0The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and toolsâ€”essentially a â€œuniversal remoteâ€ for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means developers', 'use,â€ function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\\xa0Without MCP, when you use a function call directly with an LLM API, you need to:Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.Implement handlers (the actual code that executes when a function is called) for those functions.Create different implementations for each model you support.MCP'], additional_metadata=None), TestResult(name='test_case_2', success=False, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the output perfectly aligns with the input request, providing only the core components of MCP as headings without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.004, verbose_logs='Statements:\\n[\\n    \"The core components of MCP are the Host application.\",\\n    \"The core components of MCP include the MCP client.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=False, score=0.0, reason='The score is 0.00 because the actual output incorrectly claims that the core components of MCP are only the Host application and MCP client, while the retrieval context clearly states that MCP architecture consists of four primary elements, not just two.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00813, verbose_logs='Truths (limit=None):\\n[\\n    \"MCP architecture consists of four primary elements.\",\\n    \"Host applications include Claude Desktop, AI-enhanced IDEs like Cursor, and standard web-based LLM chat interfaces.\",\\n    \"MCP client is integrated within the host application to handle connections with MCP servers.\",\\n    \"Large language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have changed how we interact with information and technology.\",\\n    \"MCP reduces development overhead and enables a more interoperable ecosystem.\",\\n    \"An official MCP registry is being planned to simplify discovery and integration of available tools.\",\\n    \"GitHub supports a wide variety of actions, including automating processes, extracting or analyzing data, and building AI-based apps.\",\\n    \"The current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\"\\n] \\n \\nClaims:\\n[\\n    \"The core components of MCP are: Host application and MCP client.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The claim states that the core components of MCP are only the Host application and MCP client, but the retrieval context mentions that MCP architecture consists of four primary elements, not just two.\"\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=True, score=1.0, reason=\"The score is 1.00 because the relevant nodes are perfectly ranked higher than the irrelevant nodes. The first node provides a direct mention of 'MCP architecture consists of four primary elements' and includes 'MCP client', which aligns with the input. The second node further supports this by discussing the 'MCP client & server ecosystem', directly relating to the expected output. The irrelevant nodes, ranked third and fourth, focus on benefits, future developments, and potential applications of MCP, which do not directly address the core components, thus justifying their lower ranking. Great job on achieving perfect precision!\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0063625, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context mentions \\'MCP architecture consists of four primary elements\\' and lists \\'MCP client\\' as one of them, which aligns with the expected output.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context includes \\'MCP client & server ecosystem\\' which directly relates to \\'MCP Client\\' and \\'MCP Servers\\' in the expected output.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This context focuses on the benefits and future developments of MCP, but does not mention the core components directly.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context discusses the potential of MCP and its applications, but does not list the core components as required by the input.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=False, score=0.18181818181818182, reason=\"The score is 0.18 because the retrieval context primarily contains references and descriptions that do not directly list the core components of MCP. While there is a mention of 'MCP architecture and core components,' it lacks detailed headings or a clear list, making it minimally relevant to the input request for core component headings.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0180125, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"MCP architecture consists of four primary elements:\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Host application: LLMs that interact with users and initiate connections. This includes Claude Desktop, AI-enhanced IDEs like Cursor, and standard web-based LLM chat interfaces.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"MCP client: Integrated within the host application to handle connections with MCP servers, translating\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"a universal way for AI applications to interact with external systems by standardizing context.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'a universal way for AI applications to interact with external systems by standardizing context\\' does not directly list the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Fig: MCP general architecture\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Fig: MCP general architecture\\' is a reference to a figure and does not provide information on the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Fig: MCP core components\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Fig: MCP core components\\' is a reference to a figure and does not provide information on the core components of MCP.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"LLM isolation & the NxM problem\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'LLM isolation & the NxM problem\\' does not relate to the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"MCP architecture and core components\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"How MCP works\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'How MCP works\\' does not directly address the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"MCP client & server ecosystem\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'MCP client & server ecosystem\\' does not directly address the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Security considerations for MCP servers\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Security considerations for MCP servers\\' does not relate to the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Conclusion\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Conclusion\\' does not provide information on the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Identity and auth news. Straight to your inbox.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Identity and auth news. Straight to your inbox.\\' is unrelated to the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Large language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Large language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology.\\' does not pertain to the core components of MCP.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire community\\\\u2014rather than remaining siloed.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement discusses the benefits of MCP but does not list the core components.\"\\n            },\\n            {\\n                \"statement\": \"As MCP continues to progress as a standard, several new developments have appeared on the horizon:\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement introduces new developments but does not provide the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement mentions the \\'Official MCP registry\\' as a development, not a core component.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"history, and more.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'history, and more.\\' is vague and does not provide any information about the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement discusses the potential of MCP to retrieve information but does not list any core components.\"\\n            },\\n            {\\n                \"statement\": \"GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement describes GitHub\\'s capabilities but does not mention the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement discusses the GitHub MCP server as a benchmark but does not list any core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Official MCP\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Official MCP\\' is incomplete and does not provide any information about the core components of MCP.\"\\n            }\\n        ]\\n    }\\n]'), MetricData(name='Concise (GEval)', threshold=0.5, success=True, score=0.7540211996828792, reason=\"The actual output succinctly identifies the key components of MCP as the 'Host application' and 'MCP client,' aligning well with the evaluation steps. It is concise and free of unnecessary verbosity, making it easy to understand. However, it could provide slightly more context to ensure all essential points are covered.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0029300000000000003, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Identify key information in the input and compare it with the actual output to ensure all important details are present.\",\\n    \"Check the actual output for unnecessary verbosity and determine if any information can be removed without losing meaning.\",\\n    \"Compare the length of the actual output with the input to ensure it is more concise while retaining all essential points.\",\\n    \"Evaluate if the information in the actual output is easy to understand and clearly presented without extraneous details.\"\\n] \\n \\nRubric:\\nNone')], conversational=False, multimodal=False, input='What are the core components of MCP, just give the heading', actual_output='The core components of MCP are: Host application and MCP client.', expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=['a universal way for AI applications to interact with external systems by standardizing context.Fig: MCP general architectureCore componentsMCP architecture consists of four primary elements:Fig: MCP core componentsHost application: LLMs that interact with users and initiate connections. This includes Claude Desktop, AI-enhanced IDEs like Cursor, and standard web-based LLM chat interfaces.MCP client: Integrated within the host application to handle connections with MCP servers, translating', 'contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.SubscribeLarge language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models', 'MCP reduces development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour', 'history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP'], additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmczqpfe801ybtrrq5oj64pbf/test-cases')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase] :\n",
    "    test_cases = []\n",
    "    for golden in goldens.goldens:\n",
    "        response = rag_chain.invoke({\"input\": golden.input})\n",
    "        \n",
    "        test_case = LLMTestCase(\n",
    "            input=golden.input,\n",
    "            actual_output=response[\"answer\"],\n",
    "            expected_output=golden.expected_output,\n",
    "            retrieval_context=[doc.page_content for doc in response[\"context\"]]\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.pull(alias=\"test_rag\")\n",
    "\n",
    "data = convert_goldens_to_test_cases(dataset)\n",
    "\n",
    "concise_metrics = GEval(\n",
    "    name=\"Concise\",\n",
    "    model=\"gpt-4o\",  # Specify OpenAI model\n",
    "    criteria=\"Assess if the actual output remains concise while preserving all essential information.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "deepeval.evaluate(\n",
    "    data,\n",
    "    metrics=[\n",
    "        deepeval.metrics.AnswerRelevancyMetric(),\n",
    "        deepeval.metrics.FaithfulnessMetric(),\n",
    "        deepeval.metrics.ContextualPrecisionMetric(),\n",
    "        deepeval.metrics.ContextualRelevancyMetric(),\n",
    "        concise_metrics\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
