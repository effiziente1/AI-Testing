{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Applications üìë"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "**RAG** is a method that imprves the Large Language Models (LLMs) by combining them with external knowledge. RAG allows the model to access and use up-to-date, specific information from external data sources.\n",
    "\n",
    "### When to Use RAG:\n",
    "\n",
    "You can use RAG for example to add a content of the classes to allow students ask questions, or for ecoomerce to include the info of your products.\n",
    "\n",
    "- **Private/Proprietary Data**: When you need to query company documents, internal knowledge bases\n",
    "- **Up-to-date Information**: When the LLM's training data is outdated\n",
    "- **Domain-Specific Knowledge**: For specialized fields not well-represented in training data\n",
    "- **Reducing Hallucinations**: By grounding responses in actual documents\n",
    "- **Cost Efficiency**: Cheaper than fine-tuning for many use cases\n",
    "\n",
    "\n",
    "### Key Components of RAG:\n",
    "\n",
    "1. **Document Store**: A collection of documents containing the knowledge you want to query can be a document, code repo\n",
    "2. **Embeddings**: Vector representations of text chunks that capture semantic meaning\n",
    "3. **Vector Store**: A database that stores embeddings and enables similarity search\n",
    "4. **Retriever**: Finds the most relevant documents based on the query\n",
    "5. **Generator**: The LLM that generates answers using retrieved context\n",
    "\n",
    "### How RAG Works:\n",
    "\n",
    "1. The info added to the LLM is break down into chunks and converted to vector\n",
    "2. The vectors are stored in a database \n",
    "3. The user submits a prompt\n",
    "5. The system tranforms into a numerical format (vector)\n",
    "5. Use the vector to search into the knowledge base \n",
    "6. The info is passed to the language model\n",
    "7. The answer is returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Application Example\n",
    "\n",
    "In this example the RAG will add some info about MCP (Model Context Protocol) and after we can check that the correct answer and context it's returned\n",
    "\n",
    "1. **Loads data** from the website https://www.descope.com/learn/post/mcp\n",
    "2. **Chunks the text** into smaller, manageable chunks\n",
    "3. **Creates embeddings** a embedding is the numerical vector representation of the text that caputre the semantic meaning and stores them in a vector database\n",
    "4. **Retrieves relevant chunks** when answering questions\n",
    "5. **Generates answers** using the retrieved context\n",
    "\n",
    "This approach allows us to answer questions about MCP even though the base LLM might not have been trained on this specific information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: \n",
    "\n",
    "We will use:\n",
    "\n",
    "- **LangChain** A framework for developing application powered by languages modesl\n",
    "- **LangSmith** is LangChain's platform for debugging, testing, and monitoring LLM applications.\n",
    "- **OpenAI** The model to add the new info about MCP\n",
    "- **DeepEval** To test the model\n",
    "- **Ollama* To use free models from your computer\n",
    "\n",
    "### API Keys and Environment Variables\n",
    "\n",
    "Before we start, we need to set up our API keys. This notebook uses:\n",
    "- **OpenAI API Key**: Required for embeddings and LLM generation\n",
    "- **LangSmith API Key**: Optional, but recommended for debugging and tracing\n",
    "\n",
    "You can get a free API key at [https://smith.langchain.com/](https://smith.langchain.com/)\n",
    "\n",
    "You need to add this API key in a .env file or set as environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded environment variables from .env file\n",
      "‚úÖ OpenAI API key loaded from environment\n",
      "‚úÖ Confident API key loaded from environment\n",
      "‚úÖ LangSmith API key loaded from environment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üéâü•≥ Congratulations! You've successfully logged in! üôå \n",
       "</pre>\n"
      ],
      "text/plain": [
       "üéâü•≥ Congratulations! You've successfully logged in! üôå \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import deepeval\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Loaded environment variables from .env file\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    print(\"Will use manual input for API keys...\")\n",
    "\n",
    "# Set up OpenAI API Key (Required)\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key loaded from environment\")\n",
    "\n",
    "# Set up Confident API Key (Required for DeepEval)\n",
    "if \"CONFIDENT_API_KEY\" not in os.environ:\n",
    "    os.environ[\"CONFIDENT_API_KEY\"] = getpass(\"Enter your Confident API key: \")\n",
    "else:\n",
    "    print(\"‚úÖ Confident API key loaded from environment\")\n",
    "\n",
    "# Set up LangSmith API Key (Optional - suppresses warning)\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = \"not-needed\"  # Prevents the warning\n",
    "else:\n",
    "    print(\"‚úÖ LangSmith API key loaded from environment\")\n",
    "\n",
    "# Disable Confident AI verbose trace logging to avoid SSL certificate errors\n",
    "os.environ[\"CONFIDENT_TRACE_VERBOSE\"] = \"NO\"\n",
    "\n",
    "api_key = os.getenv(\"CONFIDENT_API_KEY\")\n",
    "deepeval.login(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the next python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process Documents\n",
    "\n",
    "The first step in building a RAG application is to load and process your documents. This involves:\n",
    "\n",
    "1. **Loading documents** from various sources (web, PDFs, databases, etc.)\n",
    "2. **Splitting text** into smaller chunks for better retrieval\n",
    "3. **Creating embeddings** (vector representations) of each chunk\n",
    "4. **Storing embeddings** in a vector database for fast similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n",
      "Split into 24 chunks\n",
      "Created vector store with 24 vectors\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load documents from the web\n",
    "loader = WebBaseLoader(\"https://www.descope.com/learn/post/mcp\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(f\"Split into {len(splits)} chunks\")\n",
    "print(f\"Created vector store with {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create RAG Chain\n",
    "\n",
    "Now we'll create a RAG chain that combines retrieval with generation. The chain will:\n",
    "1. Take a user question\n",
    "2. Retrieve relevant documents from our vector store\n",
    "3. Format those documents as context\n",
    "4. Pass both context and question to the LLM\n",
    "5. Generate an answer based on the retrieved context\n",
    "\n",
    "#### Using LangChain Hub Prompts\n",
    "\n",
    "LangChain Hub provides pre-built, tested prompts for common use cases. The `rlm/rag-prompt` is a popular RAG prompt that instructs the model to:\n",
    "- Answer based on the provided context\n",
    "- Say \"I don't know\" if the answer isn't in the context\n",
    "- Keep answers concise (3 sentences max)\n",
    "\n",
    "In the next examle we will use a free model llama3.2 \n",
    "\n",
    "The temperature controls the randomness and creativity of the model's outputs. With lower temperature < .2 will probably returns the same words always. with higher value for example 0.8 will use more random choices and will return results with more variants. \n",
    "\n",
    "The max_tokens sets the maximun lenght of the model's repsonse. \n",
    "\n",
    "RAG applications generally use low temperature (0 to 0.3) for accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.3,\n",
    "    max_token=500\n",
    ")\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm, retriever=vectorstore.as_retriever(), prompt=prompt\n",
    ")\n",
    "\n",
    "query = \"What is Model Context Protocol?\"\n",
    "result = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG that returns the context\n",
    "\n",
    "LangChain provides multiple ways to create RAG chains. You can use the `create_retrieval_chain` to return the answer and the context (the documents that were added)\n",
    "\n",
    "1. **Automatically handles retrieval**: The chain manages document retrieval internally\n",
    "2. **Returns structured output**: Includes both the answer and the retrieved context\n",
    "3. **Uses a different prompt template**: The `langchain-ai/retrieval-qa-chat` prompt is optimized for conversational interactions\n",
    "\n",
    "This approach is useful when you want:\n",
    "- More control over the retrieval process\n",
    "- Access to both the answer and the source documents\n",
    "- A more conversational interaction style\n",
    "\n",
    "In this example I am using the OpenAI that will return better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Model Context Protocol?',\n",
       " 'context': [Document(id='8cf85fec-cffc-4b76-9930-90142262458a', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models excel at responding to natural language, they‚Äôve been constrained by their isolation from real-world data and systems.\\xa0The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means developers can build more capable, context-aware applications without reinventing the wheel for each combination of AI model and external system.This guide explains the Model Context Protocol‚Äôs architecture and capabilities, how it solves the inherent challenges of AI integration, and how you can begin using it to build better AI apps'),\n",
       "  Document(id='25a7f8cd-e246-4b92-8111-c90633762798', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content=\"What Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin us for the world's largest MCP hackathon! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.SubscribeLarge language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models excel at responding to\"),\n",
       "  Document(id='794e0146-6abf-4d92-bd24-060b9bcebae3', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='the protocol, regardless of the underlying model vendor.MCP architecture and core componentsThe Model Context Protocol uses a client-server architecture partially inspired by the Language Server Protocol (LSP), which helps different programming languages connect with a wide range of dev tools. Similarly, the aim of MCP is to provide a universal way for AI applications to interact with external systems by standardizing context.Fig: MCP general architectureCore componentsMCP architecture consists of four primary elements:Fig: MCP core componentsHost application: LLMs that interact with users and initiate connections. This includes Claude Desktop, AI-enhanced IDEs like Cursor, and standard web-based LLM chat interfaces.MCP client: Integrated within the host application to handle connections with MCP servers, translating between the host‚Äôs requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds'),\n",
       "  Document(id='171f2b10-09a2-42b3-8337-68a8c29463f8', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='important checkpoint against automated exploits. However, this protection depends on clear permission prompts that help users make informed decisions‚Äîand a transparent understanding of the proposed scopes.This responsibility largely falls on server developers, who should strictly follow the principle of least privilege. Ideally, MCP servers will request only the minimum access necessary for their functionality. This ensures servers aren‚Äôt accidentally exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources. You can learn more about customizing scopes in our OAuth scopes and provider tokens guide.ConclusionThe Model Context Protocol represents a significant leap in connecting LLMs to external systems, standardizing a fragmented ecosystem and potentially resolving the NxM problem. By universalizing how AI applications talk to tools and data sources, MCP reduces development overhead and enables')],\n",
       " 'answer': 'The Model Context Protocol (MCP) is a standardized way for Large Language Models (LLMs) to connect with external data sources and tools. It is essentially a \"universal remote\" for AI apps. Released by Anthropic as an open-source protocol, MCP eliminates the need for custom integration between LLMs and other apps, allowing developers to build more capable, context-aware applications.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\", \n",
    "    temperature=0.3,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "rag_chain = create_retrieval_chain(vectorstore.as_retriever(), combine_docs_chain)\n",
    "\n",
    "rag_chain.invoke({\"input\": \"What is Model Context Protocol?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RetrievalQA Chain\n",
    "\n",
    "Here's yet another way to create a RAG chain using the `RetrievalQA` class. This is a more traditional approach that:\n",
    "\n",
    "1. **Simplifies chain creation**: Combines retrieval and QA in a single class\n",
    "2. **Uses the same prompt**: We can reuse the `rlm/rag-prompt` from LangChain Hub\n",
    "3. **Returns structured output**: The response includes both the query and the result\n",
    "\n",
    "Key differences:\n",
    "- Uses `query` as the input key instead of `question` or `input`\n",
    "- Returns a dictionary with `query` and `result` keys\n",
    "- Handles the retrieval and formatting internally\n",
    "\n",
    "This approach is ideal when you want a simple, straightforward RAG implementation without complex chain composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Context in RAG Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved documents: 4\n",
      "\n",
      "Retrieved documents:\n",
      "\n",
      "Document 1:\n",
      "Content: changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models excel at responding to natu...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 2:\n",
      "Content: What Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin us for the world's largest MCP hackathon! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersRes...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 3:\n",
      "Content: the protocol, regardless of the underlying model vendor.MCP architecture and core componentsThe Model Context Protocol uses a client-server architecture partially inspired by the Language Server Proto...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 4:\n",
      "Content: important checkpoint against automated exploits. However, this protection depends on clear permission prompts that help users make informed decisions‚Äîand a transparent understanding of the proposed sc...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Formatted context (first 500 chars):\n",
      "changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models excel at responding to natural language, they‚Äôve been constrained by their isolation from real-world data and systems.¬†The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Ant...\n",
      "\n",
      "The prompt receives:\n",
      "- context: 3989 characters of retrieved text\n",
      "- question: What is Model Context Protocol?\n"
     ]
    }
   ],
   "source": [
    "# Define the format_docs function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format a list of documents into a single string for context.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# When using hub.pull(\"rlm/rag-prompt\"), the prompt expects:\n",
    "# - context: The retrieved documents formatted as text\n",
    "# - question: The user's question\n",
    "\n",
    "# Let's see how the context is passed through the chain\n",
    "question = \"What is Model Context Protocol?\"\n",
    "\n",
    "# Get the retrieved documents\n",
    "retrieved_docs = vectorstore.as_retriever().invoke(question)\n",
    "print(f\"Number of retrieved documents: {len(retrieved_docs)}\")\n",
    "print(\"\\nRetrieved documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "# Format the documents as context\n",
    "context = format_docs(retrieved_docs)\n",
    "print(f\"\\nFormatted context (first 500 chars):\\n{context[:500]}...\")\n",
    "\n",
    "# This is what gets passed to the prompt\n",
    "print(f\"\\nThe prompt receives:\")\n",
    "print(f\"- context: {len(context)} characters of retrieved text\")\n",
    "print(f\"- question: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Testing RAG Application with DeepEval\n",
    "\n",
    "Now that we have a working RAG application, we need to evaluate its performance. This is crucial because:\n",
    "\n",
    "1. **RAG Quality Varies**: The quality of answers depends on retrieval accuracy and generation quality\n",
    "2. **No Ground Truth**: Unlike traditional ML, we often don't have exact \"correct\" answers\n",
    "3. **Multiple Dimensions**: We need to evaluate relevance, accuracy, completeness, and more\n",
    "\n",
    "#### What is DeepEval?\n",
    "\n",
    "DeepEval is an open-source framework for evaluating LLM applications. It provides:\n",
    "- Pre-built metrics for common evaluation needs\n",
    "- Custom metric creation capabilities\n",
    "- Integration with popular LLM frameworks\n",
    "- Detailed evaluation reports\n",
    "\n",
    "#### Creating Test Cases\n",
    "\n",
    "A test case in DeepEval consists of:\n",
    "- **Input**: The question asked to the RAG system\n",
    "- **Actual Output**: What the RAG system returned\n",
    "- **Expected Output**: What we expect the system to return (optional)\n",
    "- **Retrieval Context**: The documents retrieved by the RAG system (important for RAG metrics!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "question = \"What is Model Context Protocol?\"\n",
    "test_case = LLMTestCase(\n",
    "  input=question,\n",
    "  actual_output=qa_chain.invoke({\"query\": question})[\"result\"],\n",
    "  expected_output=\"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    ")\n",
    "\n",
    "test_cases = [test_case]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Evaluation Metrics\n",
    "\n",
    "DeepEval provides various metrics to evaluate RAG systems:\n",
    "\n",
    "#### 1. GEval (General Evaluation)\n",
    "GEval is a flexible metric that uses LLMs to evaluate outputs based on custom criteria. You can define:\n",
    "- **Name**: A descriptive name for your metric\n",
    "- **Criteria**: What the LLM should evaluate\n",
    "- **Evaluation Parameters**: Which parts of the test case to evaluate\n",
    "\n",
    "#### 2. Built-in Metrics\n",
    "- **Answer Relevancy**: Measures if the answer addresses the question\n",
    "- **Faithfulness**: Checks if the answer is grounded in the retrieved context\n",
    "- **Contextual Precision**: Evaluates if relevant docs are ranked higher\n",
    "- **Contextual Relevancy**: Measures how relevant the retrieved context is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "# Custom metric to evaluate conciseness\n",
    "# This uses an LLM to judge if the output is concise while complete\n",
    "concise_metrics = GEval(\n",
    "    name = \"Concise\",\n",
    "    criteria=\"Assess if the actual output remains concise while preserving all essential information.\",\n",
    "    \n",
    "    # Only evaluate the actual output (not comparing with expected)\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Custom metric to evaluate completeness\n",
    "# This checks if all key information is retained in the output\n",
    "completeness_metrics = GEval(\n",
    "    name = \"Completeness\",\n",
    "    criteria=\"Assess whether the actual output retains all the key information from the input\",\n",
    "    \n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to evauate with a local model un comment and execute the next command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!deepeval set-ollama deepseek-r1:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with GEval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Evaluation\n",
    "\n",
    "Now let's run our evaluation with multiple metrics. DeepEval will:\n",
    "1. Execute each metric on our test case\n",
    "2. Provide detailed scores and explanations\n",
    "3. Show which metrics passed or failed based on thresholds\n",
    "\n",
    "Note: Since our test case doesn't include `retrieval_context`, metrics like Faithfulness and Contextual Precision won't work properly. In a real RAG evaluation, you should always include the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Completeness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mCompleteness \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Concise </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mConcise \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3810633ef5564f849817c1f76c1673f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Completeness [GEval] (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The actual output accurately preserves all key information from the input: it identifies MCP as an open-source protocol by Anthropic, describes its purpose in connecting LLMs to external data sources and tools, notes the standardized approach and elimination of custom code, mentions the client-server architecture inspired by LSP, and highlights security features like least privilege and OAuth scopes. No key information is missing or altered, and the response is concise as requested., error: None)\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and addressed the question directly without any irrelevant information. Great job staying focused and concise!, error: None)\n",
      "  - ‚úÖ Concise [GEval] (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The response is concise, limited to three sentences as specified, and preserves all essential information from the original content: MCP's purpose, open-source nature, origin (Anthropic), standardized integration, client-server architecture inspired by LSP, and security features like least privilege and OAuth scopes. No unnecessary information is included, and no key details are omitted, demonstrating an excellent balance of conciseness and completeness., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Model Context Protocol?\n",
      "  - actual output: <think>\n",
      "Okay, the user is asking about the Model Context Protocol (MCP). Let me check the provided contexts to find the answer.\n",
      "\n",
      "First context mentions that MCP is a standardized way for LLMs to connect with external data sources and tools, acting as a \"universal remote\" for AI apps. It's open-source from Anthropic, solving integration challenges by eliminating custom code between models and apps.\n",
      "\n",
      "Another context explains that MCP uses a client-server architecture inspired by LSP, aiming to standardize AI app interactions with external systems. The core components include host applications, MCP clients, servers, and security aspects like OAuth scopes.\n",
      "\n",
      "The third context talks about security considerations, emphasizing the principle of least privilege and protecting against supply chain attacks. The conclusion highlights MCP's role in connecting LLMs to systems, reducing development overhead.\n",
      "\n",
      "Putting this together: MCP is a protocol that allows LLMs to connect with external tools and data sources using a standardized approach. It's designed to simplify integration by avoiding custom code, with a client-server architecture and security measures. The answer should be concise, in three sentences max.\n",
      "</think>\n",
      "\n",
      "The Model Context Protocol (MCP) is an open-source protocol by Anthropic that enables large language models (LLMs) to connect with external data sources and tools using a standardized approach. It simplifies integration by eliminating the need for custom code between LLMs and applications, leveraging a client-server architecture inspired by the Language Server Protocol (LSP). MCP enhances security through principles like least privilege and OAuth scopes, ensuring safe and efficient interactions between AI systems and external resources.\n",
      "  - expected output: The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Completeness [GEval]: 100.00% pass rate\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Concise [GEval]: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Done üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmeryipf604xr14dy7f0kxzxq/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmeryipf604xr14dy7f0kxzxq/compa</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmeryipf604xr14dy7f0kxzxq/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">re-test-results</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Done üéâ! View results on \n",
       "\u001b]8;id=764115;https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmeryipf604xr14dy7f0kxzxq/compare-test-results\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmeryipf604xr14dy7f0kxzxq/compa\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=764115;https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmeryipf604xr14dy7f0kxzxq/compare-test-results\u001b\\\u001b[4;94mre-test-results\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Completeness [GEval]', threshold=0.5, success=True, score=1.0, reason='The actual output accurately preserves all key information from the input: it identifies MCP as an open-source protocol by Anthropic, describes its purpose in connecting LLMs to external data sources and tools, notes the standardized approach and elimination of custom code, mentions the client-server architecture inspired by LSP, and highlights security features like least privilege and OAuth scopes. No key information is missing or altered, and the response is concise as requested.', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.0028699999999999993, verbose_logs='Criteria:\\nAssess whether the actual output retains all the key information from the input \\n \\nEvaluation Steps:\\n[\\n    \"Identify all key information present in the input.\",\\n    \"Compare each actual output to the input to check if all key information is retained.\",\\n    \"Determine if any key information is missing or altered in each actual output.\",\\n    \"Rank the actual outputs based on how completely and accurately they preserve the key information from the input.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 1.0'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer was fully relevant and addressed the question directly without any irrelevant information. Great job staying focused and concise!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.005494000000000001, verbose_logs='Statements:\\n[\\n    \"The Model Context Protocol (MCP) is an open-source protocol by Anthropic.\",\\n    \"MCP enables large language models (LLMs) to connect with external data sources and tools using a standardized approach.\",\\n    \"MCP simplifies integration by eliminating the need for custom code between LLMs and applications.\",\\n    \"MCP leverages a client-server architecture inspired by the Language Server Protocol (LSP).\",\\n    \"MCP enhances security through principles like least privilege and OAuth scopes.\",\\n    \"MCP ensures safe and efficient interactions between AI systems and external resources.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Concise [GEval]', threshold=0.5, success=True, score=1.0, reason=\"The response is concise, limited to three sentences as specified, and preserves all essential information from the original content: MCP's purpose, open-source nature, origin (Anthropic), standardized integration, client-server architecture inspired by LSP, and security features like least privilege and OAuth scopes. No unnecessary information is included, and no key details are omitted, demonstrating an excellent balance of conciseness and completeness.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.00285, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Compare the length of each Actual Output to determine conciseness.\",\\n    \"Check if all essential information from the original content is preserved in each Actual Output.\",\\n    \"Evaluate if any Actual Output omits necessary details or includes unnecessary information.\",\\n    \"Rank the Actual Outputs by how well they balance conciseness with completeness of essential information.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 1.0')], conversational=False, multimodal=False, input='What is Model Context Protocol?', actual_output='<think>\\nOkay, the user is asking about the Model Context Protocol (MCP). Let me check the provided contexts to find the answer.\\n\\nFirst context mentions that MCP is a standardized way for LLMs to connect with external data sources and tools, acting as a \"universal remote\" for AI apps. It\\'s open-source from Anthropic, solving integration challenges by eliminating custom code between models and apps.\\n\\nAnother context explains that MCP uses a client-server architecture inspired by LSP, aiming to standardize AI app interactions with external systems. The core components include host applications, MCP clients, servers, and security aspects like OAuth scopes.\\n\\nThe third context talks about security considerations, emphasizing the principle of least privilege and protecting against supply chain attacks. The conclusion highlights MCP\\'s role in connecting LLMs to systems, reducing development overhead.\\n\\nPutting this together: MCP is a protocol that allows LLMs to connect with external tools and data sources using a standardized approach. It\\'s designed to simplify integration by avoiding custom code, with a client-server architecture and security measures. The answer should be concise, in three sentences max.\\n</think>\\n\\nThe Model Context Protocol (MCP) is an open-source protocol by Anthropic that enables large language models (LLMs) to connect with external data sources and tools using a standardized approach. It simplifies integration by eliminating the need for custom code between LLMs and applications, leveraging a client-server architecture inspired by the Language Server Protocol (LSP). MCP enhances security through principles like least privilege and OAuth scopes, ensuring safe and efficient interactions between AI systems and external resources.', expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None, additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmeryipf604xr14dy7f0kxzxq/compare-test-results')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepeval.metrics\n",
    "\n",
    "deepeval.evaluate(test_cases=test_cases, metrics=[\n",
    "    completeness_metrics,\n",
    "    deepeval.metrics.AnswerRelevancyMetric(),\n",
    "    concise_metrics\n",
    "] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Test Dataset\n",
    "\n",
    "For more comprehensive testing, let's create a dataset with multiple test cases. DeepEval allows you to:\n",
    "\n",
    "1. **Create Golden datasets**: These are test cases with expected outputs that serve as ground truth\n",
    "2. **Push datasets to the cloud**: Store and version your test datasets\n",
    "3. **Pull datasets**: Retrieve datasets for consistent testing across teams\n",
    "\n",
    "#### Golden Test Cases\n",
    "\n",
    "Golden test cases are particularly useful for:\n",
    "- Regression testing\n",
    "- Comparing different model versions\n",
    "- Establishing baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚úÖ Dataset successfully pushed to Confident AI! View at \n",
       "<a href=\"https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/datasets/cmerygdhn04wy14dygywfitaa\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/datasets/cmerygdhn04wy14dygywfitaa</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚úÖ Dataset successfully pushed to Confident AI! View at \n",
       "\u001b]8;id=110596;https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/datasets/cmerygdhn04wy14dygywfitaa\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/datasets/cmerygdhn04wy14dygywfitaa\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"What is MCP\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is Relationship between function calling & Model Context Protocol\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are the core components of MCP, just give the heading\",\n",
    "        \"reference\":\"\"\" \n",
    "                    - MCP Client\n",
    "                    - MCP Servers\n",
    "                    - Protocol Handshake\n",
    "                    - Capability Discovery\n",
    "                \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "goldens = []\n",
    "for data in test_data:\n",
    "    golden = Golden(\n",
    "        input=data['input'],\n",
    "        expected_output=data['reference']\n",
    "    )\n",
    "\n",
    "    goldens.append(golden)\n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "\n",
    "dataset.push('test_rag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Comprehensive Evaluation on Multiple Test Cases\n",
    "\n",
    "The code below demonstrates how to evaluate multiple test cases with RAG-specific metrics. Notice how the evaluation includes retrieval context, which is essential for metrics like:\n",
    "\n",
    "- **Faithfulness**: Ensures the answer is grounded in retrieved documents\n",
    "- **Contextual Precision**: Checks if relevant documents are ranked higher\n",
    "- **Contextual Relevancy**: Measures the relevance of retrieved documents\n",
    "\n",
    "These metrics help identify issues like:\n",
    "- Hallucinations (answers not supported by context)\n",
    "- Poor retrieval quality\n",
    "- Irrelevant document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f13d265a21247068aa6f97ed7ac182a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'rag_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m dataset = EvaluationDataset()\n\u001b[32m     24\u001b[39m dataset.pull(alias=\u001b[33m\"\u001b[39m\u001b[33mtest_rag\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m data = \u001b[43mconvert_goldens_to_test_cases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m concise_metrics = GEval(\n\u001b[32m     29\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mConcise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Specify OpenAI model\u001b[39;00m\n\u001b[32m     31\u001b[39m     criteria=\u001b[33m\"\u001b[39m\u001b[33mAssess if the actual output remains concise while preserving all essential information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m deepeval.evaluate(\n\u001b[32m     36\u001b[39m     test_cases=data,\n\u001b[32m     37\u001b[39m     metrics=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     ]\n\u001b[32m     44\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mconvert_goldens_to_test_cases\u001b[39m\u001b[34m(dataset)\u001b[39m\n\u001b[32m     10\u001b[39m test_cases = []\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m golden \u001b[38;5;129;01min\u001b[39;00m dataset.goldens:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     response = \u001b[43mrag_chain\u001b[49m.invoke({\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: golden.input})\n\u001b[32m     14\u001b[39m     test_case = LLMTestCase(\n\u001b[32m     15\u001b[39m         \u001b[38;5;28minput\u001b[39m=golden.input,\n\u001b[32m     16\u001b[39m         actual_output=response[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     17\u001b[39m         expected_output=golden.expected_output,\n\u001b[32m     18\u001b[39m         retrieval_context=[doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m response[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     19\u001b[39m     )\n\u001b[32m     20\u001b[39m     test_cases.append(test_case)\n",
      "\u001b[31mNameError\u001b[39m: name 'rag_chain' is not defined"
     ]
    }
   ],
   "source": [
    "import deepeval\n",
    "import deepeval.metrics\n",
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "def convert_goldens_to_test_cases(dataset: EvaluationDataset) -> List[LLMTestCase] :\n",
    "    test_cases = []\n",
    "    for golden in dataset.goldens:\n",
    "        response = rag_chain.invoke({\"input\": golden.input})\n",
    "        \n",
    "        test_case = LLMTestCase(\n",
    "            input=golden.input,\n",
    "            actual_output=response[\"answer\"],\n",
    "            expected_output=golden.expected_output,\n",
    "            retrieval_context=[doc.page_content for doc in response[\"context\"]]\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.pull(alias=\"test_rag\")\n",
    "\n",
    "data = convert_goldens_to_test_cases(dataset)\n",
    "\n",
    "concise_metrics = GEval(\n",
    "    name=\"Concise\",\n",
    "    model=\"gpt-4o\",  # Specify OpenAI model\n",
    "    criteria=\"Assess if the actual output remains concise while preserving all essential information.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "deepeval.evaluate(\n",
    "    test_cases=data,\n",
    "    metrics=[\n",
    "        deepeval.metrics.AnswerRelevancyMetric(),\n",
    "        deepeval.metrics.FaithfulnessMetric(),\n",
    "        deepeval.metrics.ContextualPrecisionMetric(),\n",
    "        deepeval.metrics.ContextualRelevancyMetric(),\n",
    "        concise_metrics\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
