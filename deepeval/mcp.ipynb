{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad95ef8",
   "metadata": {},
   "source": [
    "### How to test MCP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500a994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t0/hls4830123s9kn1cxgy9l_2r0000gn/T/ipykernel_5777/1621442025.py:32: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'results' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m agent_executor = AgentExecutor(agent=agent, tools=tools,return_intermediate_steps=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     51\u001b[39m query=\u001b[33m\"\u001b[39m\u001b[33mWho is the current president of USA in 2025, just give the name\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m response = agent.invoke(\u001b[33m\"\u001b[39m\u001b[33mWho is the current president of USA in 2025, just give the name\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain/agents/agent.py:1620\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1628\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1629\u001b[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001b[32m   1630\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain/agents/agent.py:1326\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1319\u001b[39m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1324\u001b[39m ) -> Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m         \u001b[43m[\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1336\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain/agents/agent.py:1411\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain/agents/agent.py:1433\u001b[39m, in \u001b[36mAgentExecutor._perform_agent_action\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[39m\n\u001b[32m   1431\u001b[39m         tool_run_kwargs[\u001b[33m\"\u001b[39m\u001b[33mllm_prefix\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1432\u001b[39m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1433\u001b[39m     observation = \u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1434\u001b[39m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1438\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1441\u001b[39m     tool_run_kwargs = \u001b[38;5;28mself\u001b[39m._action_agent.tool_run_logging_kwargs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain_core/tools/base.py:882\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    881\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    883\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    884\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain_core/tools/base.py:851\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m._run):\n\u001b[32m    850\u001b[39m         tool_kwargs = tool_kwargs | {config_param: config}\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m     response = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) != \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain_community/tools/ddg_search/tool.py:74\u001b[39m, in \u001b[36mDuckDuckGoSearchRun._run\u001b[39m\u001b[34m(self, query, run_manager)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run\u001b[39m(\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     70\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     71\u001b[39m     run_manager: Optional[CallbackManagerForToolRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     72\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     73\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain_community/utilities/duckduckgo_search.py:114\u001b[39m, in \u001b[36mDuckDuckGoSearchAPIWrapper.run\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run query through DuckDuckGo and return concatenated results.\"\"\"\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source == \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ddgs_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source == \u001b[33m\"\u001b[39m\u001b[33mnews\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    116\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._ddgs_news(query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/langchain_community/utilities/duckduckgo_search.py:64\u001b[39m, in \u001b[36mDuckDuckGoSearchAPIWrapper._ddgs_text\u001b[39m\u001b[34m(self, query, max_results)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mduckduckgo_search\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDGS\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m DDGS() \u001b[38;5;28;01mas\u001b[39;00m ddgs:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     ddgs_gen = \u001b[43mddgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafesearch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msafesearch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimelimit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ddgs_gen:\n\u001b[32m     73\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ddgs_gen]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/deepeval/venv/lib/python3.12/site-packages/duckduckgo_search/duckduckgo_search.py:247\u001b[39m, in \u001b[36mDDGS.text\u001b[39m\u001b[34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mlite\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    246\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._text_lite(keywords, region, timelimit, max_results)\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresults\u001b[49m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'results' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen3:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"Add two numbers + 20 and return results.\"\n",
    "    return int(a) + int(b) + 20\n",
    "\n",
    "@tool\n",
    "def subtract_numbers(a: int, b: int) -> int:\n",
    "    \"Subtract two numbers and return results.\"\n",
    "    return int(a) - int(b)\n",
    "\n",
    "tools = [add_numbers, subtract_numbers, search_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools= tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        # Placeholders fill up a **list** of messages\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools,return_intermediate_steps=True)\n",
    "query=\"Who is the current president of USA in 2025, just give the name\"\n",
    "agent_executor.invoke({\"input\": query})\n",
    "\n",
    "response = agent.invoke(\"Who is the current president of USA in 2025, just give the name\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "def query_ai_agent(question):\n",
    "    response = agent_executor.invoke({\"input\": question})\n",
    "    intermediate_steps = response['intermediate_steps']\n",
    "    agent_action, results = intermediate_steps[0]\n",
    "    tool = agent_action.tool\n",
    "    tool_input = agent_action.tool_input\n",
    "    \n",
    "    # Clean up the output by removing <think> tags\n",
    "    output = response['output']\n",
    "    if '<think>' in output:\n",
    "        # Remove everything between <think> and </think> tags\n",
    "        import re\n",
    "        output = re.sub(r'<think>.*?</think>\\s*', '', output, flags=re.DOTALL)\n",
    "        response['output'] = output.strip()\n",
    "    \n",
    "    return response, tool, tool_input\n",
    "\n",
    "response,tool, tool_input = query_ai_agent(\"What is the sum of 20 and 40\")\n",
    "print(response)\n",
    "\n",
    "print(tool)\n",
    "\n",
    "print(tool_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b4bc74",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28737c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd392acc1134c9381bf25bb62b1f63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f56e1e7a0842a2bb773fb3b7d4f540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Correctness Score: 1.0, Reason: All expected tools ['add_numbers'] were called (order not considered).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1167e3b6f4242dcb1dc1a8422c762b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Correctness Score: 0.5411796259620383, Reason: The numbers extracted from both the actual output and the expected output are identical: 20, 40, and 80. This shows complete alignment with the evaluation steps.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Relevancy Score: 0.0, Reason: The score is 0.00 because the output incorrectly states the sum of 20 and 40 as 80, which is irrelevant and incorrect. The correct sum is 60.\n",
      "\n",
      "Actual output: The sum of 20 and 40 is 80.\n",
      "Tool input: {'a': 20, 'b': 40}\n",
      "Expected output: the sum of 20 and 40 is 80\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Tool Correctness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mTool Correctness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Custom Tool Output Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mCustom Tool Output Correctness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c924011d0e6f41409dce40b9efe3ec65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âœ… Tool Correctness (score: 1.0, threshold: 0.5, strict: False, evaluation model: None, reason: All expected tools ['add_numbers'] were called (order not considered)., error: None)\n",
      "  - âœ… Custom Tool Output Correctness (GEval) (score: 0.5698107576407214, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The numbers extracted from both the actual output (20, 40, 80) and the expected output (20, 40, 80) are identical, indicating perfect alignment., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the sum of 20 and 40\n",
      "  - actual output: The sum of 20 and 40 is 80.\n",
      "  - expected output: the sum of 20 and 40 is 80\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Tool Correctness: 100.00% pass rate\n",
      "Custom Tool Output Correctness (GEval): 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Done ðŸŽ‰! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcvexa3q00h54907fltbbkri/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcvexa3q00h54907fltbbkri/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcvexa3q00h54907fltbbkri/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141mâœ“\u001b[0m Done ðŸŽ‰! View results on \n",
       "\u001b]8;id=606497;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcvexa3q00h54907fltbbkri/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcvexa3q00h54907fltbbkri/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=606497;https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcvexa3q00h54907fltbbkri/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Tool Correctness', threshold=0.5, success=True, score=1.0, reason=\"All expected tools ['add_numbers'] were called (order not considered).\", strict_mode=False, evaluation_model=None, error=None, evaluation_cost=None, verbose_logs='Expected Tools:\\n[\\n    ToolCall(\\n        name=\"add_numbers\"\\n    )\\n] \\n \\nTools Called:\\n[\\n    ToolCall(\\n        name=\"add_numbers\"\\n    )\\n]'), MetricData(name='Custom Tool Output Correctness (GEval)', threshold=0.5, success=True, score=0.5698107576407214, reason='The numbers extracted from both the actual output (20, 40, 80) and the expected output (20, 40, 80) are identical, indicating perfect alignment.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0013275000000000001, verbose_logs='Criteria:\\nCheck if both outputs contain the same numerical answer.\\n\\n    TASK: Extract the number from each output and compare them.\\n\\n    For example:\\n    - If actual output says \"The sum of 20 and 40 is 80\"\\n    - And expected output says \"the sum of 20 and 40 is 80\"\\n    - Both contain the number 80, so score = 1.0\\n\\n    SCORING:\\n    - If both outputs contain the SAME number: score = 1.0\\n    - If outputs contain DIFFERENT numbers: score = 0.0\\n    - Ignore capitalization, punctuation, and formatting differences \\n \\nEvaluation Steps:\\n[\\n    \"Extract all numbers from the actual output.\",\\n    \"Extract all numbers from the expected output.\",\\n    \"Compare the extracted numbers from both outputs.\",\\n    \"Assign a score of 1.0 if they are the same, else assign a score of 0.0.\"\\n] \\n \\nRubric:\\nNone')], conversational=False, multimodal=False, input='What is the sum of 20 and 40', actual_output='The sum of 20 and 40 is 80.', expected_output='the sum of 20 and 40 is 80', context=None, retrieval_context=None, additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmayt3ndd0alqpck9015mgsm8/evaluation/test-runs/cmcvexa3q00h54907fltbbkri/test-cases')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.test_case import ToolCall\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ToolCorrectnessMetric\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "# Custom GEval metric that understands your tool's behavior\n",
    "custom_correctness_metric = GEval(\n",
    "    name=\"Custom Tool Output Correctness\",\n",
    "    criteria=\"\"\"Check if both outputs contain the same numerical answer.\n",
    "    \n",
    "    TASK: Extract the number from each output and compare them.\n",
    "    \n",
    "    For example:\n",
    "    - If actual output says \"The sum of 20 and 40 is 80\"\n",
    "    - And expected output says \"the sum of 20 and 40 is 80\"\n",
    "    - Both contain the number 80, so score = 1.0\n",
    "    \n",
    "    SCORING:\n",
    "    - If both outputs contain the SAME number: score = 1.0\n",
    "    - If outputs contain DIFFERENT numbers: score = 0.0\n",
    "    - Ignore capitalization, punctuation, and formatting differences\"\"\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT]\n",
    ")\n",
    "\n",
    "def query_ai_agent(question):\n",
    "    response = agent_executor.invoke({\"input\": question})\n",
    "    intermediate_steps = response['intermediate_steps']\n",
    "    agent_action, results = intermediate_steps[0]\n",
    "    tool = agent_action.tool\n",
    "    tool_input = agent_action.tool_input\n",
    "    \n",
    "    # Clean up the output by removing <think> tags\n",
    "    output = response['output']\n",
    "    if '<think>' in output:\n",
    "        # Remove everything between <think> and </think> tags\n",
    "        import re\n",
    "        output = re.sub(r'<think>.*?</think>\\s*', '', output, flags=re.DOTALL)\n",
    "        response['output'] = output.strip()\n",
    "    \n",
    "    return response, tool, tool_input\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"input\" : \"What is the sum of 20 and 40\",\n",
    "        \"expected_output\": \"the sum of 20 and 40 is 80\",\n",
    "        \"tool_called\": [\n",
    "            ToolCall(name=\"add_numbers\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "response,tool, tool_input = query_ai_agent(test_data[0]['input'])\n",
    "\n",
    "test_case = LLMTestCase (\n",
    "    input=test_data[0]['input'],\n",
    "    expected_output=test_data[0]['expected_output'],\n",
    "    tools_called=[ToolCall(name=tool)],\n",
    "    actual_output=response['output'],\n",
    "    expected_tools=[ToolCall(name=\"add_numbers\")]\n",
    ")\n",
    "\n",
    "dataset = EvaluationDataset(test_cases=[test_case])\n",
    "\n",
    "# Test tool correctness\n",
    "toolCorrectMetric = ToolCorrectnessMetric()\n",
    "toolCorrectMetric.measure(test_case=test_case)\n",
    "print(f\"Tool Correctness Score: {toolCorrectMetric.score}, Reason: {toolCorrectMetric.reason}\")\n",
    "\n",
    "# Test with custom metric that understands your tool's behavior\n",
    "custom_correctness_metric.measure(test_case)\n",
    "print(f\"Custom Correctness Score: {custom_correctness_metric.score}, Reason: {custom_correctness_metric.reason}\")\n",
    "\n",
    "# Original answer relevancy (will likely fail because it expects 60)\n",
    "answer_relevancy_metric = AnswerRelevancyMetric()\n",
    "answer_relevancy_metric.measure(test_case)\n",
    "print(f\"Answer Relevancy Score: {answer_relevancy_metric.score}, Reason: {answer_relevancy_metric.reason}\")\n",
    "\n",
    "print(f\"\\nActual output: {response['output']}\")\n",
    "print(f\"Tool input: {tool_input}\")\n",
    "print(f\"Expected output: {test_data[0]['expected_output']}\")\n",
    "\n",
    "# Evaluate with the metrics that make sense for your use case\n",
    "dataset.evaluate(metrics=[toolCorrectMetric, custom_correctness_metric])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
