{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Applications 📑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "**RAG** is a method that imprves the Large Language Models (LLMs) by combining them with external knowledge. RAG allows the model to access and use up-to-date, specific information from external data sources.\n",
    "\n",
    "### When to Use RAG:\n",
    "\n",
    "You can use RAG for example to add a content of the classes to allow students ask questions, or for ecoomerce to include the info of your products.\n",
    "\n",
    "- **Private/Proprietary Data**: When you need to query company documents, internal knowledge bases\n",
    "- **Up-to-date Information**: When the LLM's training data is outdated\n",
    "- **Domain-Specific Knowledge**: For specialized fields not well-represented in training data\n",
    "- **Reducing Hallucinations**: By grounding responses in actual documents\n",
    "- **Cost Efficiency**: Cheaper than fine-tuning for many use cases\n",
    "\n",
    "\n",
    "### Key Components of RAG:\n",
    "\n",
    "1. **Document Store**: A collection of documents containing the knowledge you want to query can be a document, code repo\n",
    "2. **Embeddings**: Vector representations of text chunks that capture semantic meaning\n",
    "3. **Vector Store**: A database that stores embeddings and enables similarity search\n",
    "4. **Retriever**: Finds the most relevant documents based on the query\n",
    "5. **Generator**: The LLM that generates answers using retrieved context\n",
    "\n",
    "### How RAG Works:\n",
    "\n",
    "1. The info added to the LLM is break down into chunks and converted to vector\n",
    "2. The vectors are stored in a database \n",
    "3. The user submits a prompt\n",
    "5. The system tranforms into a numerical format (vector)\n",
    "5. Use the vector to search into the knowledge base \n",
    "6. The info is passed to the language model\n",
    "7. The answer is returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Application Example\n",
    "\n",
    "In this example the RAG will add some info about MCP (Model Context Protocol) and after we can check that the correct answer and context it's returned\n",
    "\n",
    "1. **Loads data** from the website https://www.descope.com/learn/post/mcp\n",
    "2. **Chunks the text** into smaller, manageable chunks\n",
    "3. **Creates embeddings** a embedding is the numerical vector representation of the text that caputre the semantic meaning and stores them in a vector database\n",
    "4. **Retrieves relevant chunks** when answering questions\n",
    "5. **Generates answers** using the retrieved context\n",
    "\n",
    "This approach allows us to answer questions about MCP even though the base LLM might not have been trained on this specific information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: \n",
    "\n",
    "We will use:\n",
    "\n",
    "- **LangChain** A framework for developing application powered by languages modesl\n",
    "- **LangSmith** is LangChain's platform for debugging, testing, and monitoring LLM applications.\n",
    "- **OpenAI** The model to add the new info about MCP\n",
    "- **DeepEval** To test the model\n",
    "- **Ollama* To use free models from your computer\n",
    "\n",
    "### API Keys and Environment Variables\n",
    "\n",
    "Before we start, we need to set up our API keys. This notebook uses:\n",
    "- **OpenAI API Key**: Required for embeddings and LLM generation\n",
    "- **LangSmith API Key**: Optional, but recommended for debugging and tracing\n",
    "\n",
    "You can get a free API key at [https://smith.langchain.com/](https://smith.langchain.com/)\n",
    "\n",
    "You need to add this API key in a .env file or set as environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded environment variables from .env file\n",
      "✅ OpenAI API key loaded from environment\n",
      "✅ Confident API key loaded from environment\n",
      "✅ LangSmith API key loaded from environment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉🥳 Congratulations! You've successfully logged in! 🙌 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "🎉🥳 Congratulations! You've successfully logged in! 🙌 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import deepeval\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"✅ Loaded environment variables from .env file\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    print(\"Will use manual input for API keys...\")\n",
    "\n",
    "# Set up OpenAI API Key (Required)\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key loaded from environment\")\n",
    "\n",
    "# Set up Confident API Key (Required for DeepEval)\n",
    "if \"CONFIDENT_API_KEY\" not in os.environ:\n",
    "    os.environ[\"CONFIDENT_API_KEY\"] = getpass(\"Enter your Confident API key: \")\n",
    "else:\n",
    "    print(\"✅ Confident API key loaded from environment\")\n",
    "\n",
    "# Set up LangSmith API Key (Optional - suppresses warning)\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = \"not-needed\"  # Prevents the warning\n",
    "else:\n",
    "    print(\"✅ LangSmith API key loaded from environment\")\n",
    "\n",
    "api_key = os.getenv(\"CONFIDENT_API_KEY\")\n",
    "deepeval.login(api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the quiet whisper of the dawn,  \\nWhere morning's light begins to yawn,  \\nAwakes the world in soft embrace,  \\nPainting colors, bright and bold, with grace.  \\n\\nThe dew-kissed grass, a jeweled sea,  \\nReflects the dance of winds set free.  \\nAbove, the lark in purest flight,  \\nSings hymns to greet the day and night.  \\n\\nTrees stand tall in leafy gowns,  \\nTheir shadows draped on sleeping towns.  \\nBranches stretch to catch the sun,  \\nIn unity, they rise as one.  \\n\\nThe river flows with silver gleam,  \\nA serpent's path, a woven dream.  \\nIts gentle murmur sings of tales,  \\nOf distant lands and whispered gales.  \\n\\nIn evening's cloak, the sky turns gold,  \\nA tapestry of stories told.  \\nStars appear, a celestial choir,  \\nKindling the night with ancient fire.  \\n\\nSo from dawn's breath to twilight's glow,  \\nA cycle endless, round we go.  \\nEmbrace the dance of time's sweet song,  \\nIn each new day, we find where we belong.  \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from deepeval.tracing import observe\n",
    "client = OpenAI()\n",
    "@observe()\n",
    "def llm_app(query: str) -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "# Call app to send trace to Confident AI\n",
    "llm_app(\"Write me a poem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process Documents\n",
    "\n",
    "The first step in building a RAG application is to load and process your documents. This involves:\n",
    "\n",
    "1. **Loading documents** from various sources (web, PDFs, databases, etc.)\n",
    "2. **Splitting text** into smaller chunks for better retrieval\n",
    "3. **Creating embeddings** (vector representations) of each chunk\n",
    "4. **Storing embeddings** in a vector database for fast similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n",
      "Split into 24 chunks\n",
      "Created vector store with 24 vectors\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load documents from the web\n",
    "loader = WebBaseLoader(\"https://www.descope.com/learn/post/mcp\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(f\"Split into {len(splits)} chunks\")\n",
    "print(f\"Created vector store with {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In a garden where the wildflowers sway,  \\nBeneath the sun's warm, golden ray,  \\nThe whispering breeze tells secrets untold,  \\nOf dreams woven softly, of memories bold.  \\n\\nThe robin sings sweet in morning's embrace,  \\nWhile shadows of daisies dance with grace.  \\nPetals like laughter, in colors so bright,  \\nUnfurling their stories in the heart of the light.  \\n\\nThe brook hums a tune, a melody clear,  \\nCarving through stone like the passage of years.  \\nIt winds 'round the mosses, the roots intertwined,  \\nA tapestry painted by nature's own hand.  \\n\\nAs twilight descends, the stars come alive,  \\nA canvas of wonder where wishes can thrive.  \\nEach flicker a promise, each glow a new dream,  \\nIn the stillness of night, the world softly gleams.  \\n\\nSo linger a while in this moment so rare,  \\nLet the whispers of nature unravel your care.  \\nFor in every petal, in each gentle breeze,  \\nLies the magic of life and the sweet charms of ease.  \""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from deepeval.tracing import (observe, update_current_span)\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "client = OpenAI()\n",
    " \n",
    "@observe(type='llm', model='gpt-4o-mini')\n",
    "def llm_app(query: str) -> str:\n",
    "    res = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    ).choices[0].message.content\n",
    " \n",
    "    update_current_span(test_case=LLMTestCase(input=query, actual_output=res))\n",
    "    return res\n",
    " \n",
    " \n",
    "# Call app to send trace to Confident AI\n",
    "llm_app(\"Write me a poem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create RAG Chain\n",
    "\n",
    "Now we'll create a RAG chain that combines retrieval with generation. The chain will:\n",
    "1. Take a user question\n",
    "2. Retrieve relevant documents from our vector store\n",
    "3. Format those documents as context\n",
    "4. Pass both context and question to the LLM\n",
    "5. Generate an answer based on the retrieved context\n",
    "\n",
    "#### Using LangChain Hub Prompts\n",
    "\n",
    "LangChain Hub provides pre-built, tested prompts for common use cases. The `rlm/rag-prompt` is a popular RAG prompt that instructs the model to:\n",
    "- Answer based on the provided context\n",
    "- Say \"I don't know\" if the answer isn't in the context\n",
    "- Keep answers concise (3 sentences max)\n",
    "\n",
    "In the next examle we will use a free model llama3.2 \n",
    "\n",
    "The temperature controls the randomness and creativity of the model's outputs. With lower temperature < .2 will probably returns the same words always. with higher value for example 0.8 will use more random choices and will return results with more variants. \n",
    "\n",
    "The max_tokens sets the maximun lenght of the model's repsonse. \n",
    "\n",
    "RAG applications generally use low temperature (0 to 0.3) for accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user is asking about the Model Context Protocol (MCP). Let me check the provided contexts to find the answer.\\n\\nFirst context mentions that MCP is a standardized way for LLMs to connect with external data sources and tools, acting as a \"universal remote\" for AI apps. It\\'s open-source from Anthropic, built on function calling to avoid custom integrations. Developers can build context-aware apps without reinventing the wheel.\\n\\nAnother context explains MCP\\'s architecture inspired by LSP, with client-server components. It aims to standardize AI app interactions with external systems, reducing development overhead and solving the NxM problem.\\n\\nThe third context talks about security considerations, like least privilege and permission prompts, ensuring servers request minimal access. It also mentions MCP\\'s role in connecting LLMs to tools/data sources, standardizing the ecosystem.\\n\\nPutting this together: MCP is a protocol that allows LLMs to connect with external systems via a standardized method, reducing integration efforts. It uses a client-server architecture, similar to LSP, and focuses on security by minimizing access. Anthropic developed it as open-source to enable better AI applications.\\n</think>\\n\\nThe Model Context Protocol (MCP) is an open-source protocol by Anthropic that standardizes how large language models (LLMs) connect to external data sources and tools, acting as a \"universal remote\" for AI applications. It uses a client-server architecture inspired by the Language Server Protocol (LSP) to reduce integration complexity and enable context-aware apps. MCP addresses challenges like the NxM problem by universalizing AI interactions with systems while prioritizing security through strict access controls.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import ChatOllama\n",
    "from deepeval.tracing import observe\n",
    "\n",
    "\n",
    "@observe(type='llm', model='qwen3:latest')\n",
    "def local_llms():\n",
    "    return ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.3,\n",
    "    max_token=500\n",
    ")\n",
    "llm = local_llms()\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm, retriever=vectorstore.as_retriever(), prompt=prompt\n",
    ")\n",
    "\n",
    "@observe(name=\"RAG - Local Model\")\n",
    "def invoke_local_rag_chain(query):\n",
    "    return qa_chain.invoke({\"query\": query})[\"result\"]\n",
    "\n",
    "invoke_local_rag_chain(\"What is Model Context Protocol?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG that returns the context\n",
    "\n",
    "LangChain provides multiple ways to create RAG chains. You can use the `create_retrieval_chain` to return the answer and the context (the documents that were added)\n",
    "\n",
    "1. **Automatically handles retrieval**: The chain manages document retrieval internally\n",
    "2. **Returns structured output**: Includes both the answer and the retrieved context\n",
    "3. **Uses a different prompt template**: The `langchain-ai/retrieval-qa-chat` prompt is optimized for conversational interactions\n",
    "\n",
    "This approach is useful when you want:\n",
    "- More control over the retrieval process\n",
    "- Access to both the answer and the source documents\n",
    "- A more conversational interaction style\n",
    "\n",
    "In this example I am using the OpenAI that will return better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Model Context Protocol (MCP) is a standardized way for large language models (LLMs) to connect with external data sources and tools. It serves as a \"universal remote\" for AI applications, allowing developers to build more capable and context-aware applications without needing custom integration for each combination of AI model and external system. Released by Anthropic as an open-source protocol, MCP addresses the challenges of LLM isolation from real-world data and systems, thereby enhancing the interaction between AI applications and external resources.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from deepeval.tracing import observe\n",
    "\n",
    "\n",
    "@observe(name=\"My Metrics\")\n",
    "def invoke_openai_rag_chain(query):\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    # See full prompt at https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\n",
    "    retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "    rag_chain = create_retrieval_chain(vectorstore.as_retriever(), combine_docs_chain)\n",
    "    result = rag_chain.invoke({\"input\": query})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "invoke_openai_rag_chain(\"What is Model Context Protocol?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Model Context Protocol?',\n",
       " 'context': [Document(id='037da5cd-21d4-4b17-83c1-30a9b44d96ba', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models excel at responding to natural language, they’ve been constrained by their isolation from real-world data and systems.\\xa0The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps. This means developers can build more capable, context-aware applications without reinventing the wheel for each combination of AI model and external system.This guide explains the Model Context Protocol’s architecture and capabilities, how it solves the inherent challenges of AI integration, and how you can begin using it to build better AI apps'),\n",
       "  Document(id='3e656569-f09f-4ec1-ae36-74df7d075201', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content=\"What Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin us for the world's largest MCP hackathon! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of contentsTable of ContentsLLM isolation & the NxM problemMCP architecture and core componentsHow MCP worksMCP client & server ecosystemSecurity considerations for MCP serversConclusionIdentity and auth news.  Straight to your inbox.SubscribeLarge language models (LLMs) like Claude, ChatGPT, Gemini, and LlaMA have completely changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models excel at responding to\"),\n",
       "  Document(id='d4bd2320-ee41-4bde-b061-78e905044ee9', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='the protocol, regardless of the underlying model vendor.MCP architecture and core componentsThe Model Context Protocol uses a client-server architecture partially inspired by the Language Server Protocol (LSP), which helps different programming languages connect with a wide range of dev tools. Similarly, the aim of MCP is to provide a universal way for AI applications to interact with external systems by standardizing context.Fig: MCP general architectureCore componentsMCP architecture consists of four primary elements:Fig: MCP core componentsHost application: LLMs that interact with users and initiate connections. This includes Claude Desktop, AI-enhanced IDEs like Cursor, and standard web-based LLM chat interfaces.MCP client: Integrated within the host application to handle connections with MCP servers, translating between the host’s requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds'),\n",
       "  Document(id='ab671f12-8de6-45bb-8d5f-df46eb17fb46', metadata={'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}, page_content='important checkpoint against automated exploits. However, this protection depends on clear permission prompts that help users make informed decisions—and a transparent understanding of the proposed scopes.This responsibility largely falls on server developers, who should strictly follow the principle of least privilege. Ideally, MCP servers will request only the minimum access necessary for their functionality. This ensures servers aren’t accidentally exposed to sensitive data while strengthening resilience against supply chain attacks that leverage unsecured connections between different resources. You can learn more about customizing scopes in our OAuth scopes and provider tokens guide.ConclusionThe Model Context Protocol represents a significant leap in connecting LLMs to external systems, standardizing a fragmented ecosystem and potentially resolving the NxM problem. By universalizing how AI applications talk to tools and data sources, MCP reduces development overhead and enables')],\n",
       " 'answer': 'The Model Context Protocol (MCP) is a standardized framework that allows large language models (LLMs) to connect with external data sources and tools. It serves as a \"universal remote\" for AI applications, enabling developers to create more capable and context-aware applications without needing custom integrations for each combination of AI model and external system. Released by Anthropic as an open-source protocol, MCP aims to address the challenges of LLM isolation from real-world data and systems, thereby facilitating better interaction between AI applications and various tools.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from deepeval.tracing import observe\n",
    "\n",
    "@observe(name=\"RAG - OpenAI Model\")\n",
    "def llm_app(query: str): \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    # See full prompt at https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\n",
    "    retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "    rag_chain = create_retrieval_chain(vectorstore.as_retriever(), combine_docs_chain)\n",
    "    result = rag_chain.invoke({\"input\": query})\n",
    "    return result\n",
    "    \n",
    "query = \"What is Model Context Protocol?\"\n",
    "\n",
    "llm_app(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RetrievalQA Chain\n",
    "\n",
    "Here's yet another way to create a RAG chain using the `RetrievalQA` class. This is a more traditional approach that:\n",
    "\n",
    "1. **Simplifies chain creation**: Combines retrieval and QA in a single class\n",
    "2. **Uses the same prompt**: We can reuse the `rlm/rag-prompt` from LangChain Hub\n",
    "3. **Returns structured output**: The response includes both the query and the result\n",
    "\n",
    "Key differences:\n",
    "- Uses `query` as the input key instead of `question` or `input`\n",
    "- Returns a dictionary with `query` and `result` keys\n",
    "- Handles the retrieval and formatting internally\n",
    "\n",
    "This approach is ideal when you want a simple, straightforward RAG implementation without complex chain composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Context in RAG Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved documents: 4\n",
      "\n",
      "Retrieved documents:\n",
      "\n",
      "Document 1:\n",
      "Content: changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models excel at responding to natu...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 2:\n",
      "Content: What Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightJoin us for the world's largest MCP hackathon! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersRes...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 3:\n",
      "Content: the protocol, regardless of the underlying model vendor.MCP architecture and core componentsThe Model Context Protocol uses a client-server architecture partially inspired by the Language Server Proto...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Document 4:\n",
      "Content: important checkpoint against automated exploits. However, this protection depends on clear permission prompts that help users make informed decisions—and a transparent understanding of the proposed sc...\n",
      "Metadata: {'source': 'https://www.descope.com/learn/post/mcp', 'title': 'What Is the Model Context Protocol (MCP) and How It Works', 'description': 'Learn more about MCP, the open source protocol developed by Anthropic to provide LLMs and AI agents a standardized way to connect with external data sources and tools.', 'language': 'en'}\n",
      "\n",
      "Formatted context (first 500 chars):\n",
      "changed how we interact with information and technology. They can write eloquently, perform deep research, and solve increasingly complex problems. But while typical models excel at responding to natural language, they’ve been constrained by their isolation from real-world data and systems. The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Ant...\n",
      "\n",
      "The prompt receives:\n",
      "- context: 3989 characters of retrieved text\n",
      "- question: What is Model Context Protocol?\n"
     ]
    }
   ],
   "source": [
    "# Define the format_docs function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format a list of documents into a single string for context.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# When using hub.pull(\"rlm/rag-prompt\"), the prompt expects:\n",
    "# - context: The retrieved documents formatted as text\n",
    "# - question: The user's question\n",
    "\n",
    "# Let's see how the context is passed through the chain\n",
    "question = \"What is Model Context Protocol?\"\n",
    "\n",
    "# Get the retrieved documents\n",
    "retrieved_docs = vectorstore.as_retriever().invoke(question)\n",
    "print(f\"Number of retrieved documents: {len(retrieved_docs)}\")\n",
    "print(\"\\nRetrieved documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "# Format the documents as context\n",
    "context = format_docs(retrieved_docs)\n",
    "print(f\"\\nFormatted context (first 500 chars):\\n{context[:500]}...\")\n",
    "\n",
    "# This is what gets passed to the prompt\n",
    "print(f\"\\nThe prompt receives:\")\n",
    "print(f\"- context: {len(context)} characters of retrieved text\")\n",
    "print(f\"- question: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Testing RAG Application with DeepEval\n",
    "\n",
    "Now that we have a working RAG application, we need to evaluate its performance. This is crucial because:\n",
    "\n",
    "1. **RAG Quality Varies**: The quality of answers depends on retrieval accuracy and generation quality\n",
    "2. **No Ground Truth**: Unlike traditional ML, we often don't have exact \"correct\" answers\n",
    "3. **Multiple Dimensions**: We need to evaluate relevance, accuracy, completeness, and more\n",
    "\n",
    "#### What is DeepEval?\n",
    "\n",
    "DeepEval is an open-source framework for evaluating LLM applications. It provides:\n",
    "- Pre-built metrics for common evaluation needs\n",
    "- Custom metric creation capabilities\n",
    "- Integration with popular LLM frameworks\n",
    "- Detailed evaluation reports\n",
    "\n",
    "#### Creating Test Cases\n",
    "\n",
    "A test case in DeepEval consists of:\n",
    "- **Input**: The question asked to the RAG system\n",
    "- **Actual Output**: What the RAG system returned\n",
    "- **Expected Output**: What we expect the system to return (optional)\n",
    "- **Retrieval Context**: The documents retrieved by the RAG system (important for RAG metrics!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "question = \"What is Model Context Protocol?\"\n",
    "test_case = LLMTestCase(\n",
    "  input=question,\n",
    "  actual_output=qa_chain.invoke({\"query\": question})[\"result\"],\n",
    "  expected_output=\"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    ")\n",
    "\n",
    "test_cases = [test_case]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Evaluation Metrics\n",
    "\n",
    "DeepEval provides various metrics to evaluate RAG systems:\n",
    "\n",
    "#### 1. GEval (General Evaluation)\n",
    "GEval is a flexible metric that uses LLMs to evaluate outputs based on custom criteria. You can define:\n",
    "- **Name**: A descriptive name for your metric\n",
    "- **Criteria**: What the LLM should evaluate\n",
    "- **Evaluation Parameters**: Which parts of the test case to evaluate\n",
    "\n",
    "#### 2. Built-in Metrics\n",
    "- **Answer Relevancy**: Measures if the answer addresses the question\n",
    "- **Faithfulness**: Checks if the answer is grounded in the retrieved context\n",
    "- **Contextual Precision**: Evaluates if relevant docs are ranked higher\n",
    "- **Contextual Relevancy**: Measures how relevant the retrieved context is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "# Custom metric to evaluate conciseness\n",
    "# This uses an LLM to judge if the output is concise while complete\n",
    "concise_metrics = GEval(\n",
    "    name = \"Concise\",\n",
    "    criteria=\"Assess if the actual output remains concise while preserving all essential information.\",\n",
    "    \n",
    "    # Only evaluate the actual output (not comparing with expected)\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Custom metric to evaluate completeness\n",
    "# This checks if all key information is retained in the output\n",
    "completeness_metrics = GEval(\n",
    "    name = \"Completeness\",\n",
    "    criteria=\"Assess whether the actual output retains all the key information from the input\",\n",
    "    \n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to evauate with a local model un comment and execute the next command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!deepeval set-ollama deepseek-r1:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with GEval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Evaluation\n",
    "\n",
    "Now let's run our evaluation with multiple metrics. DeepEval will:\n",
    "1. Execute each metric on our test case\n",
    "2. Provide detailed scores and explanations\n",
    "3. Show which metrics passed or failed based on thresholds\n",
    "\n",
    "Note: Since our test case doesn't include `retrieval_context`, metrics like Faithfulness and Contextual Precision won't work properly. In a real RAG evaluation, you should always include the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Completeness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCompleteness \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Concise </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mConcise \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed846445e9b47168f802e53d049ce30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Completeness [GEval] (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The response accurately identifies and preserves all key information from the input: it defines MCP as an open-source protocol by Anthropic, explains its purpose as standardizing LLM connections to external data and tools, describes the client-server architecture inspired by LSP, mentions the reduction of custom integrations (the NxM problem), and includes security considerations like scoped permissions. The answer is concise, well-structured, and covers all critical points from the provided contexts., error: None)\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and addressed the question directly without any irrelevant information. Great job staying focused and concise!, error: None)\n",
      "  - ✅ Concise [GEval] (score: 0.9060086654381816, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The response is highly concise, summarizing the essential information about MCP in three sentences as planned. It covers the protocol's purpose (standardization, universal remote for AI apps), architecture (client-server inspired by LSP), and benefits (reducing integration challenges, addressing the NxM problem, and security via scoped permissions). No unnecessary information is included, and all key details from the contexts are preserved. The only minor shortcoming is that the phrase 'context-aware, scalable AI apps' could be slightly more explicit, but overall, the balance of conciseness and completeness is excellent., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Model Context Protocol?\n",
      "  - actual output: <think>\n",
      "Okay, the user is asking about the Model Context Protocol (MCP). Let me look through the provided contexts to find the answer. \n",
      "\n",
      "First context mentions that MCP is a standardized way for LLMs to connect with external data sources and tools, acting as a \"universal remote\" for AI apps. It's open-source from Anthropic, built on function calling to eliminate custom integrations.\n",
      "\n",
      "Another context explains that MCP uses a client-server architecture inspired by LSP, aiming to standardize AI app interactions with external systems. The core components are host application, MCP client, MCP server, and external systems.\n",
      "\n",
      "The third context talks about security considerations, like permission prompts and least privilege, ensuring servers request minimal access. The conclusion highlights MCP solving the NxM problem by standardizing connections, reducing development overhead.\n",
      "\n",
      "Putting this together, the answer should define MCP, mention its purpose, architecture, and benefits. Need to keep it concise, three sentences max. Make sure to include key points: standardization, connection to external systems, client-server architecture, and reduction of integration challenges.\n",
      "</think>\n",
      "\n",
      "The Model Context Protocol (MCP) is an open-source protocol by Anthropic that standardizes how large language models (LLMs) connect to external data sources and tools, acting as a \"universal remote\" for AI applications. It uses a client-server architecture inspired by the Language Server Protocol (LSP) to streamline integration, reducing the need for custom code between LLMs and external systems. MCP addresses challenges like the NxM problem by enabling context-aware, scalable AI apps while prioritizing security through scoped permissions.\n",
      "  - expected output: The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Completeness [GEval]: 100.00% pass rate\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Concise [GEval]: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Done 🎉! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmerz3nne009twjfsxhpy805n/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmerz3nne009twjfsxhpy805n/compa</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmerz3nne009twjfsxhpy805n/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">re-test-results</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Done 🎉! View results on \n",
       "\u001b]8;id=764859;https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmerz3nne009twjfsxhpy805n/compare-test-results\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmerz3nne009twjfsxhpy805n/compa\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=764859;https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmerz3nne009twjfsxhpy805n/compare-test-results\u001b\\\u001b[4;94mre-test-results\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Completeness [GEval]', threshold=0.5, success=True, score=1.0, reason='The response accurately identifies and preserves all key information from the input: it defines MCP as an open-source protocol by Anthropic, explains its purpose as standardizing LLM connections to external data and tools, describes the client-server architecture inspired by LSP, mentions the reduction of custom integrations (the NxM problem), and includes security considerations like scoped permissions. The answer is concise, well-structured, and covers all critical points from the provided contexts.', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.0029059999999999997, verbose_logs='Criteria:\\nAssess whether the actual output retains all the key information from the input \\n \\nEvaluation Steps:\\n[\\n    \"Identify all key information present in the input.\",\\n    \"Compare each actual output to the input to check if all key information is retained.\",\\n    \"Determine if any key information is missing or altered in each actual output.\",\\n    \"Rank the actual outputs based on how completely and accurately they preserve the key information from the input.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 1.0'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer was fully relevant and addressed the question directly without any irrelevant information. Great job staying focused and concise!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.005788, verbose_logs='Statements:\\n[\\n    \"The Model Context Protocol (MCP) is an open-source protocol by Anthropic.\",\\n    \"MCP standardizes how large language models (LLMs) connect to external data sources and tools.\",\\n    \"MCP acts as a \"universal remote\" for AI applications.\",\\n    \"MCP uses a client-server architecture inspired by the Language Server Protocol (LSP).\",\\n    \"The architecture streamlines integration and reduces the need for custom code between LLMs and external systems.\",\\n    \"MCP addresses challenges like the NxM problem by enabling context-aware, scalable AI apps.\",\\n    \"MCP prioritizes security through scoped permissions.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Concise [GEval]', threshold=0.5, success=True, score=0.9060086654381816, reason=\"The response is highly concise, summarizing the essential information about MCP in three sentences as planned. It covers the protocol's purpose (standardization, universal remote for AI apps), architecture (client-server inspired by LSP), and benefits (reducing integration challenges, addressing the NxM problem, and security via scoped permissions). No unnecessary information is included, and all key details from the contexts are preserved. The only minor shortcoming is that the phrase 'context-aware, scalable AI apps' could be slightly more explicit, but overall, the balance of conciseness and completeness is excellent.\", strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.0031579999999999998, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Compare the length of each Actual Output to determine conciseness.\",\\n    \"Check if all essential information from the original content is preserved in each Actual Output.\",\\n    \"Evaluate if any Actual Output omits necessary details or includes unnecessary information.\",\\n    \"Rank the Actual Outputs by how well they balance conciseness with completeness of essential information.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.9060086654381816')], conversational=False, multimodal=False, input='What is Model Context Protocol?', actual_output='<think>\\nOkay, the user is asking about the Model Context Protocol (MCP). Let me look through the provided contexts to find the answer. \\n\\nFirst context mentions that MCP is a standardized way for LLMs to connect with external data sources and tools, acting as a \"universal remote\" for AI apps. It\\'s open-source from Anthropic, built on function calling to eliminate custom integrations.\\n\\nAnother context explains that MCP uses a client-server architecture inspired by LSP, aiming to standardize AI app interactions with external systems. The core components are host application, MCP client, MCP server, and external systems.\\n\\nThe third context talks about security considerations, like permission prompts and least privilege, ensuring servers request minimal access. The conclusion highlights MCP solving the NxM problem by standardizing connections, reducing development overhead.\\n\\nPutting this together, the answer should define MCP, mention its purpose, architecture, and benefits. Need to keep it concise, three sentences max. Make sure to include key points: standardization, connection to external systems, client-server architecture, and reduction of integration challenges.\\n</think>\\n\\nThe Model Context Protocol (MCP) is an open-source protocol by Anthropic that standardizes how large language models (LLMs) connect to external data sources and tools, acting as a \"universal remote\" for AI applications. It uses a client-server architecture inspired by the Language Server Protocol (LSP) to streamline integration, reducing the need for custom code between LLMs and external systems. MCP addresses challenges like the NxM problem by enabling context-aware, scalable AI apps while prioritizing security through scoped permissions.', expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None, additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/evaluation/test-runs/cmerz3nne009twjfsxhpy805n/compare-test-results')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepeval.metrics\n",
    "\n",
    "deepeval.evaluate(test_cases=test_cases, metrics=[\n",
    "    completeness_metrics,\n",
    "    deepeval.metrics.AnswerRelevancyMetric(),\n",
    "    concise_metrics\n",
    "] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Test Dataset\n",
    "\n",
    "For more comprehensive testing, let's create a dataset with multiple test cases. DeepEval allows you to:\n",
    "\n",
    "1. **Create Golden datasets**: These are test cases with expected outputs that serve as ground truth\n",
    "2. **Push datasets to the cloud**: Store and version your test datasets\n",
    "3. **Pull datasets**: Retrieve datasets for consistent testing across teams\n",
    "\n",
    "#### Golden Test Cases\n",
    "\n",
    "Golden test cases are particularly useful for:\n",
    "- Regression testing\n",
    "- Comparing different model versions\n",
    "- Establishing baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Dataset successfully pushed to Confident AI! View at \n",
       "<a href=\"https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/datasets/cmerygdhn04wy14dygywfitaa\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/datasets/cmerygdhn04wy14dygywfitaa</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Dataset successfully pushed to Confident AI! View at \n",
       "\u001b]8;id=440617;https://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/datasets/cmerygdhn04wy14dygywfitaa\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmerya5260095wjfs6mxm2fmf/datasets/cmerygdhn04wy14dygywfitaa\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"What is MCP\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is Relationship between function calling & Model Context Protocol\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are the core components of MCP, just give the heading\",\n",
    "        \"reference\":\"\"\" \n",
    "                    - MCP Client\n",
    "                    - MCP Servers\n",
    "                    - Protocol Handshake\n",
    "                    - Capability Discovery\n",
    "                \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "goldens = []\n",
    "for data in test_data:\n",
    "    golden = Golden(\n",
    "        input=data['input'],\n",
    "        expected_output=data['reference']\n",
    "    )\n",
    "\n",
    "    goldens.append(golden)\n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens)\n",
    "\n",
    "dataset.push('test_rag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Comprehensive Evaluation on Multiple Test Cases\n",
    "\n",
    "The code below demonstrates how to evaluate multiple test cases with RAG-specific metrics. Notice how the evaluation includes retrieval context, which is essential for metrics like:\n",
    "\n",
    "- **Faithfulness**: Ensures the answer is grounded in retrieved documents\n",
    "- **Contextual Precision**: Checks if relevant documents are ranked higher\n",
    "- **Contextual Relevancy**: Measures the relevance of retrieved documents\n",
    "\n",
    "These metrics help identify issues like:\n",
    "- Hallucinations (answers not supported by context)\n",
    "- Poor retrieval quality\n",
    "- Irrelevant document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 944. Please try again in \n",
       "1.887s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 944. Please try again in \n",
       "1.887s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1102. Please try again in \n",
       "2.204s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1102. Please try again in \n",
       "2.204s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 984. Please try again in \n",
       "1.968s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 984. Please try again in \n",
       "1.968s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1121. Please try again in \n",
       "2.242s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1121. Please try again in \n",
       "2.242s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1339. Please try again in \n",
       "2.678s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1339. Please try again in \n",
       "2.678s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 736. Please try again in \n",
       "1.472s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 736. Please try again in \n",
       "1.472s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1229. Please try again in \n",
       "2.458s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1229. Please try again in \n",
       "2.458s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 6 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 254. Please try again in \n",
       "508ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 1 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 254. Please try again in \n",
       "508ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 1 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 254. Please try again in \n",
       "508ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 2 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 254. Please try again in \n",
       "508ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 2 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 944. Please try again in \n",
       "1.887s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 944. Please try again in \n",
       "1.887s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1102. Please try again in \n",
       "2.204s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1102. Please try again in \n",
       "2.204s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 29942, Requested 984. Please try again in \n",
       "1.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 29942, Requested 984. Please try again in \n",
       "1.852s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 29317, Requested 1229. Please try again in \n",
       "1.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 29317, Requested 1229. Please try again in \n",
       "1.092s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1339. Please try again in \n",
       "2.678s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization \n",
       "org-ezfJeg5RftmjxWODpHiv0MuG on tokens per min (TPM): Limit 30000, Used 30000, Requested 1339. Please try again in \n",
       "2.678s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, \n",
       "'code': 'rate_limit_exceeded'}} Retrying: 7 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ERROR:root:OpenAI Error: Could not parse response content as the length limit was reached - \n",
       "CompletionUsage(completion_tokens=32768, prompt_tokens=1054, total_tokens=33822, \n",
       "completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0,\n",
       "rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)) \n",
       "Retrying: 8 time(s)...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ERROR:root:OpenAI Error: Could not parse response content as the length limit was reached - \n",
       "CompletionUsage(completion_tokens=32768, prompt_tokens=1054, total_tokens=33822, \n",
       "completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0,\n",
       "rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)) \n",
       "Retrying: 8 time(s)...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase] :\n",
    "    # Ensure rag_chain is defined\n",
    "    from langchain import hub\n",
    "    from langchain.chains import create_retrieval_chain\n",
    "    from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    # Create the LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    # Create rag_chain\n",
    "    retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "    rag_chain = create_retrieval_chain(vectorstore.as_retriever(), combine_docs_chain)\n",
    "    \n",
    "    test_cases = []\n",
    "    for golden in goldens.goldens:\n",
    "        response = rag_chain.invoke({\"input\": golden.input})\n",
    "        \n",
    "        test_case = LLMTestCase(\n",
    "            input=golden.input,\n",
    "            actual_output=response[\"answer\"],\n",
    "            expected_output=golden.expected_output,\n",
    "            retrieval_context=[doc.page_content for doc in response[\"context\"]]\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.pull(alias=\"test_rag\")\n",
    "\n",
    "data = convert_goldens_to_test_cases(dataset)\n",
    "\n",
    "concise_metrics = GEval(\n",
    "    name=\"Concise\",\n",
    "    model=\"gpt-4o-mini\",  # Specify OpenAI model\n",
    "    criteria=\"Assess if the actual output remains concise while preserving all essential information.\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "deepeval.evaluate(\n",
    "    data,\n",
    "    metrics=[\n",
    "        deepeval.metrics.AnswerRelevancyMetric(),\n",
    "        deepeval.metrics.FaithfulnessMetric(),\n",
    "        deepeval.metrics.ContextualPrecisionMetric(),\n",
    "        deepeval.metrics.ContextualRelevancyMetric(),\n",
    "        concise_metrics\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
