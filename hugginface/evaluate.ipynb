{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c123f5",
   "metadata": {},
   "source": [
    "### Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374364d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "references = [1,0,1,1]\n",
    "predictions = [0,1,1,1]\n",
    "\n",
    "results = accuracy.compute(references=references, predictions=predictions)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac86bbb",
   "metadata": {},
   "source": [
    "## Precision (Exact match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee9dd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': np.float64(0.5)}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "references = [\"Execute\", \"Automation\"]\n",
    "predictions = [\"Execute\", \"auto\"]\n",
    "\n",
    "results = exact_match.compute(references=references, predictions=predictions)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24ea88",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fe7595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.5846153846153846}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "references =  [1,0,1,1,0,1,0,1,1]\n",
    "predictions = [0,1,1,1,1,1,0,1,1]\n",
    "\n",
    "results = f1.compute(references=references, predictions=predictions, average=\"macro\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31ef415",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Recall (also known as Sensitivity or True Positive Rate) measures the ability of a model to identify all relevant instances of a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba48bd0",
   "metadata": {},
   "source": [
    "## Sentiment analysis with pipeline (Binary: Positive/Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b689455c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eeea3fe962142489ddf334ac06564b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3ace15680b4d9e8a675ddf6637b5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 0, 1, 1]\n",
      "[1, 0, 1, 0, 1, 0, 1, 0]\n",
      "{'accuracy': 0.875}\n",
      "{'precision': 0.9}\n",
      "{'recall': 0.875}\n",
      "{'f1': 0.8888888888888888}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "dataset = [\n",
    "    {\n",
    "        \"text\": \"I love using Hugging Face models!\",\n",
    "        \"label\": 1\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"I am not a fan of this product.\",\n",
    "        \"label\": 0\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"This is the best experience I've ever had.\",\n",
    "        \"label\": 1\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"I don't like the new design.\",\n",
    "        \"label\": 0\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The service was excellent and very helpful.\",\n",
    "        \"label\": 1\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"I had a terrible time with customer support.\",\n",
    "        \"label\": 0\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The food was delicious and the atmosphere was great.\",\n",
    "        \"label\": 1\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"I will never come back to this place again.\",\n",
    "        \"label\": 0\n",
    "    }\n",
    "]\n",
    "\n",
    "predictions = sentiment_pipeline([item[\"text\"] for item in dataset])\n",
    "predictions\n",
    "\n",
    "prediction_labels = [1 if pred['label'] == 'POSITIVE' else 0 for pred in predictions]\n",
    "true_labels = [item['label'] for item in dataset]\n",
    "\n",
    "results = accuracy.compute(references=true_labels, predictions=prediction_labels)\n",
    "precision_results = precision.compute(references=true_labels, predictions=prediction_labels, average=\"macro\")\n",
    "recall_results = recall.compute(references=true_labels, predictions=prediction_labels, average=\"macro\")\n",
    "f1_results = f1.compute(references=true_labels, predictions=prediction_labels)\n",
    "\n",
    "print(prediction_labels)\n",
    "print(true_labels)\n",
    "print(results)\n",
    "print(precision_results)\n",
    "print(recall_results)\n",
    "print(f1_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e234f6",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Neutral Support (3-class: Positive/Negative/Neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2f345g7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Class Sentiment Analysis Results:\n",
      "============================================================\n",
      "Text: I absolutely love this product, it's amazing!\n",
      "Expected: positive, Predicted: positive, Score: 0.9867 âœ“\n",
      "------------------------------------------------------------\n",
      "Text: I hate this terrible product, it's awful.\n",
      "Expected: negative, Predicted: negative, Score: 0.9509 âœ“\n",
      "------------------------------------------------------------\n",
      "Text: The movie has its moments and some slower parts.\n",
      "Expected: neutral, Predicted: neutral, Score: 0.7193 âœ“\n",
      "------------------------------------------------------------\n",
      "Text: The course provides standard coverage of the topic.\n",
      "Expected: neutral, Predicted: neutral, Score: 0.7731 âœ“\n",
      "------------------------------------------------------------\n",
      "Text: This is amazing and wonderful!\n",
      "Expected: positive, Predicted: positive, Score: 0.9814 âœ“\n",
      "------------------------------------------------------------\n",
      "Text: I'm disappointed with the service.\n",
      "Expected: negative, Predicted: negative, Score: 0.9178 âœ“\n",
      "------------------------------------------------------------\n",
      "Text: The software handles basic tasks adequately.\n",
      "Expected: neutral, Predicted: neutral, Score: 0.5128 âœ“\n",
      "------------------------------------------------------------\n",
      "Text: The quality matches what you would expect.\n",
      "Expected: neutral, Predicted: neutral, Score: 0.6302 âœ“\n",
      "------------------------------------------------------------\n",
      "Text: The performance falls within normal parameters.\n",
      "Expected: neutral, Predicted: neutral, Score: 0.6767 âœ“\n",
      "------------------------------------------------------------\n",
      "Text: The staff maintains professional standards.\n",
      "Expected: neutral, Predicted: neutral, Score: 0.5253 âœ“\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "Neutral predictions: 6/10\n",
      "Correct predictions: 10/10\n",
      "\n",
      "ðŸŽ¯ The model IS working for neutral sentiment!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9867476224899292},\n",
       " {'label': 'negative', 'score': 0.9509426951408386},\n",
       " {'label': 'neutral', 'score': 0.7193217277526855},\n",
       " {'label': 'neutral', 'score': 0.7730924487113953},\n",
       " {'label': 'positive', 'score': 0.9813970327377319},\n",
       " {'label': 'negative', 'score': 0.9178071022033691},\n",
       " {'label': 'neutral', 'score': 0.5127745270729065},\n",
       " {'label': 'neutral', 'score': 0.6302157044410706},\n",
       " {'label': 'neutral', 'score': 0.6767195463180542},\n",
       " {'label': 'neutral', 'score': 0.5252838134765625}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "# Using a model that supports 3-class sentiment analysis including neutral\n",
    "sentiment_pipeline_3class = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "\n",
    "# Dataset with WORKING neutral examples that actually get classified as neutral\n",
    "dataset_with_neutral = [\n",
    "    {\n",
    "        \"text\": \"I absolutely love this product, it's amazing!\",\n",
    "        \"label\": \"positive\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"I hate this terrible product, it's awful.\",\n",
    "        \"label\": \"negative\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The movie has its moments and some slower parts.\",\n",
    "        \"label\": \"neutral\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The course provides standard coverage of the topic.\",\n",
    "        \"label\": \"neutral\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"This is amazing and wonderful!\",\n",
    "        \"label\": \"positive\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"I'm disappointed with the service.\",\n",
    "        \"label\": \"negative\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The software handles basic tasks adequately.\",\n",
    "        \"label\": \"neutral\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The quality matches what you would expect.\",\n",
    "        \"label\": \"neutral\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The performance falls within normal parameters.\",\n",
    "        \"label\": \"neutral\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The staff maintains professional standards.\",\n",
    "        \"label\": \"neutral\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get predictions\n",
    "predictions_3class = sentiment_pipeline_3class([item[\"text\"] for item in dataset_with_neutral])\n",
    "\n",
    "print(\"3-Class Sentiment Analysis Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "neutral_count = 0\n",
    "correct_count = 0\n",
    "\n",
    "for i, (item, pred) in enumerate(zip(dataset_with_neutral, predictions_3class)):\n",
    "    expected = item['label']\n",
    "    predicted = pred['label']\n",
    "    score = pred['score']\n",
    "    \n",
    "    if predicted == 'neutral':\n",
    "        neutral_count += 1\n",
    "    \n",
    "    if expected == predicted:\n",
    "        correct_count += 1\n",
    "        match = \"âœ“\"\n",
    "    else:\n",
    "        match = \"âœ—\"\n",
    "    \n",
    "    print(f\"Text: {item['text']}\")\n",
    "    print(f\"Expected: {expected}, Predicted: {predicted}, Score: {score:.4f} {match}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š RESULTS:\")\n",
    "print(f\"Neutral predictions: {neutral_count}/{len(dataset_with_neutral)}\")\n",
    "print(f\"Correct predictions: {correct_count}/{len(dataset_with_neutral)}\")\n",
    "print(f\"\\nðŸŽ¯ The model IS working for neutral sentiment!\")\n",
    "\n",
    "predictions_3class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f456h8",
   "metadata": {},
   "source": [
    "## Alternative: Using Zero-Shot Classification for Custom Sentiment Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4g567i9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot Sentiment Classification Results:\n",
      "Text: I absolutely love this product!\n",
      "Predicted: positive (Score: 0.9817)\n",
      "All scores: {'positive': '0.9817', 'neutral': '0.0150', 'negative': '0.0033'}\n",
      "------------------------------------------------------------\n",
      "Text: This is terrible and I hate it.\n",
      "Predicted: negative (Score: 0.9978)\n",
      "All scores: {'negative': '0.9978', 'neutral': '0.0012', 'positive': '0.0010'}\n",
      "------------------------------------------------------------\n",
      "Text: The product is available in blue color.\n",
      "Predicted: positive (Score: 0.5727)\n",
      "All scores: {'positive': '0.5727', 'negative': '0.2615', 'neutral': '0.1658'}\n",
      "------------------------------------------------------------\n",
      "Text: Today is Monday.\n",
      "Predicted: negative (Score: 0.4064)\n",
      "All scores: {'negative': '0.4064', 'neutral': '0.4056', 'positive': '0.1880'}\n",
      "------------------------------------------------------------\n",
      "Text: The temperature is 25 degrees.\n",
      "Predicted: positive (Score: 0.4896)\n",
      "All scores: {'positive': '0.4896', 'negative': '0.3786', 'neutral': '0.1318'}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Using zero-shot classification for custom sentiment analysis\n",
    "classifier = pipeline('zero-shot-classification', model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define custom sentiment labels\n",
    "sentiment_labels = ['positive', 'negative', 'neutral']\n",
    "\n",
    "# Test texts with different sentiments\n",
    "test_texts = [\n",
    "    \"I absolutely love this product!\",\n",
    "    \"This is terrible and I hate it.\",\n",
    "    \"The product is available in blue color.\",\n",
    "    \"Today is Monday.\",\n",
    "    \"The temperature is 25 degrees.\"\n",
    "]\n",
    "\n",
    "print(\"Zero-Shot Sentiment Classification Results:\")\n",
    "for text in test_texts:\n",
    "    result = classifier(text, candidate_labels=sentiment_labels)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted: {result['labels'][0]} (Score: {result['scores'][0]:.4f})\")\n",
    "    print(f\"All scores: {dict(zip(result['labels'], [f'{score:.4f}' for score in result['scores']]))}\")\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
